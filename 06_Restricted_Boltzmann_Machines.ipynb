{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><sub>This notebook is distributed under the <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\" target=\"_blank\">Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license</a>.</sub></div>\n",
    "<h1>Hands on Machine Learning  <span style=\"font-size:12px;\"><i>by <a href=\"https://webgrec.ub.edu/webpages/000004/cat/dmaluenda.ub.edu.html\" target=\"_blank\">David Maluenda</a></i></span></h1>\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://atenea.upc.edu/course/view.php?id=71605\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/upc_logo_49px.png\" width=\"130\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "  </td>\n",
    "  <td>   <!-- gColab -->\n",
    "    <a href=\"https://colab.research.google.com/github/dmaluenda/hands_on_machine_learning/blob/master/06_Restricted_Boltzmann_Machines.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/colab_logo_32px.png\" />\n",
    "      Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- github -->\n",
    "    <a href=\"https://github.com/dmaluenda/hands_on_machine_learning/blob/master/06_Restricted_Boltzmann_Machines.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/github_logo_32px.png\" />\n",
    "      View source on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- download -->\n",
    "    <a href=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/06_Restricted_Boltzmann_Machines.ipynb\"  target=\"_blank\"\n",
    "          download=\"06_Restricted_Boltzmann_Machines\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/download_logo_32px.png\" />\n",
    "      Download notebook\n",
    "      </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{VI}$. Restricted Boltzmann Machines\n",
    "\n",
    "Hands on \"Machine Learning on Classical and Quantum data\" course of\n",
    "[Master in Photonics - PHOTONICS BCN](https://photonics.masters.upc.edu/en/general-information)\n",
    "[[UPC](https://photonics.masters.upc.edu/en) +\n",
    "[UB](https://www.ub.edu/web/ub/en/estudis/oferta_formativa/master_universitari/fitxa/P/M0D0H/index.html?) +\n",
    "[UAB](https://www.uab.cat/en/uab-official-masters-degrees-study-guides/) +\n",
    "[ICFO](https://www.icfo.eu/lang/studies/master-studies)].\n",
    "\n",
    "Tutorial 6\n",
    "\n",
    "This notebook shows how to:\n",
    "- define a Restricted Boltzmann Machine (RBM)\n",
    "- train a RBM to sample from an observed probability distribution\n",
    "- appliy this to the MNIST digits images.\n",
    "- It also implements some of the tricks discussed by the inventor of RBM, G. Hinton, in [1]\n",
    "\n",
    "**References**:\n",
    "\n",
    "[1] A Practical Guide to Training Restricted Boltzmann Machines, Geoffrey Hinton. [UTML TR 2010â€“003](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf) (2010)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports:-Basically-numpy,-matplotlib-and-tensorflow\" data-toc-modified-id=\"Imports:-Basically-numpy,-matplotlib-and-tensorflow-0\"><span class=\"toc-item-num\">0&nbsp;&nbsp;</span>Imports: Basically numpy, matplotlib and tensorflow</a></span></li><li><span><a href=\"#Basic-training,-according-to-the-simple-principle-of-an-RBM\" data-toc-modified-id=\"Basic-training,-according-to-the-simple-principle-of-an-RBM-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Basic training, according to the simple principle of an RBM</a></span></li><li><span><a href=\"#More-advanced-training\" data-toc-modified-id=\"More-advanced-training-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>More advanced training</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports: Basically numpy, matplotlib and tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "\n",
    "# tensorflow is just to load the MNIST data\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoltzmannStep(v,b,w,do_random_sampling=True):\n",
    "    \"\"\"\n",
    "    Perform a single step of the Markov chain,\n",
    "    going from visible units v to hidden units h,\n",
    "    according to biases b and weights w.\n",
    "    \n",
    "    z_j = b_j + sum_i v_i w_ij\n",
    "    \n",
    "    and P(h_j=1|v) = 1/(exp(-z_j)+1)\n",
    "    \n",
    "    Note: you can go from h to v, by inserting\n",
    "    instead of v the h, instead of b the a, and\n",
    "    instead of w the transpose of w\n",
    "    \"\"\"\n",
    "    batchsize=np.shape(v)[0]\n",
    "    hidden_dim=np.shape(w)[1]\n",
    "    z=b+np.dot(v,w)\n",
    "    P=1/(np.exp(-z)+1)\n",
    "    # now, the usual trick to obtain 0 or 1 according\n",
    "    # to a given probability distribution:\n",
    "    # just produce uniform (in [0,1]) random numbers and\n",
    "    # check whether they are below the cutoff given by P\n",
    "    if do_random_sampling:\n",
    "        p=np.random.uniform(size=[batchsize,hidden_dim])\n",
    "        return(np.array(p<=P,dtype='int'))\n",
    "    else:\n",
    "        return(P) # no binary random output, just the prob. distribution itself!\n",
    "    \n",
    "def BoltzmannSequence(v,a,b,w,drop_h_prime=False,do_random_sampling=True,\n",
    "                      do_not_sample_h_prime=False,\n",
    "                     do_not_sample_v_prime=False):\n",
    "    \"\"\"\n",
    "    Perform one sequence of steps v -> h -> v' -> h'\n",
    "    of a Boltzmann machine, with the given\n",
    "    weights w and biases a and b!\n",
    "    \n",
    "    All the arrays have a shape [batchsize,num_neurons]\n",
    "    (where num_neurons is num_visible for v and\n",
    "    num_hidden for h)\n",
    "    \n",
    "    You can set drop_h_prime to True if you want to\n",
    "    use this routine to generate arbitrarily long sequences\n",
    "    by calling it repeatedly (then don't use h')\n",
    "    Returns: v,h,v',h'\n",
    "    \"\"\"\n",
    "    h=BoltzmannStep(v,b,w,do_random_sampling=do_random_sampling)\n",
    "    if do_not_sample_v_prime:\n",
    "        v_prime=BoltzmannStep(h,a,np.transpose(w),do_random_sampling=False)\n",
    "    else:\n",
    "        v_prime=BoltzmannStep(h,a,np.transpose(w),do_random_sampling=do_random_sampling)\n",
    "    if not drop_h_prime:\n",
    "        if do_not_sample_h_prime: # G. Hinton recommends not sampling in the v'->h' step (reduces noise)\n",
    "            h_prime=BoltzmannStep(v_prime,b,w,do_random_sampling=False)\n",
    "        else:\n",
    "            h_prime=BoltzmannStep(v_prime,b,w,do_random_sampling=do_random_sampling)\n",
    "    else:\n",
    "        h_prime=np.zeros(np.shape(h))\n",
    "    return(v,h,v_prime,h_prime)\n",
    "\n",
    "def trainStep(v,a,b,w,do_random_sampling=True,do_not_sample_h_prime=False,\n",
    "             do_not_sample_v_prime=False):\n",
    "    \"\"\"\n",
    "    Given a set of randomly selected training samples\n",
    "    v (of shape [batchsize,num_neurons_visible]), \n",
    "    and given biases a,b and weights w: update\n",
    "    those biases and weights according to the\n",
    "    contrastive-divergence update rules:\n",
    "    \n",
    "    delta w_ij = eta ( <v_i h_j> - <v'_i h'_j> )\n",
    "    delta a_i  = eta ( <v_i> - <v'_i>)\n",
    "    delta b_j  = eta ( <h_j> - <h'_j>)\n",
    "    \n",
    "    Returns delta_a, delta_b, delta_w, but without the eta factor!\n",
    "    It is up to you to update a,b,w!\n",
    "    \"\"\"\n",
    "    v,h,v_prime,h_prime=BoltzmannSequence(v,a,b,w,do_random_sampling=do_random_sampling,\n",
    "                                         do_not_sample_h_prime=do_not_sample_h_prime,\n",
    "                                         do_not_sample_v_prime=do_not_sample_v_prime)\n",
    "    return( np.average(v,axis=0)-np.average(v_prime,axis=0) ,\n",
    "            np.average(h,axis=0)-np.average(h_prime,axis=0) ,\n",
    "            np.average(v[:,:,None]*h[:,None,:],axis=0)-\n",
    "               np.average(v_prime[:,:,None]*h_prime[:,None,:],axis=0) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_sample_images(batchsize,num_visible,x_train,threshold=0.7,do_digitize=True):\n",
    "    \"\"\"\n",
    "    Produce 'batchsize' samples, of length num_visible.\n",
    "    Returns array v of shape [batchsize,num_visible]\n",
    "    \"\"\"\n",
    "    j=np.random.randint(low=0,high=np.shape(x_train)[0],size=batchsize) # pick random samples\n",
    "    \n",
    "    # reshape suitably, and digitize (so output is 0/1 values)\n",
    "    if do_digitize:\n",
    "        return( np.array( np.reshape(x_train[j,:,:],[batchsize,num_visible])>threshold, dtype='int' ) )\n",
    "    else:\n",
    "        return(  np.reshape(x_train[j,:,:],[batchsize,num_visible]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST data using tensorflow/keras\n",
    "(x_train,y_train),(x_test,y_test)=tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "x_train=x_train/256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the shape of these arrays\n",
    "print(np.shape(x_train),np.shape(y_train),np.shape(x_test),np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a few images, for fun\n",
    "nimages=10\n",
    "fig,ax=plt.subplots(ncols=nimages,nrows=1,figsize=(nimages,1))\n",
    "for n in range(nimages):\n",
    "    ax[n].imshow(x_train[n,:,:])\n",
    "    ax[n].set_title(str(y_train[n])) # the label\n",
    "    ax[n].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a few random samples:\n",
    "nimages=7\n",
    "Npixels=28\n",
    "samples=produce_sample_images(batchsize=nimages,num_visible=Npixels**2,x_train=x_train)\n",
    "# now unpack them again and display them:\n",
    "fig,ax=plt.subplots(ncols=nimages,nrows=1,figsize=(nimages,1))\n",
    "for n in range(nimages):\n",
    "    ax[n].imshow(np.reshape(samples[n,:],[Npixels,Npixels]))\n",
    "    ax[n].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a little trick (useful later): show them all at once, in one imshow\n",
    "# some weird reshape/transpose gymnastics, found by trial and error\n",
    "plt.imshow(np.transpose(np.reshape(np.transpose(np.reshape(samples,[nimages,Npixels,Npixels]),\n",
    "                                                axes=[0,2,1]),[nimages*Npixels,Npixels])))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training, according to the simple principle of an RBM \n",
    "\n",
    "All units are binary (0 or 1) all the time, randomly chosen according to the calculated probability distribution.\n",
    "\n",
    "*modify the code to get both machines at the same time*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now: the training\n",
    "# here: purely using random binary sampling of all\n",
    "# units at all times (this is not the most efficient way,\n",
    "# but implements directly the basic principle shown in the lecture)\n",
    "\n",
    "Npixels=28\n",
    "num_visible=Npixels**2\n",
    "num_hidden=60\n",
    "batchsize=50\n",
    "eta=0.1\n",
    "nsteps=3000\n",
    "skipsteps=10\n",
    "\n",
    "a=np.random.randn(num_visible)\n",
    "b=np.random.randn(num_hidden)\n",
    "w=0.01*np.random.randn(num_visible,num_hidden)\n",
    "\n",
    "# test_samples=np.zeros([num_visible,nsteps])\n",
    "\n",
    "for j in range(nsteps):\n",
    "    v=produce_sample_images(batchsize,num_visible,x_train)\n",
    "    da,db,dw=trainStep(v,a,b,w)\n",
    "    a+=eta*da\n",
    "    b+=eta*db\n",
    "    w+=eta*dw\n",
    "    print(\"{:05d} / {:05d}\".format(j,nsteps),end=\"   \\r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now: visualize the typical samples generated (from some starting point)\n",
    "# run several times to continue this. It basically is a random walk\n",
    "# through the space of all possible configurations, hopefully according\n",
    "# to the probability distribution that has been trained!\n",
    "\n",
    "nsteps=1000\n",
    "num_samples=20\n",
    "test_samples=np.zeros([num_samples,num_visible])\n",
    "skipsteps=1\n",
    "substeps=400 # how many steps to take before showing a new picture\n",
    "\n",
    "v_prime=np.zeros(num_visible)\n",
    "h=np.zeros(num_hidden)\n",
    "h_prime=np.zeros(num_hidden)\n",
    "\n",
    "for j in range(nsteps):\n",
    "    for k in range(substeps):\n",
    "        v,h,v_prime,h_prime=BoltzmannSequence(v,a,b,w,drop_h_prime=True) # step from v via h to v_prime!\n",
    "    test_samples[j%num_samples,:]=v[0,:]\n",
    "    v=np.copy(v_prime) # use the new v as a starting point for next step!\n",
    "    if j%skipsteps==0 or j==nsteps-1:\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(np.transpose(np.reshape(np.transpose(np.reshape(test_samples,[num_samples,Npixels,Npixels]),\n",
    "                                                axes=[0,2,1]),[num_samples*Npixels,Npixels])),\n",
    "                  interpolation='none')\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More advanced training\n",
    "\n",
    "Do not randomly sample h' and v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now: the training\n",
    "#\n",
    "# Here we use the more sophisticated approach, where\n",
    "# h' and v' are not binary (not randomly sampled), rather\n",
    "# they are taken as the probability distribution (numbers\n",
    "# between 0 and 1). This is a trick recommend by G. Hinton\n",
    "# in his review of Boltzmann Machines. It effectively means\n",
    "# less sampling noise.\n",
    "#\n",
    "# Also, we initialize the biases and weights according to the\n",
    "# tricks given in that review!\n",
    "\n",
    "Npixels=28\n",
    "num_visible=Npixels**2\n",
    "num_hidden=60\n",
    "batchsize=10\n",
    "eta=0.0001\n",
    "nsteps=10*30000\n",
    "skipsteps=10\n",
    "\n",
    "# get average brightness of training images:\n",
    "p_avg=np.average(np.reshape(x_train,[np.shape(x_train)[0],Npixels**2]),axis=0)\n",
    "a=np.log(p_avg/(1.0+1e-6-p_avg)+1e-6) # recipe for visible biases\n",
    "b=np.zeros(num_hidden) # recipe for hidden biases\n",
    "w=0.01*np.random.randn(num_visible,num_hidden) # recipe for weights\n",
    "\n",
    "# test_samples=np.zeros([num_visible,nsteps])\n",
    "\n",
    "for j in range(nsteps):\n",
    "    v=produce_sample_images(batchsize,num_visible,x_train,\n",
    "                           do_digitize=False)\n",
    "    da,db,dw=trainStep(v,a,b,w,\n",
    "                      do_not_sample_h_prime=True,\n",
    "                       do_not_sample_v_prime=True)\n",
    "    a+=eta*da\n",
    "    b+=eta*db\n",
    "    w+=eta*dw\n",
    "    print(\"{:06d} / {:06d}\".format(j,nsteps),end=\"   \\r\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now: visualize the typical samples generated (from some starting point)\n",
    "# run several times to continue this. It basically is a random walk\n",
    "# through the space of all possible configurations, hopefully according\n",
    "# to the probability distribution that has been trained!\n",
    "\n",
    "nsteps=20\n",
    "num_samples=20\n",
    "test_samples=np.zeros([num_samples,batchsize,num_visible])\n",
    "test_hidden=np.zeros([num_samples,batchsize,num_hidden])\n",
    "skipsteps=1\n",
    "substeps=400 # how many steps to take before showing a new picture\n",
    "\n",
    "v_prime=np.zeros([batchsize,num_visible])\n",
    "h=np.zeros([batchsize,num_hidden])\n",
    "h_prime=np.zeros([batchsize,num_hidden])\n",
    "\n",
    "v=produce_sample_images(batchsize,num_visible,x_train,\n",
    "                       do_digitize=False)\n",
    "    \n",
    "for j in range(nsteps):\n",
    "    for k in range(substeps):\n",
    "        v,h,v_prime,h_prime=BoltzmannSequence(v,a,b,w,\n",
    "                                  drop_h_prime=True,\n",
    "                                  do_not_sample_v_prime=True) # step from v via h to v_prime!\n",
    "    test_samples[j%num_samples,:,:]=v[:,:]\n",
    "    test_hidden[j%num_samples,:]=h[:,:]\n",
    "    v=np.copy(v_prime) # use the new v as a starting point for next step!\n",
    "    if j%skipsteps==0 or j==nsteps-1:\n",
    "        clear_output(wait=True)\n",
    "        fig,ax=plt.subplots(ncols=1,nrows=batchsize,figsize=(num_samples,batchsize))\n",
    "        for n in range(batchsize):\n",
    "            ax[n].imshow(np.transpose(np.reshape(np.transpose(np.reshape(test_samples[:,n,:],[num_samples,Npixels,Npixels]),\n",
    "                                                    axes=[0,2,1]),[num_samples*Npixels,Npixels])),\n",
    "                      interpolation='none')\n",
    "            ax[n].axis('off')\n",
    "        plt.show()\n",
    "        fig,ax=plt.subplots(ncols=1,nrows=batchsize,figsize=(num_samples,batchsize))\n",
    "        for n in range(batchsize):\n",
    "            ax[n].imshow(np.transpose(np.reshape(np.transpose(np.reshape(test_hidden[:,n,:],[num_samples,6,10]),\n",
    "                                                    axes=[0,2,1]),[num_samples*10,6])),\n",
    "                      interpolation='none')\n",
    "            ax[n].axis('off')\n",
    "        plt.show()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}