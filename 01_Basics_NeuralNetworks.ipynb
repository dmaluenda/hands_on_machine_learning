{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://atenea.upc.edu/course/view.php?id=71605\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/upc_logo_49px.png\" width=\"130\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "  </td>\n",
    "  <td>   <!-- gColab -->\n",
    "    <a href=\"https://colab.research.google.com/github/dmaluenda/hands_on_machine_learning/blob/master/01_Basics_NeuralNetworks.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/colab_logo_32px.png\" />\n",
    "      Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- github -->\n",
    "    <a href=\"https://github.com/dmaluenda/hands_on_machine_learning/blob/master/01_Basics_NeuralNetworks.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/github_logo_32px.png\" />\n",
    "      View source on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- download -->\n",
    "    <a href=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/01_Basics_NeuralNetworks.ipynb\"  target=\"_blank\" download=\"01_Basics_NeuralNetworks\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/download_logo_32px.png\" />\n",
    "      Download notebook\n",
    "      </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{I}$. Neural Networks with Pure Python\n",
    "\n",
    "Hands on \"Machine Learning on Classical and Quantum data\" course of\n",
    "[Master in Photonics - PHOTONICS BCN](https://photonics.masters.upc.edu/en/general-information)\n",
    "[[UPC](https://photonics.masters.upc.edu/en) +\n",
    "[UB](https://www.ub.edu/web/ub/en/estudis/oferta_formativa/master_universitari/fitxa/P/M0D0H/index.html?) +\n",
    "[UAB](https://www.uab.cat/en/uab-official-masters-degrees-study-guides/) +\n",
    "[ICFO](https://www.icfo.eu/lang/studies/master-studies)].\n",
    "\n",
    "Tutorial 1\n",
    "\n",
    "This notebook shows how to:\n",
    "- implement the forward-pass (evaluation) of a deep, fully connected neural network in a few lines of python\n",
    "- do that efficiently using batches\n",
    "- illustrate the results for randomly initialized neural networks\n",
    "\n",
    "**References**:\n",
    "\n",
    "[1] [Machine Learning for Physicists](https://machine-learning-for-physicists.org/) by Florian Marquardt.<br>\n",
    "[2] [NumPy](https://numpy.org/doc/stable/user/whatisnumpy.html): the fundamental package for scientific computing in Python.<br>\n",
    "[3] [Matplotlib](https://matplotlib.org/stable/tutorials/introductory/usage.html): a comprehensive library for creating static, animated, and interactive visualizations in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports:-only-numpy-and-matplotlib\" data-toc-modified-id=\"Imports:-only-numpy-and-matplotlib-0\"><span class=\"toc-item-num\">0&nbsp;&nbsp;</span>Imports: only numpy and matplotlib</a></span></li><li><span><a href=\"#A-very-simple-neural-network-(no-hidden-layer)\" data-toc-modified-id=\"A-very-simple-neural-network-(no-hidden-layer)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>A very simple neural network (no hidden layer)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-implementation-(single-input)\" data-toc-modified-id=\"Simple-implementation-(single-input)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Simple implementation (single input)</a></span></li><li><span><a href=\"#More-compact\" data-toc-modified-id=\"More-compact-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>More compact</a></span></li><li><span><a href=\"#Multiple-inputs\" data-toc-modified-id=\"Multiple-inputs-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Multiple inputs</a></span></li></ul></li><li><span><a href=\"#NN-with-one-hidden-layer\" data-toc-modified-id=\"NN-with-one-hidden-layer-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>NN with one hidden layer</a></span></li><li><span><a href=\"#'batch'-processing-of-Neural-Networks\" data-toc-modified-id=\"'batch'-processing-of-Neural-Networks-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>'batch' processing of Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Matrix-vector-multiplication-of-array-dimensions\" data-toc-modified-id=\"Matrix-vector-multiplication-of-array-dimensions-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Matrix-vector multiplication of array dimensions</a></span></li><li><span><a href=\"#Defining-functions-to-evaluate-a-layer-and-a-network-with-batch-processing\" data-toc-modified-id=\"Defining-functions-to-evaluate-a-layer-and-a-network-with-batch-processing-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Defining functions to evaluate a layer and a network with batch processing</a></span></li><li><span><a href=\"#Now-visualize-this-multi-layer-net,-now-more-efficiently!\" data-toc-modified-id=\"Now-visualize-this-multi-layer-net,-now-more-efficiently!-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Now visualize this multi-layer net, now more efficiently!</a></span></li><li><span><a href=\"#A-network-with-MANY-hidden-layers-(same-size-each)\" data-toc-modified-id=\"A-network-with-MANY-hidden-layers-(same-size-each)-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>A network with MANY hidden layers (same size each)</a></span></li><li><span><a href=\"#[Homework]-Try-with-another-nonlinear-activation-function\" data-toc-modified-id=\"[Homework]-Try-with-another-nonlinear-activation-function-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>[Homework] Try with another nonlinear activation function</a></span></li></ul></li><li><span><a href=\"#Summing-up-in-general-functions\" data-toc-modified-id=\"Summing-up-in-general-functions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Summing up in general functions</a></span></li><li><span><a href=\"#Fancy-visualization-of-Neural-Networks-with-Pure-Python\" data-toc-modified-id=\"Fancy-visualization-of-Neural-Networks-with-Pure-Python-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Fancy visualization of Neural Networks with Pure Python</a></span><ul class=\"toc-item\"><li><span><a href=\"#Some-internal-routines-for-plotting-the-network\" data-toc-modified-id=\"Some-internal-routines-for-plotting-the-network-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Some internal routines for plotting the network</a></span></li><li><span><a href=\"#No-hidden-layer-NN\" data-toc-modified-id=\"No-hidden-layer-NN-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>No hidden layer NN</a></span></li><li><span><a href=\"#Deep-dense-NN\" data-toc-modified-id=\"Deep-dense-NN-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Deep dense NN</a></span></li><li><span><a href=\"#Something-not-random\" data-toc-modified-id=\"Something-not-random-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Something not random</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlT_ZrS-R1rG"
   },
   "source": [
    "## Imports: only numpy and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T15:54:31.308199Z",
     "iopub.status.busy": "2021-04-19T15:54:31.307840Z",
     "iopub.status.idle": "2021-04-19T15:54:31.896606Z",
     "shell.execute_reply": "2021-04-19T15:54:31.895654Z",
     "shell.execute_reply.started": "2021-04-19T15:54:31.308135Z"
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1626874792493,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "zRmzXX4wR1rG",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the \"numpy\" library for linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# get \"matplotlib\" for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 300  # highres display\n",
    "from matplotlib.axes._axes import _log as mpl_ax_logger\n",
    "mpl_ax_logger.setLevel('ERROR')  # ignore warnings\n",
    "\n",
    "\n",
    "# for nice inset colorbars:\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "# time control to count it and manage it\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r68KcALRR1rH"
   },
   "source": [
    "## A very simple neural network (no hidden layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T98H_rSNR1rH"
   },
   "source": [
    "A network with $N_0$ input neurons and $N_1$ output neurons (no hidden layer)\n",
    "\n",
    "$$y^{\\rm out}_j = f(\\sum_k^{N_0} w_{jk} y^{\\rm in}_k + b_j) \\quad ; \\quad j=1\\dots N_1$$\n",
    "\n",
    "where $w$ is the weight matrix, $b$ is the bias vector, and $f$ would be the activation function (e.g. the sigmoid here), which is applied independently for each $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple implementation (single input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1626874814911,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "xtRaehMZR1rI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N0 = 3  # input layer size\n",
    "N1 = 2  # output layer size\n",
    "\n",
    "# initialize random weights: array dimensions N1xN0\n",
    "w = np.random.uniform(low=-1, high=+1, size=(N1,N0))\n",
    "\n",
    "# initialize random biases: N1 vector\n",
    "b = np.random.uniform(low=-1, high=+1, size=N1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with an arbitrary input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1626874822256,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "LY1EGJKoR1rI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# input values\n",
    "y_in = np.array([0.2, 0.4, -0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1626874831072,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "SR-4V6z2R1rJ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# evaluate network by hand, in two steps\n",
    "z = np.dot(w, y_in) + b  # result: the vector of 'z' values, length N1\n",
    "y_out = 1 / (1 + np.exp(-z))  # the 'sigmoid' function (applied elementwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1626874836630,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "KTpiYEIFR1rJ",
    "outputId": "d13d700c-1365-4dbf-d2e1-29aa014625ee",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"network input y_in:\", y_in)\n",
    "print(\"weights w:\", w)\n",
    "print(\"bias vector b:\", b)\n",
    "print(\"linear superposition z:\", z)\n",
    "print(\"network output y_out:\", y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbIUsnbaR1rK"
   },
   "source": [
    "### More compact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMfY7VTBR1rK"
   },
   "source": [
    "Still stay with the simple network, but define a function that evaluates the network, and visualize the  output for various inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1626874872832,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "8ahzBFksR1rL",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# a function that applies the network\n",
    "def apply_basic_net(y_in):\n",
    "    global w, b  # let's use global variables. We could pass them as arguments\n",
    "    \n",
    "    z = np.dot(w, y_in) + b    \n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with a different number of neurons\n",
    "\n",
    "**NOTE**: NNs with 2 input neurons and 1 output neuron are ideal to learn how it\n",
    "works because it allows us to visualize its behavior in a single picture. Let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1626874886437,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "tzGCtU9uR1rL",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N0 = 2  # input layer size\n",
    "N1 = 1  # output layer size\n",
    "\n",
    "w = np.random.uniform(low=-10, high=+10, size=(N1,N0))  # random weights: N1xN0\n",
    "b = np.random.uniform(low=-1, high=+1, size=N1)  # biases: N1 vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a valid input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1626874895575,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "CsE8xEuPR1rL",
    "outputId": "e3396801-1459-4a7e-b070-ff414f33aec9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_in = []  # Fill this array \n",
    "apply_basic_net(y_in)  # a simple test with an arbitrary input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which shape y_in must be? And the output? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple inputs\n",
    "\n",
    "Let's see how the network acts with many different inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1626874936164,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "kFEaqMXaR1rM",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M = 50  # Number of inputs to explore\n",
    "y_out = np.zeros([M, M]) # array MxM, to hold the result\n",
    "\n",
    "for j1 in range(M): \n",
    "    for j2 in range(M):\n",
    "        # out of these integer indices, generate\n",
    "        # two values in the range -0.5...0.5\n",
    "        # and then apply the network to those two\n",
    "        # values, being the input\n",
    "        value0 = float(j1) / M - 0.5\n",
    "        value1 = float(j2) / M - 0.5\n",
    "        y_in = [value0, value1]\n",
    "        y_out[j1, j2] = apply_basic_net(y_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: this is NOT the most efficient way to do this! (but simple). We will later learn how to use array syntax efficiently\n",
    "\n",
    "The output is an array (50 $\\times$ 50) containing all combinations of values from -0.5 to 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1149,
     "status": "ok",
     "timestamp": 1626874942725,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "kHY8ZoF-R1rM",
    "outputId": "64336e64-6e04-4ee7-d735-6e058534d52f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display image\n",
    "plt.imshow(y_out, origin='lower', extent=(-0.5,0.5,-0.5,0.5))\n",
    "plt.colorbar()\n",
    "plt.title(\"NN output as a function of input values\")\n",
    "plt.xlabel(\"y_2\")\n",
    "plt.ylabel(\"y_1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What represent this image? Which kind of image is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adIF2dA2R1rM"
   },
   "source": [
    "## NN with one hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foHTd0oMR1rN"
   },
   "source": [
    "The idea here is to have multiple weight matrices (for each pair of subsequent layers there is one weight matrix). The function that \"applies a layer\", i.e. goes from one layer to the next, is essentially the same as the function evaluating the simple network above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1626874964760,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "OrOJeDuzR1rN",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def apply_basic_layer(y_in, w, b):\n",
    "    \"\"\" A function that evaluates one layer based on \n",
    "        the neuron values in the preceding layer.\n",
    "        Now, we pass w and b as argument to be able \n",
    "        to accommodate them to a certain layer\n",
    "    \"\"\"\n",
    "    z = np.dot(w, y_in) + b\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention on the array's sizes to initialize weighs and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1626874970040,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "v5JXwyNNR1rN",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N0 = 2  # input layer size\n",
    "N1 = 30 # hidden layer size\n",
    "N2 = 1  # output layer size\n",
    "\n",
    "# weights and biases\n",
    "# from input layer to hidden layer:\n",
    "w1 = np.random.uniform(low=-10, high=+10, size=(N1,N0)) # random weights: N1xN0\n",
    "b1 = np.random.uniform(low=-1, high=+1, size=N1)        # biases: N1 vector\n",
    "\n",
    "# weights+biases from hidden layer to output layer:\n",
    "w2 = np.random.uniform(low=-5, high=+5, size=(N2,N1)) # random weights: N2xN1\n",
    "b2 = np.random.uniform(low=-1, high=+1, size=N2) # biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1626874973852,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "Cz1yN_CPR1rO",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# evaluate the network by subsequently\n",
    "# evaluating the two steps (input to hidden and hidden to output)\n",
    "def apply_basic_net(y_in):\n",
    "    global w1, b1, w2, b2\n",
    "    \n",
    "    y1 = apply_basic_layer(y_in, w1, b1)\n",
    "    y2 = apply_basic_layer(y1, w2, b2)\n",
    "    \n",
    "    return y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, obtain values for a range of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1626874983951,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "6Tdm8QPIR1rO",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M = 300  # number of inputs to explore\n",
    "y_out = np.zeros([M, M])  # array MxM, to hold the result\n",
    "\n",
    "t0 = time()  # initial time\n",
    "for j1 in range(M):\n",
    "    for j2 in range(M):\n",
    "        value0 = float(j1) / M - 0.5\n",
    "        value1 = float(j2) / M - 0.5\n",
    "        y_in = [value0, value1]\n",
    "        y_out[j1, j2]=apply_basic_net(y_in)\n",
    "\n",
    "print(\"Elapsed time: %.0f seconds for %d inputs.\" % (time()-t0, M**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Again, this is NOT the most efficient way to do this! (but simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "executionInfo": {
     "elapsed": 764,
     "status": "ok",
     "timestamp": 1626875087549,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "R40sBA-3R1rO",
    "outputId": "591f9ce8-c86d-4f6c-a798-5e4ef433788b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display image\n",
    "plt.imshow(y_out, origin='lower', extent=(-0.5,0.5,-0.5,0.5))\n",
    "plt.colorbar()\n",
    "plt.title(\"NN output as a function of input values\")\n",
    "plt.xlabel(\"y_2\")\n",
    "plt.ylabel(\"y_1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZCUJY6-R1rP"
   },
   "source": [
    "Obviously, the shape of the output is more 'complex' that of a simple network without any\n",
    "hidden layer. Let's go further in that direction..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7oWq_SPR1rP"
   },
   "source": [
    "## 'batch' processing of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DS-HkOzxR1rP"
   },
   "source": [
    "Goal: apply a network to many samples in parallel (no 'for' loops!).\n",
    "Let's solve the less efficient method above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhfXVsoxR1rP"
   },
   "source": [
    "### Matrix-vector multiplication of array dimensions\n",
    "\n",
    "See how the dot product works in a no hidden layer NN (or a single step from one layer to the next in a more deep NN).\n",
    "\n",
    "For instance, let's see with $N_0=8$ (input neurons), $N_1=7$ (output neurons) and $M=50$ (different inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2777,
     "status": "ok",
     "timestamp": 1626875178917,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "bojqaDqrR1rP",
    "outputId": "64950c99-8a19-424d-a447-f6f5c1a4b44f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N0 = 8\n",
    "N1 = 7\n",
    "M = 50\n",
    "\n",
    "W = np.zeros([N1, N0])  # Weight is N1xN0, as before\n",
    "y = np.zeros([N0, M]) # Let's define a matrix to hold the M different inputs\n",
    "\n",
    "(np.dot(W, y)).shape  # Let's see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to add the bias vector entries, in the most naive way (beware!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "error",
     "timestamp": 1626875182252,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "6eTZn4pYR1rQ",
    "outputId": "4e807f81-5951-4792-a7bb-779e5dbb8bdb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "B = np.zeros(N1)  # Bias size is corresponding with the output neurons\n",
    "(np.dot(W, y) + B).shape  # will produce an error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is that error? How can we avoid it?\n",
    "\n",
    "Ok, we have to think a bit more...\n",
    "\n",
    "Let's play with the order of the indices (arrays shapes).\n",
    "\n",
    "Let's try to put the $M$ to the very beginning to hold the different inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1626875213771,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "MlopEfpER1rQ",
    "outputId": "7a127d70-359b-4340-ec05-129f8ce2bf2e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = np.zeros([M, N0])\n",
    "(np.dot(y, W)).shape  # will produce an error! Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have exploited out earlier... let's think\n",
    "\n",
    "We can transpose the weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.zeros([N0, N1])\n",
    "(np.dot(y, W)).shape  # Fixed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add again the bias vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1626875227687,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "eB4MSTDpR1rQ",
    "outputId": "b74d153f-8e19-4d45-ad3d-45e352cda7fa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "B = np.zeros(7)\n",
    "(np.dot(y, W) + B).shape  # Voila!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the desired result, which contains 50 results of 7 values at once. This is full of zeros, but we have asserted that the method is consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvu0gA4nR1rQ"
   },
   "source": [
    "### Defining functions to evaluate a layer, and a network with batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDGZYcPHR1rR"
   },
   "source": [
    "Set up for batch processing, i.e. parallel evaluation of many input samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1626875241146,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "WDnsidUPR1rR",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def apply_layer_batch(y_in, w, b): # a function that applies a layer, just as before\n",
    "    z = np.dot(y_in, w) + b  # but, note different order in matrix product!\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1626875244101,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "ee2uQFPYR1rR",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def apply_net_batch(y_in): # same as before\n",
    "    global w1, b1, w2, b2\n",
    "    \n",
    "    y1 = apply_layer_batch(y_in, w1, b1)\n",
    "    y2 = apply_layer_batch(y1, w2, b2)\n",
    "    \n",
    "    return y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the NN model. Note that it is independent of the batch! It just\n",
    "allows batching with the new ordering of the matrix indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1626875246429,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "f4jS8Uu0R1rR",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N0 = 2  # input layer size\n",
    "N1 = 30  # hidden layer size\n",
    "N2 = 1  # output layer size\n",
    "\n",
    "# from input layer to hidden layer:\n",
    "w1 = np.random.uniform(low=-10, high=+10, size=(N0,N1)) # NEW ORDER!! N0xN1\n",
    "b1 = np.random.uniform(low=-1, high=+1, size=N1)        # biases: N1 vector\n",
    "\n",
    "# from hidden layer to output layer:\n",
    "w2 = np.random.uniform(low=-5, high=+5, size=(N1, N2)) # NEW ORDER N1xN2\n",
    "b2 = np.random.uniform(low=-1, high=+1, size=N2) # biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an input with a certain batch size (e.g. 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1626875247989,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "G2b6UfwkR1rR",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batchsize = 90000\n",
    "y = np.random.uniform(low=-1, high=1, size=(batchsize,2))  # Why this 2 here??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1626875249135,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "FngEZZD0R1rS",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "y_out = apply_net_batch(y)\n",
    "print(\"Elapsed time: %f seconds for %d inputs.\" % (time()-t0, batchsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1626875250087,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "750oqszIR1rS",
    "outputId": "ccc05ddf-2880-42bf-c3cb-a9663823feab",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.shape(y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were 90,000 samples evaluated in parallel!!! (less than 1 second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jt56KtwSR1rS"
   },
   "source": [
    "### Now visualize this multi-layer net, now more efficiently!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78ZkH4P6R1rS"
   },
   "source": [
    "Before, we had a two nested loops running all possible values for each input neuron.\n",
    "Now, we have to think a way to put all that possibilities in a batch array. How to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1626875258337,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "J48VP4KBR1rS",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M = 300\n",
    "# Generate a 'mesh grid', i.e. x,y values in an ordered way. Let's see\n",
    "values = np.linspace(-0.5,0.5,M)  # This crates an array from -0.5 to 0.5 of M elements\n",
    "v0, v1 = np.meshgrid(values, values)  # Check meshgrid documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1626875261064,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "XlczAYqkR1rT",
    "outputId": "601b6849-b593-4033-8b68-9a30839288cb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(v0, origin='lower')\n",
    "ax[0].set_title(\"input value v0\")\n",
    "ax[1].imshow(v1, origin='lower')\n",
    "ax[1].set_title(\"input value v1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look, `v0` increases with the horizontal direction and remains constant on the\n",
    "vertical, like the $x$ coordinate. While `v1` runs just on the opposite,\n",
    "like $y$ coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1626875272772,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "odaw2KOMR1rT",
    "outputId": "77d7659e-f0d4-4317-dafe-8496bbf522ba",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v0flat = v0.flatten()  # make 1D arrays out of 2D array, reading from left to right\n",
    "v1flat = v1.flatten()  #  then, goes for the next line\n",
    "# that means: MxM matrix becomes M^2 vector\n",
    "print('v0flat: ', v0flat, ' ; shape: ', v0flat.shape)\n",
    "print('v1flat: ', v1flat, ' ; shape: ', v1flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1626875274972,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "BudbyrMVR1rT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batchsize = v0flat.size  # number of samples\n",
    "y_in = np.zeros([batchsize,2])  # initialize\n",
    "y_in[:,0] = v0flat # fill first component (index 0)\n",
    "y_in[:,1] = v1flat # fill second component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply net to all these samples simultaneously! At the end, this is equivalent to the `apply_net` in the double loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1626875276327,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "8eRZpeBMR1rT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "y_out=apply_net_batch(y_in) \n",
    "print(\"Elapsed time: %f seconds for %d inputs.\" % (time()-t0, batchsize))\n",
    "y_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is not a vector, but a funny flattened matrix of batchsize x 1. Why is `x 1`?\n",
    "\n",
    "Let's go back from a flattened array to a real matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1626875278579,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "DWWzQ4x5R1rU",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_2D = np.reshape(y_out[:,0],[M,M]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1626875281319,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "pM1hFhirR1rU",
    "outputId": "47d3a65a-63d3-4057-d079-a7153205f3a0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(y_2D, origin='lower')\n",
    "plt.title(\"NN output (one hidden layer)\")\n",
    "plt.xlabel(\"v0\")\n",
    "plt.ylabel(\"v1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLUulorJR1rU"
   },
   "source": [
    "### A network with MANY hidden layers (same size each)\n",
    "\n",
    "We can create a single `weights` hyper-matrix containing the weight of all hidden layers. It can be seen as a book with several sheets, where every sheet (slice) is a matrix corresponding to a certain weight matrix for that hidden layer. We increase the range of these weight in order to make them more relevant on the NN.\n",
    "\n",
    "We will do the same for the `biases`. Before, it's a vector, then it becomes a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1626875292158,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "W2_okOLTR1rU",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Nlayers = 20  # not counting the input layer & the output layer\n",
    "LayerSize = 100\n",
    "\n",
    "# Note the extra dim at first for the weights and biases\n",
    "weights = np.random.uniform(low=-3, high=3, size=[Nlayers,LayerSize,LayerSize]) \n",
    "biases = np.random.uniform(low=-1, high=1, size=[Nlayers,LayerSize])\n",
    "\n",
    "# for the first layer having 2 inputs\n",
    "weightsFirst = np.random.uniform(low=-1, high=1, size=[2,LayerSize])\n",
    "biasesFirst = np.random.uniform(low=-1, high=1, size=LayerSize)\n",
    "\n",
    "# for the final layer having 1 output\n",
    "weightsFinal = np.random.uniform(low=-1, high=1, size=[LayerSize,1])\n",
    "biasesFinal = np.random.uniform(low=-1, high=1, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1626875295792,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "WzujiQ6ER1rU",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def apply_multi_net(y_in):\n",
    "    global weights, biases, weightsFinal, biasesFinal, Nlayers\n",
    "    \n",
    "    y = apply_layer_batch(y_in, weightsFirst, biasesFirst)    \n",
    "    for j in range(Nlayers):\n",
    "        y = apply_layer_batch(y, weights[j,:,:], biases[j,:])  # We take the j-th slice\n",
    "    output = apply_layer_batch(y, weightsFinal, biasesFinal)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a set of pair-values (inputs) using a flattened meshgrid, as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1626875297291,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "kbIE7_qWR1rV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate a 'mesh grid', i.e. x,y values in an image\n",
    "M = 300  # Let's do it quite resolutive\n",
    "v0, v1 = np.meshgrid(np.linspace(-0.5,0.5,M), np.linspace(-0.5,0.5,M))\n",
    "batchsize = M**2  # number of samples = M^2  (it's equivalent as before)\n",
    "\n",
    "y_in = np.zeros([batchsize, 2])  # initialize\n",
    "y_in[:,0] = v0.flatten() # fill first component (index 0)\n",
    "y_in[:,1] = v1.flatten() # fill second component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the dens NN in batch mode (300 $\\times$ 300 = 90,000 different inputs at once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1626875298627,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "kaPZGeWNR1rV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "y_out = apply_multi_net(y_in) # apply net to all these samples! (it take some seconds)\n",
    "print(\"Elapsed time: %f seconds for %d inputs.\" % (time()-t0, batchsize))\n",
    "y_2D = np.reshape(y_out, [M,M]) # back to 2D image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1626875301384,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "vKFIzkmKR1rW",
    "outputId": "eb87c9b9-8382-456c-c26a-c8a0243918af",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(y_2D, origin='lower', extent=[-0.5,0.5,-0.5,0.5], interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLMlbZzrR1rX"
   },
   "source": [
    "### [Homework] Try with another nonlinear activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "executionInfo": {
     "elapsed": 1239,
     "status": "ok",
     "timestamp": 1626879923136,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "KZQblTcOR1rX",
    "outputId": "853b697a-7ed0-4773-f435-c60a5d6befd6",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summing up in general functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a couple of functions to create an arbitrary deep and fully connected neural network in batch mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_layer(y_in, w, b, activation):\n",
    "    \"\"\" Batch processing of a single layer:\n",
    "           y_in: input values  -> shape: [batchsize, num_neurons_in]\n",
    "           w:    weight matrix -> shape: [n_neurons_in, n_neurons_out]\n",
    "           b:    bias vector   -> length: n_neurons_out\n",
    "    \n",
    "           activation is some string of the following ones:\n",
    "             - sigmoid\n",
    "             - jump\n",
    "             - linear\n",
    "             - reLU\n",
    "    \n",
    "           returns the values of the output neurons in the next layer \n",
    "              -> shape: [batchsize, n_neurons_out]\n",
    "    \"\"\"\n",
    "    z = np.dot(y_in, w) + b\n",
    "    if activation == 'sigmoid':\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    elif activation == 'jump':\n",
    "        return np.array(z>0, dtype='float')\n",
    "    elif activation == 'linear':\n",
    "        return z\n",
    "    elif activation == 'reLU':\n",
    "        return (z > 0) * z\n",
    "\n",
    "    \n",
    "def apply_net(y_in, weights, biases, activations):\n",
    "    \"\"\" Apply a whole network of multiple layers.\n",
    "          y_in: input values  -> shape: [batchsize, num_neurons_in]\n",
    "          weights, biases and activations must be any iterable \n",
    "          which length is the layers' number containing\n",
    "              weight matrix  -> shape: [n_neurons_in, n_neurons_out]\n",
    "              bias vector    -> length: n_neurons_out\n",
    "              activation str -> sigmoid, jump linear or reLU\n",
    "          Alternatively, they can be extended matrices\n",
    "          where a simple slicing generates the proper weight, \n",
    "          bias and activation.\n",
    "    \"\"\"\n",
    "    y = y_in\n",
    "    for j in range(len(biases)):\n",
    "        y = apply_layer(y, weights[j], biases[j], activations[j])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fancy visualization of Neural Networks with Pure Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some internal routines for plotting the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLUE_COLOR = [0, 0.4, 0.8]\n",
    "ORANGE_COLOR = [1, 0.3, 0]\n",
    "\n",
    "def plot_connection_line(ax, X, Y, W, vmax=1.0, linewidth=3):\n",
    "    \"\"\" Draw a fancy line from (X[0], Y[0]) to (X[1], Y[1])\n",
    "        according to the weight W into the frame ax.\n",
    "    \"\"\"\n",
    "    t = np.linspace(0,1,20)  # free parameter to draw lines\n",
    "    \n",
    "    if W > 0:  # Color depending on the weight's sign\n",
    "        col = BLUE_COLOR\n",
    "    else:\n",
    "        col = ORANGE_COLOR\n",
    "    \n",
    "    # fancy line from (X0, Y0) to (X1, Y1)\n",
    "    xx = X[0] + t*(X[1] - X[0])  # Linear in horizontal\n",
    "    yy = Y[0] + (3*t**2 - 2*t**3) * (Y[1] - Y[0])  # Round borders\n",
    "    \n",
    "    # plotting the line according to the weight\n",
    "    ax.plot(xx, yy, alpha=abs(W)/vmax,\n",
    "            color=col, linewidth=linewidth)\n",
    "\n",
    "    \n",
    "def plot_neuron_alpha(ax, X, Y, B, size=100.0, vmax=1.0):\n",
    "    \"\"\" Draw a single neuron in position (X, Y) according to \n",
    "        the bias B, into the frame ax.\n",
    "    \"\"\"\n",
    "    if B > 0:\n",
    "        col = BLUE_COLOR\n",
    "    else:\n",
    "        col = ORANGE_COLOR\n",
    "        \n",
    "    ax.scatter([X], [Y], marker='o', c=col, alpha=abs(B)/vmax, \n",
    "               s=size, zorder=10)\n",
    "\n",
    "    \n",
    "def plot_neuron(ax, X, Y, B, size=100.0, vmax=1.0):\n",
    "    \"\"\" Draw a single neuron in position (X, Y) independently to \n",
    "        the bias B, into the frame ax.\n",
    "    \"\"\"\n",
    "    if B > 0:\n",
    "        col = BLUE_COLOR\n",
    "    else:\n",
    "        col = ORANGE_COLOR\n",
    "        \n",
    "    ax.scatter([X], [Y], marker='o', c=col, s=size, zorder=10)\n",
    "    \n",
    "    \n",
    "def visualize_network(weights, biases, activations, M=100,\n",
    "                      y0range=[-1,1], y1range=[-1,1],\n",
    "                      size=400.0, linewidth=5.0):\n",
    "    \"\"\"\n",
    "    Visualize a neural network with 2 input \n",
    "    neurons and 1 output neuron (plot output vs input in a 2D plot)\n",
    "    \n",
    "    weights is a list of the weight matrices for the\n",
    "    layers, where weights[j] is the matrix for the connections\n",
    "    from layer j to layer j+1 (where j==0 is the input)\n",
    "    \n",
    "    weights[j][m,k] is the weight for input neuron k going to output neuron m\n",
    "    (note: internally, m and k are swapped, see the explanation of\n",
    "    batch processing in lecture 2)\n",
    "    \n",
    "    biases[j] is the vector of bias values for obtaining the neurons \n",
    "    in layer j+1, biases[j][k] is the bias for neuron k in layer j+1\n",
    "    \n",
    "    activations is a list of the activation functions for\n",
    "    the different layers: choose 'linear','sigmoid', \n",
    "    'jump' (i.e. step-function), and 'reLU'\n",
    "    \n",
    "    M is the resolution (MxM grid)\n",
    "    \n",
    "    y0range is the range of y0 neuron values (horizontal axis)\n",
    "    y1range is the range of y1 neuron values (vertical axis)\n",
    "    \"\"\"\n",
    "    # Let's transpose the weight to be able the batch processing\n",
    "    swapped_weights = []\n",
    "    for j in range(len(weights)):\n",
    "        swapped_weights.append(np.transpose(weights[j]))\n",
    "        \n",
    "    # Let's create a set of input-pairs by means of a mesh grid\n",
    "    y0, y1 = np.meshgrid(np.linspace(y0range[0], y0range[1], M),\n",
    "                         np.linspace(y1range[0], y1range[1], M))\n",
    "    y_in = np.zeros([M*M, 2])\n",
    "    y_in[:, 0] = y0.flatten()\n",
    "    y_in[:, 1] = y1.flatten()\n",
    "    \n",
    "    # Let's apply the NN\n",
    "    y_out = apply_net(y_in, swapped_weights, biases, activations)\n",
    "\n",
    "    # We will plot a diagram at left and the result at right\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(8,4))\n",
    "    \n",
    "    \n",
    "    # For the diagram\n",
    "    \n",
    "    #  1: posX and posY are arrays containing the positions of neurons\n",
    "    posX = [[0, 0]]  # same column for both (at left)    \n",
    "    posY = [[-0.5, +0.5]]  # for 2 inputs, let's putted centered in high\n",
    "\n",
    "    \n",
    "    vmax = 0.0 # for finding the maximum weight\n",
    "    vmaxB = 0.0 # for maximum bias\n",
    "    for j in range(len(biases)):  # for every layer on the NN\n",
    "        n_neurons = len(biases[j])  # neurons in the current layer\n",
    "        \n",
    "        posX.append(np.full(n_neurons, j+1))  # next column to the previous one\n",
    "        posY.append(np.array(range(n_neurons)) - 0.5 * (n_neurons-1)) # spread\n",
    "        \n",
    "        vmax = np.maximum(vmax, np.max(np.abs(weights[j])))  # to get the maximum\n",
    "        vmaxB = np.maximum(vmaxB, np.max(np.abs(biases[j])))\n",
    "\n",
    "    #   2: plot connections\n",
    "    for j in range(len(biases)):  # for each layer\n",
    "        for k in range(len(posX[j])):  # for each neuron\n",
    "            for m in range(len(posX[j+1])):  # for each following neuron\n",
    "                plot_connection_line(ax[0],  # first column of the plot\n",
    "                                     [posX[j][k], posX[j+1][m]], # [X0,X1]\n",
    "                                     [posY[j][k], posY[j+1][m]], # [Y0,Y1]\n",
    "                                     swapped_weights[j][k,m],    # its weight\n",
    "                                     vmax=vmax,  # to get normalized plots\n",
    "                                     linewidth=linewidth)\n",
    "    \n",
    "    #   3: plot neurons\n",
    "    for k in range(len(posX[0])):  # input neurons (have no bias!)\n",
    "        plot_neuron(ax[0], posX[0][k], posY[0][k],\n",
    "                    vmaxB, vmax=vmaxB, size=size)\n",
    "        \n",
    "    for j in range(len(biases)): # all other neurons\n",
    "        for k in range(len(posX[j+1])):\n",
    "            plot_neuron_alpha(ax[0], posX[j+1][k], posY[j+1][k],\n",
    "                              biases[j][k], vmax=vmaxB, size=size)\n",
    "    \n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    # now: the output of the network\n",
    "    img = ax[1].imshow(np.reshape(y_out, [M,M]),\n",
    "                       origin='lower',\n",
    "                       extent=[y0range[0],y0range[1],y1range[0],y1range[1]])\n",
    "    ax[1].set_xlabel(r'$y_0$')\n",
    "    ax[1].set_ylabel(r'$y_1$')\n",
    "    \n",
    "    axins1 = inset_axes(ax[1],\n",
    "                        width=\"40%\",  # width = 50% of parent_bbox width\n",
    "                        height=\"5%\",  # height : 5%\n",
    "                        loc='upper right')\n",
    "\n",
    "    imgmin = np.min(y_out)\n",
    "    imgmax = np.max(y_out)\n",
    "    color_bar = fig.colorbar(img, cax=axins1, orientation=\"horizontal\",\n",
    "                             ticks=np.linspace(imgmin,imgmax,3))\n",
    "    cbxtick_obj = plt.getp(color_bar.ax.axes, 'xticklabels')\n",
    "    plt.setp(cbxtick_obj, color=\"white\")\n",
    "    axins1.xaxis.set_ticks_position(\"bottom\")\n",
    "\n",
    "    ax[1].set_title(activations[0])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No hidden layer NN\n",
    "\n",
    "Let's visualize a simple network (no hidden layer) with different activation function.\n",
    "\n",
    "Notice that no hidden layer means that weight, biases and activations are list of length 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANGE = [-3, 3]  # all tests will be with the same range\n",
    "\n",
    "weights=[ [     # a list of matrices (length 1 in this case)\n",
    "    [0.2, 0.9]  # from 2 input neurons to a single output neuron: 1x2\n",
    "    ] ]\n",
    "\n",
    "biases=[   # a list of vectors (length 1 in this case)\n",
    "    [0.5]  # bias for 1 single output neuron: 1 value\n",
    "    ]\n",
    "\n",
    "visualize_network(weights, biases, ['sigmoid'],\n",
    "                  y0range=RANGE, y1range=RANGE)\n",
    "\n",
    "visualize_network(weights, biases, ['jump'],\n",
    "                  y0range=RANGE, y1range=RANGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep dense NN\n",
    "\n",
    "Let's visualize a NN of 1 hidden layer of 3 neurons using different activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weights = [ [  # a list of matrices (length 2 in this case)\n",
    "        [0.2, 0.9],  # hidden layer of 3 neurons\n",
    "        [-0.5, 0.3], #  -> 3 neurons with 2 inputs: matrix of 3x2\n",
    "        [0.8, -1.3]\n",
    "    ],                 \n",
    "    [ \n",
    "        [-0.3,0.7,0.5] # from 3 hidden neurons to one output: 1x3\n",
    "    ]  ]\n",
    "\n",
    "biases=[  # a list of vectors (length 2 in this case)\n",
    "        [0.1, -0.5, -0.5], # biases of 3 hidden neurons\n",
    "        [-0.2] # bias for output neuron\n",
    "       ]\n",
    "\n",
    "visualize_network(weights, biases, ['sigmoid', 'sigmoid'],\n",
    "                  y0range=RANGE, y1range=RANGE)\n",
    "\n",
    "visualize_network(weights, biases, ['jump', 'jump'],\n",
    "                  y0range=RANGE, y1range=RANGE)\n",
    "\n",
    "visualize_network(weights, biases, ['reLU', 'reLU'],\n",
    "                  y0range=RANGE, y1range=RANGE)\n",
    "\n",
    "visualize_network(weights, biases, ['linear', 'linear'],\n",
    "                  y0range=RANGE, y1range=RANGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More layers and some activation function combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with 5 intermediate neurons, for fun:\n",
    "\n",
    "weights = [ [  # list of matrices\n",
    "                [0.2, 0.9],  # 5x2 \n",
    "                [-0.5, 0.3], \n",
    "                [0.8, -1.3], \n",
    "                [-0.3, -0.9], \n",
    "                [-0.8, -1.2] \n",
    "          ],\n",
    "          [ \n",
    "                [-0.3, 0.7, 0.5, -0.3, 0.4]  # 1x5\n",
    "          ] ]\n",
    "\n",
    "biases = [  # list of vectors\n",
    "                [0.1, -0.5, -0.5, 0.3, 0.2],  # bias for the hidden layer\n",
    "                [0.5]  # bias for the output bias\n",
    "         ]   # bias for the output neuron\n",
    "\n",
    "visualize_network(weights, biases,              \n",
    "                  activations=[ 'jump', 'linear' ],\n",
    "                  y0range=RANGE, y1range=RANGE, M=400)\n",
    "\n",
    "visualize_network(weights, biases,              \n",
    "                  activations=[ 'sigmoid', 'linear' ],\n",
    "                  y0range=RANGE, y1range=RANGE, M=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sharper the sigmoid: scale all weights and biases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor=10.0  # as high as sharpen \n",
    "\n",
    "# this needs np.array(), because you cannot do factor*<python-list>\n",
    "ws = [factor*np.array(matrix) for matrix in weights]\n",
    "bs = [factor*np.array(vector) for vector in biases]\n",
    "\n",
    "visualize_network(ws, bs, activations=[ 'sigmoid', 'linear' ],\n",
    "                  y0range=RANGE, y1range=RANGE, M=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something not random\n",
    "\n",
    "Many superimposed lines can be used to construct arbitrary shapes, with only a single hidden layer.\n",
    "\n",
    "Let's draw a kind of circle by means of several lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 10  # factor to sharpen sigmoid\n",
    "\n",
    "n_lines = 10  # number of lines to draw\n",
    "\n",
    "phi = np.linspace(0, 2*np.pi, n_lines+1)  # Angular variable\n",
    "phi = phi[0:-1]  # the last value is 2pi, which is equivalent to the 0\n",
    "\n",
    "weight_hidden = np.zeros([n_lines, 2])   # comment this shape\n",
    "weight_hidden[:,0] = factor*np.cos(phi)  # x=cos(phi)\n",
    "weight_hidden[:,1] = factor*np.sin(phi)  # y=sin(phi)\n",
    "\n",
    "bias_hidden = np.full(n_lines, factor*(+0.5))  # all neurons acts equally\n",
    "\n",
    "\n",
    "visualize_network(weights=[ \n",
    "                            weight_hidden,           # from input to hidden\n",
    "                            np.full([1,n_lines],1.0) # from hidden to output\n",
    "                          ],\n",
    "                  biases=[ \n",
    "                            bias_hidden,\n",
    "                            [0.0]\n",
    "                         ],\n",
    "                  activations=['sigmoid',  # activation for hidden\n",
    "                               'linear'    # activation for output\n",
    "                              ],\n",
    "                  y0range=RANGE, y1range=RANGE,\n",
    "                  size=30.0, M=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "01_Basics_NeuralNetworksPython.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {
    "height": "326px",
    "width": "580px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "295px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}