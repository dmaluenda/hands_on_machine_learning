{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><sub>This notebook is distributed under the <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\" target=\"_blank\">Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license</a>.</sub></div>\n",
    "<h1>Hands on Machine Learning  <span style=\"font-size:12px;\"><i>by <a href=\"https://webgrec.ub.edu/webpages/000004/cat/dmaluenda.ub.edu.html\" target=\"_blank\">David Maluenda</a></i></span></h1>\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://atenea.upc.edu/course/view.php?id=71605\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/upc_logo_49px.png\" width=\"130\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "  </td>\n",
    "  <td>   <!-- gColab -->\n",
    "    <a href=\"https://colab.research.google.com/github/dmaluenda/hands_on_machine_learning/blob/master/04_TimeSeries.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/colab_logo_32px.png\" />\n",
    "      Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- github -->\n",
    "    <a href=\"https://github.com/dmaluenda/hands_on_machine_learning/blob/master/04_TimeSeries.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/github_logo_32px.png\" />\n",
    "      View source on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- download -->\n",
    "    <a href=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/04_TimeSeries.ipynb\"  target=\"_blank\"\n",
    "          download=\"04_TimeSeries\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/download_logo_32px.png\" />\n",
    "      Download notebook\n",
    "      </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{IV}$. TimeSeries (LSTM and attention)\n",
    "\n",
    "Hands on \"Machine Learning on Classical and Quantum data\" course of\n",
    "[Master in Photonics - PHOTONICS BCN](https://photonics.masters.upc.edu/en/general-information)\n",
    "[[UPC](https://photonics.masters.upc.edu/en) +\n",
    "[UB](https://www.ub.edu/web/ub/en/estudis/oferta_formativa/master_universitari/fitxa/P/M0D0H/index.html?) +\n",
    "[UAB](https://www.uab.cat/en/uab-official-masters-degrees-study-guides/) +\n",
    "[ICFO](https://www.icfo.eu/lang/studies/master-studies)].\n",
    "\n",
    "Tutorial 4\n",
    "\n",
    "This notebook shows how to:\n",
    "- Create a simple RNN using a single LSTM layer\n",
    "- Create a more sophisticated RNN for time-series predictions\n",
    "- Apply a attention block to improve predictions and preserve memory in RNN\n",
    "- Use Batch Normalization\n",
    "- Play with transformers\n",
    "\n",
    "**References**:\n",
    "\n",
    "[1] [Keras](https://keras.io/getting_started/): a deep learning API written in Python.<br>\n",
    "[2] [Tensorflow](https://www.tensorflow.org/api_docs/python/tf): an open source machine learning platform.<br>\n",
    "[3] [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs) post.<br>\n",
    "[4] Keras examples: [Timeseries forecasting for weather prediction](https://keras.io/examples/timeseries/timeseries_weather_forecasting/). <br>\n",
    "[5] [Seq2Seq LSTM with Luong Attention tutorial](https://levelup.gitconnected.com/building-seq2seq-lstm-with-luong-attention-in-keras-for-time-series-forecasting-1ee00958decb) in gitconnected.com. <br>\n",
    "[6] Attention is all you need, [ArXiv](https://arxiv.org/abs/1706.03762).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports:-Basically-numpy,-matplotlib,-tensorflow-and-pandas\" data-toc-modified-id=\"Imports:-Basically-numpy,-matplotlib,-tensorflow-and-pandas-0\"><span class=\"toc-item-num\">0&nbsp;&nbsp;</span>Imports: Basically numpy, matplotlib, tensorflow and pandas</a></span></li><li><span><a href=\"#Basic-LSTM--for-weather-forecasting\" data-toc-modified-id=\"Basic-LSTM--for-weather-forecasting-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Basic LSTM  for weather forecasting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Climate-Data-Time-Series\" data-toc-modified-id=\"Climate-Data-Time-Series-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Climate Data Time-Series</a></span><ul class=\"toc-item\"><li><span><a href=\"#Raw-Data-Visualization\" data-toc-modified-id=\"Raw-Data-Visualization-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Raw Data Visualization</a></span></li></ul></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training-dataset\" data-toc-modified-id=\"Training-dataset-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Training dataset</a></span></li><li><span><a href=\"#Validation-dataset\" data-toc-modified-id=\"Validation-dataset-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Validation dataset</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Prediction\" data-toc-modified-id=\"Prediction-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Prediction</a></span></li></ul></li><li><span><a href=\"#Synthetic-data-for-the-seq2seq-models\" data-toc-modified-id=\"Synthetic-data-for-the-seq2seq-models-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Synthetic data for the seq2seq models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Split-sequences-to-80%-train-set-and-20%-test-set\" data-toc-modified-id=\"Split-sequences-to-80%-train-set-and-20%-test-set-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Split sequences to 80% train set and 20% test set</a></span></li><li><span><a href=\"#Detrending\" data-toc-modified-id=\"Detrending-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Detrending</a></span></li><li><span><a href=\"#Combine-sequences\" data-toc-modified-id=\"Combine-sequences-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Combine sequences</a></span></li><li><span><a href=\"#Normalize\" data-toc-modified-id=\"Normalize-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Normalize</a></span></li><li><span><a href=\"#Truncate\" data-toc-modified-id=\"Truncate-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Truncate</a></span></li></ul></li><li><span><a href=\"#Simple-Seq2Seq-LSTM-Model\" data-toc-modified-id=\"Simple-Seq2Seq-LSTM-Model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Simple Seq2Seq LSTM Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-input-layer\" data-toc-modified-id=\"The-input-layer-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>The input layer</a></span></li><li><span><a href=\"#The-encoder-LSTM\" data-toc-modified-id=\"The-encoder-LSTM-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>The encoder LSTM</a></span></li><li><span><a href=\"#Decoder\" data-toc-modified-id=\"Decoder-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Decoder</a></span></li><li><span><a href=\"#Model-building\" data-toc-modified-id=\"Model-building-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Model building</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Prediction-(evaluation)\" data-toc-modified-id=\"Prediction-(evaluation)-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Prediction (evaluation)</a></span></li></ul></li><li><span><a href=\"#Seq2Seq-Model-with-Luong-Attention\" data-toc-modified-id=\"Seq2Seq-Model-with-Luong-Attention-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Seq2Seq Model with Luong Attention</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Input-layer\" data-toc-modified-id=\"The-Input-layer-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>The Input layer</a></span></li><li><span><a href=\"#The-encoder-LSTM\" data-toc-modified-id=\"The-encoder-LSTM-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>The encoder LSTM</a></span></li><li><span><a href=\"#The-Decoder-LSTM\" data-toc-modified-id=\"The-Decoder-LSTM-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>The Decoder LSTM</a></span></li><li><span><a href=\"#Attention-Layer\" data-toc-modified-id=\"Attention-Layer-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Attention Layer</a></span></li><li><span><a href=\"#Model-building\" data-toc-modified-id=\"Model-building-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Model building</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Prediction-(evaluation)\" data-toc-modified-id=\"Prediction-(evaluation)-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Prediction (evaluation)</a></span></li></ul></li><li><span><a href=\"#Attention-is-all-you-need:-Transformers\" data-toc-modified-id=\"Attention-is-all-you-need:-Transformers-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Attention is all you need: Transformers</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports: Basically numpy, matplotlib, tensorflow and pandas\n",
    "`!pip install pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "\n",
    "# for subplots within subplots:\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# for nice inset colorbars: (approach changed from lecture 1 'Visualization' notebook)\n",
    "from mpl_toolkits.axes_grid1.inset_locator import InsetPosition\n",
    "\n",
    "# for updating display \n",
    "# (very simple animation)\n",
    "from IPython.display import clear_output\n",
    "from time import time, sleep\n",
    "\n",
    "# Set up a random number generator with a fixed seed, so that\n",
    "# running this whole notebook repeatedly should always give\n",
    "# the same result (useful for debugging)\n",
    "rng = np.random.RandomState(23455)\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "# Data analysis\n",
    "import pandas as pd\n",
    "\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBFkg7p3CDBd"
   },
   "source": [
    "## Basic LSTM  for weather forecasting\n",
    "\n",
    "Based on [this keras tutorial](https://keras.io/examples/timeseries/timeseries_weather_forecasting/) by:\n",
    "\n",
    "> **Authors:** [Prabhanshu Attri](https://prabhanshu.com/github), [Yashika Sharma](https://github.com/yashika51), [Kristi Takach](https://github.com/ktakattack), [Falak Shah](https://github.com/falaktheoptimist)<br>\n",
    "**Date created:** 2020/06/23<br>\n",
    "**Last modified:** 2020/07/20<br>\n",
    "**Description:** This notebook demonstrates how to do timeseries forecasting using a LSTM model.\n",
    "\n",
    "### Climate Data Time-Series\n",
    "\n",
    "We will be using Jena Climate dataset recorded by the\n",
    "[Max Planck Institute for Biogeochemistry](https://www.bgc-jena.mpg.de/wetter/).\n",
    "The dataset consists of 14 features such as temperature, pressure, humidity etc, recorded once per\n",
    "10 minutes.\n",
    "\n",
    "**Location**: Weather Station, Max Planck Institute for Biogeochemistry\n",
    "in Jena, Germany\n",
    "\n",
    "**Time-frame Considered**: Jan 10, 2009 - December 31, 2016\n",
    "\n",
    "\n",
    "The table below shows the column names, their value formats, and their description.\n",
    "\n",
    "Index| Features      |Format             |Description\n",
    "-----|---------------|-------------------|-----------------------\n",
    "1    |Date Time      |01.01.2009 00:10:00|Date-time reference\n",
    "2    |p (mbar)       |996.52             |The pascal SI derived unit of pressure used to quantify internal pressure. Meteorological reports typically state atmospheric pressure in millibars.\n",
    "3    |T (degC)       |-8.02              |Temperature in Celsius\n",
    "4    |Tpot (K)       |265.4              |Temperature in Kelvin\n",
    "5    |Tdew (degC)    |-8.9               |Temperature in Celsius relative to humidity. Dew Point is a measure of the absolute amount of water in the air, the DP is the temperature at which the air cannot hold all the moisture in it and water condenses.\n",
    "6    |rh (%)         |93.3               |Relative Humidity is a measure of how saturated the air is with water vapor, the %RH determines the amount of water contained within collection objects.\n",
    "7    |VPmax (mbar)   |3.33               |Saturation vapor pressure\n",
    "8    |VPact (mbar)   |3.11               |Vapor pressure\n",
    "9    |VPdef (mbar)   |0.22               |Vapor pressure deficit\n",
    "10   |sh (g/kg)      |1.94               |Specific humidity\n",
    "11   |H2OC (mmol/mol)|3.12               |Water vapor concentration\n",
    "12   |rho (g/m ** 3) |1307.75            |Airtight\n",
    "13   |wv (m/s)       |1.03               |Wind speed\n",
    "14   |max. wv (m/s)  |1.75               |Maximum wind speed\n",
    "15   |wd (deg)       |152.3              |Wind direction in degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dEQXgI4rCDBe",
    "outputId": "daecd65c-a425-4207-8d9d-ab68e2b45cee"
   },
   "outputs": [],
   "source": [
    "os.makedirs('data', exist_ok=True)  # let's put all auxiliar data in a directory\n",
    "uri = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\"\n",
    "zip_path = keras.utils.get_file(origin=uri, fname=os.getcwd()+\"/data/jena_climate_2009_2016.csv.zip\")\n",
    "zip_file = ZipFile(zip_path)\n",
    "zip_file.extractall(path='data')\n",
    "csv_path = \"data/jena_climate_2009_2016.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k284UDz8CDBh"
   },
   "source": [
    "#### Raw Data Visualization\n",
    "\n",
    "To give us a sense of the data we are working with, each feature has been plotted below.\n",
    "This shows the distinct pattern of each feature over the time period from 2009 to 2016.\n",
    "It also shows where anomalies are present, which will be addressed during normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AZSRltKmCDBh",
    "outputId": "6edf75d5-eb9a-47e1-d817-79b936908e92"
   },
   "outputs": [],
   "source": [
    "titles = [\n",
    "    \"Pressure\",\n",
    "    \"Temperature\",\n",
    "    \"Temperature in Kelvin\",\n",
    "    \"Temperature (dew point)\",\n",
    "    \"Relative Humidity\",\n",
    "    \"Saturation vapor pressure\",\n",
    "    \"Vapor pressure\",\n",
    "    \"Vapor pressure deficit\",\n",
    "    \"Specific humidity\",\n",
    "    \"Water vapor concentration\",\n",
    "    \"Airtight\",\n",
    "    \"Wind speed\",\n",
    "    \"Maximum wind speed\",\n",
    "    \"Wind direction in degrees\",\n",
    "]\n",
    "\n",
    "feature_keys = [\n",
    "    \"p (mbar)\",\n",
    "    \"T (degC)\",\n",
    "    \"Tpot (K)\",\n",
    "    \"Tdew (degC)\",\n",
    "    \"rh (%)\",\n",
    "    \"VPmax (mbar)\",\n",
    "    \"VPact (mbar)\",\n",
    "    \"VPdef (mbar)\",\n",
    "    \"sh (g/kg)\",\n",
    "    \"H2OC (mmol/mol)\",\n",
    "    \"rho (g/m**3)\",\n",
    "    \"wv (m/s)\",\n",
    "    \"max. wv (m/s)\",\n",
    "    \"wd (deg)\",\n",
    "]\n",
    "\n",
    "colors = [\n",
    "    \"blue\",\n",
    "    \"orange\",\n",
    "    \"green\",\n",
    "    \"red\",\n",
    "    \"purple\",\n",
    "    \"brown\",\n",
    "    \"pink\",\n",
    "    \"gray\",\n",
    "    \"olive\",\n",
    "    \"cyan\",\n",
    "]\n",
    "\n",
    "date_time_key = \"Date Time\"\n",
    "\n",
    "\n",
    "def show_raw_visualization(data):\n",
    "    time_data = data[date_time_key]\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=7, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n",
    "    )\n",
    "    for i in range(len(feature_keys)):\n",
    "        key = feature_keys[i]\n",
    "        c = colors[i % (len(colors))]\n",
    "        t_data = data[key]\n",
    "        t_data.index = time_data\n",
    "        t_data.head()\n",
    "        ax = t_data.plot(\n",
    "            ax=axes[i // 2, i % 2],\n",
    "            color=c,\n",
    "            title=\"{} - {}\".format(titles[i], key),\n",
    "            rot=25,\n",
    "        )\n",
    "        ax.legend([titles[i]])\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "show_raw_visualization(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBmvUy4eCDBm"
   },
   "source": [
    "This heat map shows the correlation between different features.\n",
    "\n",
    "Check https://pandas.pydata.org/docs/reference/frame.html#computations-descriptive-stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "ordi_MIzCDBn",
    "outputId": "d7b4bbed-6aaf-4887-ff41-a16f9ba7ecef"
   },
   "outputs": [],
   "source": [
    "\n",
    "def show_heatmap(data):\n",
    "    plt.matshow(data.corr())\n",
    "    plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=90)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.yticks(range(data.shape[1]), data.columns, fontsize=14)\n",
    "\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "    plt.title(\"Feature Correlation Heatmap\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_heatmap(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cCFl-_zCDBo"
   },
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Here we are picking ~300,000 data points for training. Observation is recorded every\n",
    "10 mins, that means 6 times per hour. We will resample one point per hour since no\n",
    "drastic change is expected within 60 minutes. We do this via the `sampling_rate`\n",
    "argument in `timeseries_dataset_from_array` utility.\n",
    "\n",
    "We are tracking data from past 720 timestamps (720/6=120 hours). This data will be\n",
    "used to predict the temperature after 72 timestamps (72/6=12 hours).\n",
    "\n",
    "Since every feature has values with\n",
    "varying ranges, we do normalization to confine feature values to a range of `[0, 1]` before\n",
    "training a neural network.\n",
    "We do this by subtracting the mean and dividing by the standard deviation of each feature.\n",
    "\n",
    "71.5 % of the data will be used to train the model, i.e. 300,693 rows. `split_fraction` can\n",
    "be changed to alter this percentage.\n",
    "\n",
    "The model is shown data for first 5 days i.e. 720 observations, that are sampled every\n",
    "hour. The temperature after 72 (12 hours * 6 observation per hour) observation will be\n",
    "used as a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZZkQVK_CDBo"
   },
   "outputs": [],
   "source": [
    "split_fraction = 0.715\n",
    "train_split = int(split_fraction * int(df.shape[0]))\n",
    "step = 6\n",
    "\n",
    "past = 720\n",
    "future = 72\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "def normalize(data, train_split):\n",
    "    data_mean = data[:train_split].mean(axis=0)\n",
    "    data_std = data[:train_split].std(axis=0)\n",
    "    return (data - data_mean) / data_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLVo5flkCDBp"
   },
   "source": [
    "We can see from the correlation heatmap, few parameters like Relative Humidity and\n",
    "Specific Humidity are redundant. Hence we will be using select features, not all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w80WwVRnCDBp",
    "outputId": "1aa05f8b-bf73-4d5a-ad6c-22f37e608d70"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The selected parameters are:\",\n",
    "    \", \".join([titles[i] for i in [0, 1, 5, 7, 8, 10, 11]]),\n",
    ")\n",
    "selected_features = [feature_keys[i] for i in [0, 1, 5, 7, 8, 10, 11]]\n",
    "features = df[selected_features]\n",
    "features.index = df[date_time_key]\n",
    "features.head()\n",
    "\n",
    "features = normalize(features.values, train_split)\n",
    "features = pd.DataFrame(features)\n",
    "features.head()\n",
    "\n",
    "train_data = features.loc[0 : train_split - 1]\n",
    "val_data = features.loc[train_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0Bl7xxtCDBq"
   },
   "source": [
    "#### Training dataset\n",
    "\n",
    "The training dataset labels starts from the 792nd observation (720 + 72)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpJC8peKCDBq"
   },
   "outputs": [],
   "source": [
    "start = past + future\n",
    "end = start + train_split\n",
    "\n",
    "x_train = train_data[[i for i in range(7)]].values\n",
    "y_train = features.iloc[start:end][[1]]\n",
    "\n",
    "sequence_length = int(past / step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhDTyD1zCDBq"
   },
   "source": [
    "The `timeseries_dataset_from_array` function takes in a sequence of data-points gathered at\n",
    "equal intervals, along with time series parameters such as length of the\n",
    "sequences/windows, spacing between two sequence/windows, etc., to produce batches of\n",
    "sub-timeseries inputs and targets sampled from the main timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXj1fBJiCDBr"
   },
   "outputs": [],
   "source": [
    "dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ1zu5IqCDBr"
   },
   "source": [
    "#### Validation dataset\n",
    "\n",
    "The validation dataset must not contain the last 792 rows as we won't have label data for\n",
    "those records, hence 792 must be subtracted from the end of the data.\n",
    "\n",
    "The validation label dataset must start from 792 after train_split, hence we must add\n",
    "past + future (792) to label_start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Otn3tDuYCDBr",
    "outputId": "6388ca71-1cb3-4af9-e7dc-43f3a188ec44"
   },
   "outputs": [],
   "source": [
    "x_end = len(val_data) - past - future\n",
    "\n",
    "label_start = train_split + past + future\n",
    "\n",
    "x_val = val_data.iloc[:x_end][[i for i in range(7)]].values\n",
    "y_val = features.iloc[label_start:][[1]]\n",
    "\n",
    "dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_val,\n",
    "    y_val,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "\n",
    "print(\"Input shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQwxfj7hCDBs"
   },
   "source": [
    "### Training\n",
    "\n",
    "Check https://stackoverflow.com/questions/44273249/in-keras-what-exactly-am-i-configuring-when-i-create-a-stateful-lstm-layer-wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tlyhiSJCDBs",
    "outputId": "8a362521-b135-445e-b691-46a978cdf97e"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "lstm_out = keras.layers.LSTM(32)(inputs)\n",
    "outputs = keras.layers.Dense(1)(lstm_out)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iz8rnXiCDBs"
   },
   "source": [
    "We'll use the `ModelCheckpoint` callback to regularly save checkpoints, and\n",
    "the `EarlyStopping` callback to interrupt training when the validation loss\n",
    "is not longer improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CT1JJ1fRCDBt",
    "outputId": "fd0e7296-1b9c-42fb-db73-2066fbc6c3d1"
   },
   "outputs": [],
   "source": [
    "path_checkpoint = \"data/LSTM_model_checkpoint.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=1,  # Let's put just 1 epoch to save time...\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback, modelckpt_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQAlVUnKCDBt"
   },
   "source": [
    "We can visualize the loss with the function below. After one point, the loss stops\n",
    "decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "h1ZfjJ7mCDBu",
    "outputId": "78fe754e-5ed8-45e2-9da9-b76062f27b11"
   },
   "outputs": [],
   "source": [
    "\n",
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_loss(history, \"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fzx7rdSOCDBu"
   },
   "source": [
    "### Prediction\n",
    "\n",
    "The trained model above is now able to make predictions for 5 sets of values from\n",
    "validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bJhaP0rFCDBu",
    "outputId": "877ddec9-259e-456d-9780-a8fec207eb7e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "for (x, y), feat in zip(dataset_val.take(5), selected_features):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        12,\n",
    "        f\"Single Step Prediction ({feat})\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data for the seq2seq models\n",
    "\n",
    "*Code based on [this example](https://levelup.gitconnected.com/building-seq2seq-lstm-with-luong-attention-in-keras-for-time-series-forecasting-1ee00958decb)*.\n",
    "\n",
    "More info regarding attention [here](https://ai.plainenglish.io/introduction-to-attention-mechanism-bahdanau-and-luong-attention-e2efd6ce22da).\n",
    "\n",
    "In the next two sections, we are going to build two Seq2Seq Models in Keras, the simple Seq2Seq LSTM Model, and the Seq2Seq LSTM Model with Luong Attention, and compare their forecasting accuracy.\n",
    "\n",
    "\n",
    "First of all, let’s create some time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ = 1000\n",
    "t = np.linspace(0, 50*np.pi, n_)\n",
    "# pattern + trend + noise\n",
    "x1 = sum([20*np.sin(i*t+np.pi) for i in range(5)]) + 0.01*(t**2) + np.random.normal(0, 6, n_)\n",
    "x2 = sum([15*np.sin(2*i*t+np.pi) for i in range(5)]) + 0.5*t + np.random.normal(0, 6, n_)\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.plot(range(len(x1)), x1, label='x1')\n",
    "plt.plot(range(len(x2)), x2, label='x2')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve just created two sequences, x1 and x2, by combining sin waves, trend, and random noise. Next we will preprocess x1 and x2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split sequences to 80% train set and 20% test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "train_len = int(train_ratio * t.shape[0])\n",
    "print(train_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the sequence length is n_ = 1000, the first 800 data points will be used as our train data, while the rest will be used as our test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detrending\n",
    "\n",
    "It is not a must to detrend time series. However stationary time series will make model training much easier. There are many ways to detrend time series, such as taking difference of sequence with its lag1. Here for the simplicity, we assume the order of trend is known and we are just going to simply fit separate trend lines to x1 and x2, and then subtract the trend from the corresponding original sequence.\n",
    "\n",
    "We will create index number of each position in the sequence, for easier detrending and trend recover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_index = np.array(range(len(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use np.polyfit to complete this small task. Note that only the first 800 data points are used to fit the trend lines, this is because we want to avoid data leak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_trend_param = np.polyfit(x_index[:train_len], x1[:train_len], 2)\n",
    "x2_trend_param = np.polyfit(x_index[:train_len], x2[:train_len], 1)\n",
    "print(x1_trend_param)\n",
    "print(x2_trend_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above values we got, we can now come up with the trend lines for x1 and x2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_trend = (x_index**2)*x1_trend_param[0]+x_index*x1_trend_param[1]+x1_trend_param[2]\n",
    "x2_trend = x_index*x2_trend_param[0]+x2_trend_param[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s plot the trend lines together with x1 and x2, and check if they look good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "plt.plot(range(len(x1)), x1, label='x1')\n",
    "plt.plot(range(len(x1_trend)), x1_trend, linestyle='--', label='x1_trend')\n",
    "plt.plot(range(len(x2)), x2, label='x2')\n",
    "plt.plot(range(len(x2_trend)), x2_trend, linestyle='--', label='x2_trend')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result looks fine, now we can deduct the trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_detrend = x1 - x1_trend\n",
    "x2_detrend = x2 - x2_trend\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.plot(range(len(x1_detrend)), x1_detrend, label='x2_detrend')\n",
    "plt.plot(range(len(x2_detrend)), x2_detrend, label='x2_detrend')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the trend, x1 and x2 become stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine sequences\n",
    "\n",
    "For easier preprocessing in next several steps, we can combine the sequences and their relevant information together into one array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lbl = np.column_stack([x1_detrend, x2_detrend, x_index, [1]*train_len+[0]*(len(x_index)-train_len)])\n",
    "print(x_lbl.shape)\n",
    "print(x_lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the combined array we created x_lbl:\n",
    " - the first column is the detrended x1\n",
    " - the second column is the detrended x2\n",
    " - the third column is the index\n",
    " - the fourth column is the label (1 for train set and 0 for test set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize\n",
    "\n",
    "Normalisation can help model avoid favouring large features while ignoring very small features. Here we can just simply normalise x1 and x2 by dividing the corresponding maximum values in train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_max = x_lbl[x_lbl[:, 3]==1, :2].max(axis=0)\n",
    "x_train_max = x_train_max.tolist()+[1]*2  # only normalize for the first 2 columns\n",
    "print(x_train_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above code only calculates maximum value of column 1 (detrended x1) and column 2 (detrended x2), the denominator of column 3 (index) and column 4 (label) are set to 1. This is because we do not input column 3 and column 4 into neural network, and hence no need to normalise them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_normalize = np.divide(x_lbl, x_train_max)\n",
    "print(x_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "plt.plot(range(train_len), x_normalize[:train_len, 0], label='x1_train_normalized')\n",
    "plt.plot(range(train_len), x_normalize[:train_len, 1], label='x2_train_normalized')\n",
    "plt.plot(range(train_len, len(x_normalize)), x_normalize[train_len:, 0], label='x1_test_normalized')\n",
    "plt.plot(range(train_len, len(x_normalize)), x_normalize[train_len:, 1], label='x2_test_normalized')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After normalisation, all the values are more or less within range from -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncate\n",
    "\n",
    "Next, we will cut sequence into smaller pieces by sliding an input window (length = 200 time steps) and an output window (length = 20 time steps), and put these samples in 3d numpy arrays.\n",
    "\n",
    "![](resources/04_fig1.png \"dsfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(x, feature_cols=range(3), target_cols=range(3), label_col=3, train_len=100, test_len=20):\n",
    "    in_, out_, lbl = [], [], []\n",
    "    for i in range(len(x)-train_len-test_len+1):\n",
    "        in_.append(x[i:(i+train_len), feature_cols].tolist())\n",
    "        out_.append(x[(i+train_len):(i+train_len+test_len), target_cols].tolist())\n",
    "        lbl.append(x[i+train_len, label_col])\n",
    "    return np.array(in_), np.array(out_), np.array(lbl)\n",
    "X_in, X_out, lbl = truncate(x_normalize, feature_cols=range(3), target_cols=range(3), \n",
    "                            label_col=3, train_len=200, test_len=20)\n",
    "print(X_in.shape, X_out.shape, lbl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function truncate generates 3 arrays:\n",
    " - input to neural network X_in: it contains 781 samples, length of each sample is 200 time steps, and each sample contains 3 features: detrended and normalised x1, detrended and normalised x2, and original assigned data position index. Only the first 2 features will be used for training.\n",
    " - target in neural network X_out: it contains 781 samples, length of each sample is 20 time steps, and each sample contains the same 3 features as in X_in. Only the first 2 features will be used as target, and the third feature will only be used to recover trend of the prediction.\n",
    " - label lbl: 1 for train set and 0 for test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input_train = X_in[np.where(lbl==1)]\n",
    "X_output_train = X_out[np.where(lbl==1)]\n",
    "X_input_test = X_in[np.where(lbl==0)]\n",
    "X_output_test = X_out[np.where(lbl==0)]\n",
    "print(X_input_train.shape, X_output_train.shape)\n",
    "print(X_input_test.shape, X_output_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is ready to be input into neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Seq2Seq LSTM Model\n",
    "\n",
    "![04_fig2](resources/04_fig2.png)\n",
    "\n",
    "The above figure represents unfolded single layer of Seq2Seq LSTM model:\n",
    " - The encoder LSTM cell: The value of each time step is input into the encoder LSTM cell together with previous cell state c and hidden state h, the process repeats until the last cell state c and hidden state h are generated.\n",
    " - The decoder LSTM cell: We use the last cell state c and hidden state h from the encoder as the initial states of the decoder LSTM cell. The last hidden state of encoder is also copied 20 times, and each copy is input into the decoder LSTM cell together with previous cell state c and hidden state h. The decoder outputs hidden state for all the 20 time steps, and these hidden states are connected to a dense layer to output the final result.\n",
    "\n",
    "Set number of hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layers():\n",
    "    in_train = layers.Input(shape=(X_input_train.shape[1], X_input_train.shape[2]-1))\n",
    "    out_train = layers.Input(shape=(X_output_train.shape[1], X_output_train.shape[2]-1))\n",
    "   \n",
    "    return in_train, out_train\n",
    "\n",
    "seq2seq_input_train, seq2seq_output_train = get_input_layers()\n",
    "\n",
    "print(seq2seq_input_train)\n",
    "print(seq2seq_output_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The encoder LSTM\n",
    "\n",
    "We need to pay attention to 2 import parameters return_sequences and return_state, because they decide what LSTM returns.\n",
    " - return_sequences=False, return_state=False: return the last hidden state: state_h\n",
    " - return_sequences=True, return_state=False: return stacked hidden states (num_timesteps * num_cells): one hidden state output for each input time step\n",
    " - return_sequences=False, return_state=True: return 3 arrays: state_h, state_h, state_c\n",
    " - return_sequences=True, return_state=True: return 3 arrays: stacked hidden states, last state_h, last state_c\n",
    "\n",
    "For simple Seq2Seq model, we only need last state_h and last state_c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(in_train, ret_seq=True, ret_st=True):\n",
    "    out1, out2, out3 = layers.LSTM(n_hidden, activation='elu', dropout=0.2, \n",
    "                                   recurrent_dropout=0.2, \n",
    "                                   return_sequences=ret_seq, \n",
    "                                   return_state=ret_st)(in_train)\n",
    "    \n",
    "    print('out1:', out1)\n",
    "    print('out2:', out2)\n",
    "    print('out3:', out3)\n",
    "    \n",
    "    batch1_in = out2 if ret_seq else out1\n",
    "    \n",
    "    encoder_last_h = layers.BatchNormalization(momentum=0.6)(batch1_in)\n",
    "    encoder_last_c = layers.BatchNormalization(momentum=0.6)(out3)\n",
    "\n",
    "    return encoder_last_h, encoder_last_c, out1\n",
    "\n",
    "\n",
    "seq2seq_encoder_last_h, seq2seq_encoder_last_c, _ = get_encoder(seq2seq_input_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalisation is added because we want to avoid gradient explosion caused by the activation function ELU in the encoder.\n",
    "\n",
    "More about Batch Normalization at https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Next, we make 20 copies of the last hidden state of encoder and use them as input to the decoder. The last cell state and the last hidden state of the encoder are also used as the initial states of decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder(output_train, encoder_h, encoder_c):\n",
    "    decoder = layers.RepeatVector(output_train.shape[1])(encoder_h)\n",
    "    decoder = layers.LSTM(n_hidden, activation='elu', dropout=0.2, \n",
    "                          recurrent_dropout=0.2, return_state=False, \n",
    "                          return_sequences=True)(decoder, \n",
    "                                                 initial_state=[encoder_h, \n",
    "                                                                encoder_c])\n",
    "    print('decoder:', decoder)\n",
    "    return decoder\n",
    "\n",
    "seq2seq_decoder = get_decoder(seq2seq_output_train, \n",
    "                              seq2seq_encoder_last_h, \n",
    "                              seq2seq_encoder_last_c)\n",
    "\n",
    "seq2seq_out = layers.TimeDistributed(layers.Dense(seq2seq_output_train.shape[2]))(seq2seq_decoder)\n",
    "print('out:', seq2seq_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building\n",
    "\n",
    "Then we put everything into the model, and compile it. Here we simply use MSE as the loss function and MAE as the evaluation metric. Note that we set clipnorm=1 for Adam optimiser. This is to normalise the gradient, so as to avoid gradient explosion during back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model = Model(inputs=seq2seq_input_train, outputs=seq2seq_out)\n",
    "seq2seq_opt = optimizers.Adam(learning_rate=0.01, clipnorm=1)\n",
    "seq2seq_model.compile(loss='mean_squared_error', optimizer=seq2seq_opt, metrics=['mae'])\n",
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(seq2seq_model, to_file='data/seq2seq_model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epc = 50\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "history = seq2seq_model.fit(X_input_train[:, :, :2], X_output_train[:, :, :2], \n",
    "                            validation_split=0.2, epochs=epc, verbose=1, \n",
    "                            callbacks=[es], batch_size=100)\n",
    "seq2seq_train_mae = history.history['mae']\n",
    "seq2seq_valid_mae = history.history['val_mae']\n",
    " \n",
    "seq2seq_model.save('data/model_forecasting_seq2seq.h5')\n",
    "\n",
    "plt.plot(seq2seq_train_mae, label='train mae'), \n",
    "plt.plot(seq2seq_valid_mae, label='validation mae')\n",
    "plt.ylabel('mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('train vs. validation accuracy (mae)')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, \n",
    "           shadow=False, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction (evaluation)\n",
    "\n",
    "1. The model prediction as well as the true values are unnormalised:\n",
    "1. Then we combine the unnormalised outputs with their corresponding index, so that we can recover the trend.\n",
    "1. Next, we put all the outputs with recovered trend into a dictionary data_final.\n",
    "\n",
    "Let's put all this in a function to be used in the next model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_predictions(model):\n",
    "    # 1.\n",
    "    train_pred_detrend = model.predict(X_input_train[:, :, :2])*x_train_max[:2]\n",
    "    test_pred_detrend = model.predict(X_input_test[:, :, :2])*x_train_max[:2]\n",
    "    print(train_pred_detrend.shape, test_pred_detrend.shape)\n",
    "\n",
    "    train_true_detrend = X_output_train[:, :, :2]*x_train_max[:2]\n",
    "    test_true_detrend = X_output_test[:, :, :2]*x_train_max[:2]\n",
    "    print(train_true_detrend.shape, test_true_detrend.shape)\n",
    "    \n",
    "    # 2.\n",
    "    train_pred_detrend = np.concatenate([train_pred_detrend, \n",
    "                                         np.expand_dims(X_output_train[:, :, 2], \n",
    "                                                        axis=2)\n",
    "                                        ], axis=2)\n",
    "    test_pred_detrend = np.concatenate([test_pred_detrend, \n",
    "                                        np.expand_dims(X_output_test[:, :, 2], \n",
    "                                                       axis=2)\n",
    "                                       ], axis=2)\n",
    "    print(train_pred_detrend.shape, test_pred_detrend.shape)\n",
    "\n",
    "    train_true_detrend = np.concatenate([train_true_detrend, \n",
    "                                         np.expand_dims(X_output_train[:, :, 2], \n",
    "                                                        axis=2)\n",
    "                                        ], axis=2)\n",
    "    test_true_detrend = np.concatenate([test_true_detrend, \n",
    "                                        np.expand_dims(X_output_test[:, :, 2], \n",
    "                                                       axis=2)\n",
    "                                       ], axis=2)\n",
    "    print(train_true_detrend.shape, test_true_detrend.shape)\n",
    "    \n",
    "    # 3.\n",
    "    data_final = dict()\n",
    "    for dt, lb in zip([train_pred_detrend, train_true_detrend, \n",
    "                       test_pred_detrend, test_true_detrend], \n",
    "                      ['train_pred', 'train_true', 'test_pred', 'test_true']):\n",
    "        dt_x1 = dt[:, :, 0] + (dt[:, :, 2]**2)*x1_trend_param[0] + dt[:, :, 2]*x1_trend_param[1] + x1_trend_param[2]\n",
    "        dt_x2 = dt[:, :, 1] + dt[:, :, 2]*x2_trend_param[0] + x2_trend_param[1]\n",
    "        data_final[lb] = np.concatenate(\n",
    "            [np.expand_dims(dt_x1, axis=2), np.expand_dims(dt_x2, axis=2)], axis=2)\n",
    "        print(lb+': {}'.format(data_final[lb].shape))\n",
    "        \n",
    "    return data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_data_final = check_predictions(seq2seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a quick check to see if the prediction value distribution is reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lb in ['train', 'test']:\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.hist(seq2seq_data_final[lb+'_pred'].flatten(), bins=100, color='orange', \n",
    "             alpha=0.5, label=lb+' pred')\n",
    "    plt.hist(seq2seq_data_final[lb+'_true'].flatten(), bins=100, color='green', \n",
    "             alpha=0.5, label=lb+' true')\n",
    "    plt.legend()\n",
    "    plt.title('value distribution: '+lb)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data distribution of prediction and true values are almost overlapped, so we are good.\n",
    "\n",
    "We can also plot MAE of all samples in time order, to see if there is clear pattern. The ideal situation is when line is random, otherwise it may indicate that the model is not sufficiently trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lb in ['train', 'test']:\n",
    "    MAE_overall = abs(seq2seq_data_final[lb+'_pred'] - seq2seq_data_final[lb+'_true']).mean()\n",
    "    MAE_ = abs(seq2seq_data_final[lb+'_pred'] - seq2seq_data_final[lb+'_true']).mean(axis=(1, 2))\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.plot(MAE_)\n",
    "    plt.title('MAE '+lb+': overall MAE = '+str(MAE_overall))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above plots, we can say that there are still certain periodical pattens in both train and test MAE. Training for more epochs may lead to better results.\n",
    "\n",
    "Next we are going to check some random samples and see if the predicted lines and corresponding true lines are aligned. (184th and 37th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the nth prediction of each time step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ith_timestep = random.choice(range(seq2seq_data_final[lb+'_pred'].shape[1]))\n",
    "plt.figure(figsize=(15, 5))\n",
    "train_start_t = 0\n",
    "test_start_t = seq2seq_data_final['train_pred'].shape[0]\n",
    "for lb, tm, clrs in zip(['train', 'test'], [train_start_t, test_start_t], \n",
    "                        [['green', 'red'], ['blue', 'orange']]):\n",
    "    for i, x_lbl in zip([0, 1], ['x1', 'x2']):\n",
    "        plt.plot(range(tm, tm+seq2seq_data_final[lb+'_pred'].shape[0]), \n",
    "                 seq2seq_data_final[lb+'_pred'][:, ith_timestep, i], \n",
    "                 linestyle='--', linewidth=1, color=clrs[0], label='pred '+x_lbl)\n",
    "        plt.plot(range(tm, tm+seq2seq_data_final[lb+'_pred'].shape[0]), \n",
    "                 seq2seq_data_final[lb+'_true'][:, ith_timestep, i], \n",
    "                 linestyle='-', linewidth=1, color=clrs[1], label='true '+x_lbl)\n",
    "    \n",
    "plt.title('{}th time step in all samples'.format(ith_timestep))\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a closer look at the prediction on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = 'test'\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, x_lbl, clr in zip([0, 1], ['x1', 'x2'], ['green', 'blue']):\n",
    "    plt.plot(seq2seq_data_final[lb+'_pred'][:, ith_timestep, i], \n",
    "             linestyle='--', color=clr, label='pred '+x_lbl)\n",
    "    plt.plot(seq2seq_data_final[lb+'_true'][:, ith_timestep, i], \n",
    "             linestyle='-', color=clr, label='true '+x_lbl)\n",
    "\n",
    "plt.title('({}): {}th time step in all samples'.format(lb, ith_timestep))\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Model with Luong Attention\n",
    "\n",
    "![](resources/04_fig3.png \"\")\n",
    "\n",
    "One of the limitations of simple Seq2Seq model is: only the last state of encoder RNN is used as input to decoder RNN. If the sequence is very long, the encoder will tend to have much weaker memory about earlier time steps. Attention mechanism can solve this problem. An attention layer is going to assign proper weight to each hidden state output from encoder, and map them to output sequence.\n",
    "\n",
    "Next we will build Luong Attention on top of Model 1, and use Dot method to calculate alignment score.\n",
    "\n",
    "More info regarding attention [here](https://ai.plainenglish.io/introduction-to-attention-mechanism-bahdanau-and-luong-attention-e2efd6ce22da).\n",
    "\n",
    "### The Input layer\n",
    "\n",
    "It is the same as in Model 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 100\n",
    "luong_input_train = layers.Input(shape=(X_input_train.shape[1], X_input_train.shape[2]-1))\n",
    "luong_output_train = layers.Input(shape=(X_output_train.shape[1], X_output_train.shape[2]-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The encoder LSTM\n",
    "This is slightly different from Model 1: besides returning the last hidden state and the last cell state, we also need to return the stacked hidden states for alignment score calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luong_encoder_h, luong_encoder_c, luong_encoder_stack_h = get_encoder(luong_input_train, ret_seq=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder LSTM\n",
    "Next, we repeat the last hidden state of encoder 20 times, and use them as input to decoder LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luong_decoder_stack_h = get_decoder(luong_output_train, luong_encoder_h, luong_encoder_c)\n",
    "print(luong_decoder_stack_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layer\n",
    "To build the attention layer, the first thing to do is to calculate the alignment score, and apply softmax activation function over it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luong_attention = layers.dot([luong_decoder_stack_h, luong_encoder_stack_h], axes=[2, 2])\n",
    "luong_attention = layers.Activation('softmax')(luong_attention)\n",
    "print(luong_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can calculate the context vector, and also apply batch normalisation on top of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luong_context = layers.dot([luong_attention, luong_encoder_stack_h], axes=[2,1])\n",
    "luong_context = layers.BatchNormalization(momentum=0.6)(luong_context)\n",
    "print(luong_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we concat the context vector and stacked hidden states of decoder, and use it as input to the last dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_combined_context = layers.concatenate([luong_context, luong_decoder_stack_h])\n",
    "print(decoder_combined_context)\n",
    "\n",
    "luong_out = layers.TimeDistributed(layers.Dense(luong_output_train.shape[2]))(decoder_combined_context)\n",
    "print(luong_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building\n",
    "\n",
    "Then we can compile the model. The parameters are the same as those in Model 1, for the sake of comparing the performance of the 2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "luong_model = Model(inputs=luong_input_train, outputs=luong_out)\n",
    "luong_opt = optimizers.Adam(learning_rate=0.01, clipnorm=1)\n",
    "luong_model.compile(loss='mean_squared_error', optimizer=luong_opt, metrics=['mae'])\n",
    "luong_model.summary()\n",
    "plot_model(luong_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epc = 50\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "history = luong_model.fit(X_input_train[:, :, :2], X_output_train[:, :, :2], validation_split=0.2, \n",
    "                    epochs=epc, verbose=1, callbacks=[es], \n",
    "                    batch_size=100)\n",
    "luong_train_mae = history.history['mae']\n",
    "luong_valid_mae = history.history['val_mae']\n",
    " \n",
    "luong_model.save('data/luong_model_forecasting_seq2seq.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and evaluation process is the same as illustrated in Model 1. After training for 100 epochs (the same number of training epochs as Model 1), we can evaluate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(luong_train_mae, label='train mae'), \n",
    "plt.plot(luong_valid_mae, label='validation mae')\n",
    "plt.ylabel('mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('train vs. validation accuracy (mae)')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, \n",
    "           shadow=False, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction (evaluation)\n",
    "\n",
    "Below are the plots of sample MAE vs. sample order for train set and test set. Again, the model is not sufficiently trained since we can still see some periodical pattern. But for easier comparison of the 2 models, we are not going to train it more for now. Note that the overall MAE of both train set and test set are slightly improved compare to Model 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luong_data_final = check_predictions(luong_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lb in ['train', 'test']:\n",
    "    s2s_MAE_overall = abs(seq2seq_data_final[lb+'_pred'] - seq2seq_data_final[lb+'_true']).mean()\n",
    "    s2s_MAE_ = abs(seq2seq_data_final[lb+'_pred'] - seq2seq_data_final[lb+'_true']).mean(axis=(1, 2))\n",
    "    luo_MAE_overall = abs(luong_data_final[lb+'_pred'] - luong_data_final[lb+'_true']).mean()\n",
    "    luo_MAE_ = abs(luong_data_final[lb+'_pred'] - luong_data_final[lb+'_true']).mean(axis=(1, 2))\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.plot(luo_MAE_)\n",
    "    plt.plot(s2s_MAE_, ':')\n",
    "    plt.title('MAE '+lb+': overall MAE = '+str(luo_MAE_overall)+\n",
    "              '  (seq2seq: '+str(s2s_MAE_overall)+')')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = 'test'\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, x_lbl, clr in zip([0, 1], ['x1', 'x2'], ['green', 'blue']):\n",
    "    plt.plot(luong_data_final[lb+'_pred'][:, ith_timestep, i], linestyle='--', color=clr, label='pred '+x_lbl)\n",
    "    plt.plot(luong_data_final[lb+'_true'][:, ith_timestep, i], linestyle='-', color=clr, label='true '+x_lbl)\n",
    "    plt.plot(seq2seq_data_final[lb+'_pred'][:, ith_timestep, i], linestyle=':', color=clr, label='pred (seq2seq)'+x_lbl)\n",
    "plt.title('({}): {}th time step in all samples'.format(lb, ith_timestep))\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention is all you need: Transformers\n",
    "\n",
    "Check the transformers paper: https://arxiv.org/abs/1706.03762\n",
    "\n",
    "Check a brief explanation of them: https://medium.com/@max_garber/simple-keras-transformer-model-74724a83bb83\n",
    "\n",
    "Play with the biggest transformer model by Open IA (registration needed): https://beta.openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
