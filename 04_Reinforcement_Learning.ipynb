{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"text-align: right\"><sub>This notebook is distributed under the <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\" target=\"_blank\">Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license</a>.</sub></div>\n",
    "<h1>Hands on Machine Learning  <span style=\"font-size:12px;\"><i>by <a href=\"https://webgrec.ub.edu/webpages/000004/cat/dmaluenda.ub.edu.html\" target=\"_blank\">David Maluenda</a></i></span></h1>\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://atenea.upc.edu/course/view.php?id=95161\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/upc_logo_49px.png\" width=\"130\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "  </td>\n",
    "  <td>   <!-- gColab -->\n",
    "    <a href=\"https://colab.research.google.com/github/dmaluenda/hands_on_machine_learning/blob/master/04_Reinforcement_Learning.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/colab_logo_32px.png\" />\n",
    "      Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- github -->\n",
    "    <a href=\"https://github.com/dmaluenda/hands_on_machine_learning/blob/master/04_Reinforcement_Learning.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/github_logo_32px.png\" />\n",
    "      View source on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- download -->\n",
    "    <a href=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/04_Reinforcement_Learning.ipynb\"  target=\"_blank\"\n",
    "          download=\"04_Reinforcement_Learning\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/download_logo_32px.png\" />\n",
    "      Download notebook\n",
    "      </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# $\\text{IV}$. Reinforcement Learning\n",
    "\n",
    "Hands on \"Machine Learning on Classical and Quantum data\" course of\n",
    "[Master in Photonics - PHOTONICS BCN](https://photonics.masters.upc.edu/en/general-information)\n",
    "[[UPC](https://photonics.masters.upc.edu/en) +\n",
    "[UB](https://www.ub.edu/web/ub/en/estudis/oferta_formativa/master_universitari/fitxa/P/M0D0H/index.html?) +\n",
    "[UAB](https://www.uab.cat/en/uab-official-masters-degrees-study-guides/) +\n",
    "[ICFO](https://www.icfo.eu/lang/studies/master-studies)].\n",
    "\n",
    "**Tutorial 4**\n",
    "\n",
    "This notebook shows how to:\n",
    "- solve a challenge with reinforcement learning\n",
    "- implement the Actor-Critic method\n",
    "- use a Gradient Tape to train non-conventional loss functions\n",
    "\n",
    "**References**:\n",
    "\n",
    "[1] [Keras](https://keras.io/getting_started/): a deep learning API written in Python.<br>\n",
    "[2] [Keras examples](https://keras.io/examples/rl/).<br>\n",
    "[3] Gym, a toolkit for developing and comparing reinforcement learning algorithms. By [OpenAI](https://gym.openai.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0. Imports: NumPy, Matplotlib, Tensorflow and <u>gymnasium</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will use the [Gymnasium](https://gymnasium.farama.org/) module to solve a challenging game. You have to install it before, if you do not already have it, just by run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium\n",
    "!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from time import sleep, time\n",
    "\n",
    "# For simple animation\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. CartPole game from Farama Foundation gymnasium\n",
    "\n",
    "We will use the CartPole game. Documentation: https://gymnasium.farama.org/environments/classic_control/cart_pole/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will draw a Figure with a cart and an upstanding pole. This is just a window where to show the current state of the game. Check the two printed lines and the last code line to get familiar with the environ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAACvCAYAAADZsbM+AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAABT1JREFUeJzt3U+LFHcex/FvdXTMqj0TBpKVwbkZyF6WxSwIPolcJPd9EoGcgs/AW+5ek2ewJ4+eVlgwEoSsEXv1kuj0OH92xq5QjWNQMjru1HQP83m9ZBisboovyNtf1a8Lumnbti0gwmDeAwCzI3gIIngIIngIIngIIngIIngIIngIcuogb5pMJjUajWo4HFbTNEc/FfBeuufnxuNxrays1GAwOFzwXeyrq6vvNwEwcw8fPqyLFy8eLvhuZd872eLiYn/TAb1YW1ubLsp7rR4q+L3L+C52wcPx9a5bbpt2EETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEETwEOTUvAdgtl7sbNezn/9d7eTFyyNNLV78S53+03DOkzELgg+zu7VeP926WZOdrZdHmvrsi68EH8IlPVXtZN4TMCOCp6pt5z0BMyJ4qi3BpxA8VvgggqdawccQPFb4IIKnWrv0MQTPdNuODILHJX0QwWPTLojg8aRdEMHjwZsggsc9fBDBI/gggsemXRDBx2tt2gURPFb4IILHk3ZBBI9NuyCCxyV9EMFjhQ8ieKotu/QpBJ+maappmtcOtRPBpxB8mA9Of1gL55dfO7b162hu8zBbgg/Tre7N4IPXjv3+LTScdIJP9MYlPTkEH6eZ/iGT4NN0rVvhYwk+s/h5D8GcCD6R3mMJPpB7+FyCT9Pdv7uHjyX4MNPUBR9L8IFc0ucSfByX9MkEn3lNP+8pmBPBx7HCJxN8ILnnEnwiK3wswafxOXw0wUc+SS/4VKfmPQD9297ers3NzT98rW0ntbO7+8b7/1dPnz7d93xnz56thYWF3udk9gR/At28ebOuX7/+h691V/Nff/l5/f3TP7869v3339W3//hm3/PduHGjrl27diSzMluCP4HW19fr0aNH+wa/vvHXur95uX7ZWamPTz+o8fqP+76/s7GxcYTTMkuCT9NW3d/4W21tXJ5u4fy6c6EebP1QVbfmPRkzYNMuTPeVE+Pdj17907c1qOcvur+TQPCBLizcr1PN9jT3hWazLiz8NO+ROI6X9Pfu3avz588f3TT04smTJ299/V93/llLPz+op7uf1PLpx/X4v/fe+v7RaFR3797teUr63rfpPfhnz57V7hsf6XD87PeR3J5bd/5TVd3PwXSbdm/72I75e/78ef/BX7lypRYXF//fmZiR27dv93q+S5cu1dWrV3s9J/1aW1s70Pvcw0MQwUMQwUMQD96cQGfOnOl1r8Vz9CdH07Zt9yzGOzcElpaWprv0Nu2Ov/F43Ouu+vLycp07d66389G/gzZqhT+BhsPh9Afe5B4egggegggegggegggegggegggegggegggegggegggegggegggegggegggegggegggegggegggegggegggegggegggegggegggegggegggegggegggegggeghzo66L3vkK++w5q4PjZa3Ov1UMFPx6Pp79XV1f7mA04Il2rS0tL+77etO/6L6GqJpNJjUajGg6H1TRN3zMCh9Rl3MW+srJSg8HgcMEDJ4NNOwgieAgieAgieAgieAgieAgieKgcvwFeTSDl3zM00AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def render(env, rewards=None):\n",
    "    \"\"\" Let's render the environ in a inline Figure, instead of a dedicated window.\n",
    "        To do so, we need to set the environ mode to 'rgb_array' and imshow the \n",
    "        rendered image. To do not acumulate many figures, a clear_ouput is launch first.\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.title(f\"Total rewards: {rewards}\") if rewards else None\n",
    "    plt.imshow(env.render())\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "env = gym.make(\"CartPole-v0\", render_mode=\"rgb_array\")  # Creates the environment\n",
    "\n",
    "reset_output = env.reset()\n",
    "print(f\"reset_output: {reset_output}\")\n",
    "state = reset_output[0]\n",
    "print(f\"state: {state}\")\n",
    "\n",
    "render(env)  # This renders the environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the documentation's game is:\n",
    "\n",
    "* **Action Space**\n",
    "\n",
    "    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction of the fixed force the cart is pushed with.\n",
    "    \n",
    "| Num | Action                 |\n",
    "|-----|------------------------|\n",
    "|  0  | Push cart to the left  |\n",
    "|  1  | Push cart to the right |\n",
    "\n",
    "\n",
    "* **State Space**\n",
    "\n",
    "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "    \n",
    "| Num | Observation           | Min                  | Max                |\n",
    "|-----|-----------------------|----------------------|--------------------|\n",
    "| 0   | Cart Position         | -4.8                 | 4.8                |\n",
    "| 1   | Cart Velocity         | -Inf                 | Inf                |\n",
    "| 2   | Pole Angle            | ~ -0.418 rad (-24Â°)  | ~ 0.418 rad (24Â°)  |\n",
    "| 3   | Pole Angular Velocity | -Inf                 | Inf                |\n",
    "\n",
    "* **Rewards**\n",
    "\n",
    "    Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken, including the termination step, is allotted.\n",
    "\n",
    "\n",
    "* **Starting State**\n",
    "\n",
    "    All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n",
    "\n",
    "\n",
    "* **Episode Termination**\n",
    "\n",
    "    The episode terminates if any one of the following occurs:\n",
    "    1. Pole Angle is greater than Â±12Â°\n",
    "    2. Cart Position is greater than Â±2.4 (center of the cart reaches the edge of the display)\n",
    "    3. Episode length is greater than 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `env.step()` function. You have to set an action as argument. Take the returning parameters and print them. What are them? Check https://gymnasium.farama.org/api/env/#gymnasium.Env.step . Pay special attention to the first three returned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Neural network player (Actor-Critic Agent)\n",
    "\n",
    "The Agent in Actor-Critic reinforcement learning is a single NN model with two separated output layers, one for Actor, one for Critic.\n",
    "\n",
    "* **Actor** tries to decide which action is better for a given environmental state, thus it needs as many output neurons as the actions' space dimension. Also, since just one action can be chosen from the actions' space, at the end, this is like a categorical classification. Therefore, the softmax for the Actor's output is the most appropriated activation function. In this case, we have two possible actions (move to left or to right), then a two softmax neurons as output is the choice.\n",
    "\n",
    "* **Critic** tries to estimate the total reward obtained from the action just token, according to the current state. Since this is just a number with no defined range, a single output neuron with no activation function (linear) is the best option to the Critic's output layer.\n",
    "\n",
    "In both cases we use the same input block containing the input layer, with as many neurons as the state's space dimensions. In this case, the model will deal with four dimensional state space.\n",
    "\n",
    "Additionally, the model can has a common hidden block of any combination of any type of layers. The hidden block is the core of the model, who will process the state of the environ to get valuable information. In this example, a Dense layer of 128 ReLu neurons is enough for the common hidden block.\n",
    "\n",
    "Define a neural network to be your Agent. Since it needs to have two separated outputs, use the functional approach.\n",
    "\n",
    "Do not compile the model, since we will train it on our own. Just print the summary to check the layers sizes and connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call your model with the current state from the environ. What Tensor.shape the model expects and returns? Why? How to solve it? Check the [`numpy.newaxis` documentation](https://numpy.org/doc/2.2/reference/constants.html#numpy.newaxis )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Playing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to play a complete game (episode) of the CartPole using the model as a player. Take as argument a boolean flag to render the game (illustrative purposes) or not (training purposes).\n",
    "\n",
    "The function have to return the history of the game, useful for training. It is three lists: 1. the partial reward obtained in every step, 2. the probabilities predictions by actor in every step, and 3. the critic values predicted in every step. The three lists have the same size, equal to the number of steps achieved until the game is terminated.\n",
    "\n",
    "Firstly, the function has to reset the environ and to get the initial state.\n",
    "\n",
    "Then, make a ***while*** loop until the environ returns `terminated=True`.\n",
    "\n",
    "Inside the loop:\n",
    "\n",
    "1. Run your model to get the probabilities from the actor and a value from the critic, according to the current state. Take care of the Tensor shapes.\n",
    "2. Random choose an action, taking into account the probabilities predicted by the actor. Check the `p` parameter of <u>[numpy.random.choice()](https://numpy.org/doc/2.1/reference/random/generated/numpy.random.choice.html)</u>. This will introduce some noise when probabilities are about fifty-fifty, it is more human. Also, it helps to explore more possibilities while training. This is specially interesting when no action (do nothing) is not an option.\n",
    "3. Apply the action to the environ with the `step()` function and take the new current state, the reward and if the game has terminated.\n",
    "4. Render the current environ by calling the `render(env, rewards)` function only if the argument flag is True. Pass the currently cumulated rewards as argument. This is the sum of all previous (and current) rewards. Check the `sum()` function for in-built lists.\n",
    "5. Populate the returning lists. **The probabilities list has to be populated just with the probability obtained for the action done**, while discarding the probability of other actions not done. \n",
    "\n",
    "Finally, return these three history lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the play function, and print the returned lists. The last is just to realize and check that lists. To avoid many printed lines, you can print the length of the lists and the first four items of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Reinforcement training\n",
    "\n",
    "We will follow the Actor-Critic algorithm. More information [here](https://medium.com/intro-to-artificial-intelligence/the-actor-critic-reinforcement-learning-algorithm-c8095a655c14). \n",
    "\n",
    "<hr>\n",
    "\n",
    "Remember, we have access to the game's history (episode) through three lists of $T$ number of items:\n",
    " * **Action probabilities**: Probability predicted by actor to do that specific action for this certain state: $[\\pi_\\theta(a|s)]_T = [\\pi_\\theta(a_0|s_0)\\,,\\,\\dots\\,,\\, \\pi_\\theta(a_{T-1}|s_{T-1})]$.\n",
    " * **Critic values**: Value predicted by critic: $[V_\\phi(s)]_T=[V_\\phi(s_0)\\,,\\,\\dots\\,,\\,V_\\phi(s_{T-1})]$.\n",
    " * **Rewards**: $+1$ for each step in CartPole: $[r]_T = [r_0\\,,\\,\\dots\\,,\\,r_{T-1}]$.\n",
    "\n",
    "There is an extra hyperparameter in Actor-Critic learning called **Discount Factor**: $\\gamma$.\n",
    "\n",
    "<hr>\n",
    "\n",
    "First, let's define four functions to atomize the whole process:\n",
    "\n",
    "### 4.1. Discounted Returns (gain)\n",
    "After an episode finishes, the total discounted return for that episode $G_t$ at each time step $t$ is\n",
    "\\begin{equation}\n",
    "G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots = \\sum_{k=0}^{T-t-1} \\gamma^k r_{t+k} \\quad\\quad\\quad \\rm{for:}\\quad t=0,1,\\dots,T-1 \n",
    "\\end{equation}\n",
    "So, it is the $[G]_T$ gains list of $T$ items.\n",
    "\n",
    "Write a function to compute the equation above. It should take the $[r]_T$ list of rewards as argument and return a same size list $[G]_T$ for the gains, according to the equation.\n",
    "\n",
    "$\\gamma$ can be an extra argument. For games where more time means better result, it should be near to one. For games where as shorter as better, it should be near to 0. Therefore, let's set it to 0.99 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Advantage\n",
    "The advantage $A_t$ indicates how much better an action is compared to the critic's expectation, in a certain $t$ time step:\n",
    "\\begin{equation}\n",
    "A_t = G_t - V_{\\phi}(s_t) \\quad\\quad\\quad \\rm{for:}\\quad t=0,1,\\dots,T-1 \n",
    "\\end{equation}\n",
    "Again, it is a list of $T$ items.\n",
    "\n",
    "Write a function to calculate the Advantage the Actor takes from the Critic. Take the $[r]_T$ rewards list and the $[V_\\phi(s)]_T$ critic's predictions list from the arguments, compute the discounted returns $[G]_T$ using the function you have just created and compute the $[A]_T$ advantage list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Actor Loss \n",
    "The actor loss is defined to maximize the policy performance. Since the Actor has to decide an action, this is a categorical classification problem, where there is just one good action to be applied, the best choice. For that reason, we have used a softmax activation function. Then, the loss function for this is the categorical cross-entropy. However, we have no ground truth, since we have no direct access to the real better action. Then, the critic is a workaround to be able to compute a \"ground truth\" for the cathegorical cross-entropy, the Advantage. Because, by minimizing the negative log-probability of selected actions weighted by the advantage, we will train the Actor to get better than the critic. Therefore, the Advantage is calculated as:\n",
    "\\begin{equation}\n",
    "L_{\\text{actor}}(\\theta) = -\\frac{1}{T}\\sum_{t=0}^{T-1}\\log \\pi_{\\theta}(a_t|s_t) \\cdot A_t\n",
    "\\end{equation}\n",
    "\n",
    "Write a function to calculate the actor loss. Take as argument the $[r]_T$ rewards lists, the $[\\pi_\\theta(a|s)]_T$ probabilities predictions list, and the $[V_\\phi(s)]_T$ critic's predictions list. Compute the $[A]_T$ advantage using the function you have just written. Finally, take into account the $L_\\text{actor}$ actor's loss has to be just an average number.\n",
    "\n",
    "To make the logarithm, use the `tensorflow.keras.ops.log()` function, since it is more appropiated for `tf.Tensor` variables type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Critic Loss\n",
    "The critic just wants to predict the final reward that a certain action will gain for a certain state. Therefore, it wants to replicate the discounted return (gain). Thus, an MSE between the $[V_\\phi(s)]_T$ predicted values and the $[G]_T$ gain is a good loss function to it:\n",
    "\\begin{equation}\n",
    "L_{\\text{critic}}(\\phi) = \\frac{1}{T}\\sum_{t=0}^{T-1}\\left(G_t - V_{\\phi}(s_t)\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "Write a function to compute the averaged Critic's loss from the $[r]_T$ rewards list and the $[V_\\phi(s)]$ critic's predictions list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's use all this functions to compute the gradients and train the model.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### 4.5 Learning loop\n",
    "\n",
    "We will use the `Adam` optimizer from `keras.optimizers`.\n",
    "\n",
    "Since we will manually apply the gradients using [its own funciton](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam#apply), we do not need to compile the model with this optimizer, but we just need to define a variable with this optimizer with a certain learning rate, e.g. $\\eta=0.005$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model just by playing some (or many) complete games and learning from the rewards obtained.\n",
    "\n",
    "Do a *while* loop until the game is considered solved by the model. This is when it reaches 200 points for a single play (episode), i.e. `points`$=\\sum_{t=0}^T r_t$.\n",
    "\n",
    "Then, in the ***while loop***:\n",
    "\n",
    "1. Open a `tenosrflow.GradinetTape()` using a `with ... as ... :` block (https://www.tensorflow.org/guide/autodiff). This will store all tf.Tensor values during the play (forward pass through the model) to be able to compute the gradient (backpropagation).\n",
    "   * Inside the `with_as` block:\n",
    "     1. Play a complete game using the function done in section 3 and get the lists of rewards, probability prediction, and critic predictions for the current episode. Pass a `False` for the rendering flag, expect every 10 training loops (episodes), since it takes time. So, to avoid rendering the environ for all episodes will speed up the training.\n",
    "     2. Get the actor and critic losses using the functions done before with the returned lists by the playing function.\n",
    "     3. Compute the total loss as the sum of the Actor's loss and the Critic's loss.\n",
    "<br>\n",
    "<br>\n",
    "2. Compute the gradient using `tape.gradient(loss, variables)`, where `loss` is the total loss computed inside the `with_as` block, and `variables` are the trainable variables of your model (`model.trainable_variables`). It returns the gradient.\n",
    "<br>\n",
    "<br>\n",
    "4. Apply the gradient to the model using the Adam optimizer defined above, using the `apply(grad, variables)` function of your optimizer, where `grad` is the gradient returned by the tape and `variables` are again those trainable variables of your model.\n",
    "<br>\n",
    "<br>\n",
    "5. Check the total obtained `points` in this current episode, and print it. Use `print(points, end=', ')` to avoid many printed lines.\n",
    "<br>\n",
    "<br>\n",
    "6. Store the total obtained points in a history list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the history to check the evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Checking the model\n",
    "\n",
    "Play again a complete game calling the function written in section 1.3 and check its behavior and the total reward obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the model reaches the target rewards during the training just by luck. Check the obtained points' evolution. If it is consistent, probably you have just gotten a good behavior in this last play. However, if it suddenly reached the maximum, probably it was by luck.\n",
    "\n",
    "If the last, you can re-run the learning cell. It will continue with the training, not from the beginning, just continuing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "217.458px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
