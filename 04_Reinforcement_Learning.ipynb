{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"text-align: right\"><sub>This notebook is distributed under the <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\" target=\"_blank\">Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license</a>.</sub></div>\n",
    "<h1>Hands on Machine Learning  <span style=\"font-size:12px;\"><i>by <a href=\"https://webgrec.ub.edu/webpages/000004/cat/dmaluenda.ub.edu.html\" target=\"_blank\">David Maluenda</a></i></span></h1>\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://atenea.upc.edu/course/view.php?id=95161\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/upc_logo_49px.png\" width=\"130\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "  </td>\n",
    "  <td>   <!-- gColab -->\n",
    "    <a href=\"https://colab.research.google.com/github/dmaluenda/hands_on_machine_learning/blob/master/04_Reinforcement_Learning.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/colab_logo_32px.png\" />\n",
    "      Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- github -->\n",
    "    <a href=\"https://github.com/dmaluenda/hands_on_machine_learning/blob/master/04_Reinforcement_Learning.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/github_logo_32px.png\" />\n",
    "      View source on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- download -->\n",
    "    <a href=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/04_Reinforcement_Learning.ipynb\"  target=\"_blank\"\n",
    "          download=\"04_Reinforcement_Learning\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/download_logo_32px.png\" />\n",
    "      Download notebook\n",
    "      </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# $\\text{IV}$. Reinforcement Learning\n",
    "\n",
    "Hands on \"Machine Learning on Classical and Quantum data\" course of\n",
    "[Master in Photonics - PHOTONICS BCN](https://photonics.masters.upc.edu/en/general-information)\n",
    "[[UPC](https://photonics.masters.upc.edu/en) +\n",
    "[UB](https://www.ub.edu/web/ub/en/estudis/oferta_formativa/master_universitari/fitxa/P/M0D0H/index.html?) +\n",
    "[UAB](https://www.uab.cat/en/uab-official-masters-degrees-study-guides/) +\n",
    "[ICFO](https://www.icfo.eu/lang/studies/master-studies)].\n",
    "\n",
    "**Tutorial 4**\n",
    "\n",
    "This notebook shows how to:\n",
    "- solve a challenge with reinforcement learning\n",
    "- implement the Actor-Critic method\n",
    "- implement the Deep Deterministic Policy Gradient\n",
    "- use a Gradient Tape\n",
    "\n",
    "**References**:\n",
    "\n",
    "[1] [Keras](https://keras.io/getting_started/): a deep learning API written in Python.<br>\n",
    "[2] [Keras examples](https://keras.io/examples/rl/).<br>\n",
    "[3] Gym, a toolkit for developing and comparing reinforcement learning algorithms. By [OpenAI](https://gym.openai.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0. Imports: NumPy, Matplotlib, Tensorflow and gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will use the [Gymnasium](https://gymnasium.farama.org/) module to solve a challenging game. You have to install it before if you do not already have it just by run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /Users/malu/opt/anaconda3/envs/MSc2025/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/malu/opt/anaconda3/envs/MSc2025/lib/python3.12/site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/malu/opt/anaconda3/envs/MSc2025/lib/python3.12/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/malu/opt/anaconda3/envs/MSc2025/lib/python3.12/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/malu/opt/anaconda3/envs/MSc2025/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: gymnasium[classic-control] in /Users/malu/opt/anaconda3/envs/MSc2025/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/malu/opt/anaconda3/envs/MSc2025/lib/python3.12/site-packages (from gymnasium[classic-control]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/malu/opt/anaconda3/envs/MSc2025/lib/python3.12/site-packages (from gymnasium[classic-control]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/malu/opt/anaconda3/envs/MSc2025/lib/python3.12/site-packages (from gymnasium[classic-control]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/malu/opt/anaconda3/envs/MSc2025/lib/python3.12/site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /Users/malu/opt/anaconda3/envs/MSc2025/lib/python3.12/site-packages (from gymnasium[classic-control]) (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from time import sleep, time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Actor Critic Method (CartPole game from OpenAI gym)\n",
    "\n",
    "We will use the CartPole game. Documentation: https://gymnasium.farama.org/environments/classic_control/cart_pole/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell below **should open a new window** named '*pygame window*' with a cart and an upstanding pole. This is just a window where to show the current state of the game. If it is not opened in your computer, please, ask professor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset_output: (array([-0.00949323, -0.03900715,  0.01438253, -0.03724991], dtype=float32), {})\n",
      "state: [-0.00949323 -0.03900715  0.01438253 -0.03724991]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\", render_mode=\"human\")  # Create the environment\n",
    "\n",
    "reset_output = env.reset()\n",
    "print(f\"reset_output: {reset_output}\")\n",
    "state = reset_output[0]\n",
    "print(f\"state: {state}\")\n",
    "\n",
    "env.render()  # This creates a figure, open it and make it always visible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the documentation's game is:\n",
    "\n",
    "* **Action Space**\n",
    "\n",
    "    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction of the fixed force the cart is pushed with.\n",
    "    \n",
    "    | Num | Action                 |\n",
    "    |-----|:-----------------------|\n",
    "    | 0   | Push cart to the left  |\n",
    "    | 1   | Push cart to the right |\n",
    "\n",
    "* **State Space**\n",
    "\n",
    "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "    \n",
    "    | Num | Observation           | Min                  | Max                |\n",
    "    |-----|-----------------------|----------------------|--------------------|\n",
    "    | 0   | Cart Position         | -4.8                 | 4.8                |\n",
    "    | 1   | Cart Velocity         | -Inf                 | Inf                |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°)  | ~ 0.418 rad (24°)  |\n",
    "    | 3   | Pole Angular Velocity | -Inf                 | Inf                |\n",
    "\n",
    "* **Rewards**\n",
    "\n",
    "    Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken, including the termination step, is allotted.\n",
    "\n",
    "\n",
    "* **Starting State**\n",
    "\n",
    "    All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n",
    "\n",
    "\n",
    "* **Episode Termination**\n",
    "\n",
    "    The episode terminates if any one of the following occurs:\n",
    "    1. Pole Angle is greater than ±12°\n",
    "    2. Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "    3. Episode length is greater than 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `env.step()` function. You have to set an action as argument. Take the returning parameters and print them. What are them? Check https://gymnasium.farama.org/api/env/#gymnasium.Env.step . Pay special attention to the first three returned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info, _ = env.step(1)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Neural network player (Actor-Critic Agent)\n",
    "\n",
    "The Agent in Actor-Critic reinforcement learning is a single NN model with two separate output layers, one for Actor, one for Critic.\n",
    "\n",
    "* **Actor** try to decide which action is better for a given environmental state. In this case we have two possible actions (move to left or to right), then a two softmax neurons as output is the choice.\n",
    "\n",
    "* **Critic** try to estimate the next reward, according to the current state. A single output sigmoid neuron is enough.\n",
    "\n",
    "In both cases we use the same input block containing the input layer (with as many neurons like the state shape) and a hidden layer of a certain number of neurons, 128 for instance. The hidden block might be any combination of any type of layers.\n",
    "\n",
    "Define a neural network to be your Agent. Since it needs to has two separated ouputs, use the functional approach.\n",
    "\n",
    "Do not compile the model, since we will train it on our own. Just print the summary to check the layers sizes and connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m640\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │        \u001b[38;5;34m258\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,027</span> (4.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,027\u001b[0m (4.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,027</span> (4.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,027\u001b[0m (4.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_inputs = 4    # state space\n",
    "num_actions = 2   # action space\n",
    "num_hidden = 128  # free hyper-parameter\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "hidden = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(hidden)\n",
    "critic = layers.Dense(1, activation='sigmoid')(hidden)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call your model with the current state from the environ. What Tensor.shape the model expects and returns? Why? How to solve it? Check https://numpy.org/doc/2.2/reference/constants.html#numpy.newaxis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(state[np.newaxis, :])\n",
    "\n",
    "actor_out, critic_out = output\n",
    "\n",
    "actor_probs = actor_out[0,:]\n",
    "critic_value = critic_out[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3 Playing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to play a complete game of the CartPole using the model as a player.\n",
    "\n",
    "The function have to return the history of the game, useful for training. It is three lists: 1. the rewards obtained in every step, 2. the probabilities predictions by actor in every step, and 3. the critic values predicted in every step. \n",
    "\n",
    "The first step of the function is to reset the environ and to get the initial state.\n",
    "\n",
    "Then, make a *while* loop until the environ returns `terminated=True`.\n",
    "\n",
    "Inside the loop:\n",
    "\n",
    "1. Run your model to get the probabilities from the actor and a value from the critic, according to the current state. Think about the Tensor sizes.\n",
    "2. Random choose an action, taking into account the probabilities predicted by the actor. Check the `p` parameter of <u>[numpy.random.choice()](https://numpy.org/doc/2.1/reference/random/generated/numpy.random.choice.html)</u>. This will introduce some noise when probabilities are about fifty-fifty, it is more human. Also, it helps to explore more possibilities while training.\n",
    "3. Apply the action to the environ with the `step()` function and take the new current state, the reward and if the game has terminated.\n",
    "4. Populate the returning lists. **The probabilities list has to be populated just with the probability obtained for the action done**, while discarding the probability of other actions not done. \n",
    "\n",
    "Finally, return that history lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def play():\n",
    "\n",
    "    cum_reward = 0\n",
    "    rewards_list = []\n",
    "    probabilities_list = []\n",
    "    critic_list = []\n",
    "    \n",
    "    state = env.reset()[0]  # let's start a new game. It returns the initial state, and something else ignored\n",
    "    done = False  # it's not done at the very beginning\n",
    "    final_mark = 0\n",
    "    \n",
    "    while not done:\n",
    "\n",
    "        state_batch = state[None,:]  # from state to a batch\n",
    "        \n",
    "        prob_batch, critic_batch = model(state_batch)\n",
    "\n",
    "        prob = prob_batch[0,:]       # from batch to true values\n",
    "        critic = critic_batch[0, :]  # from batch to true values\n",
    "\n",
    "        action = np.random.choice(num_actions, p=prob.numpy())\n",
    "\n",
    "        # apply an action and get the new information\n",
    "        state, reward, done, info, _ = env.step(action)\n",
    "\n",
    "        rewards_list.append(reward)\n",
    "        probabilities_list.append(prob[action])\n",
    "        critic_list.append(critic)\n",
    "    \n",
    "    return rewards_list, probabilities_list, critic_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the play function, and print the final mark, this is the sum of all stepping rewards. Check the `sum()` function for an in-built lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n"
     ]
    }
   ],
   "source": [
    "rw_list, prob_list, crit_list = play()\n",
    "\n",
    "print(sum(rw_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.4 Reinforcement training\n",
    "\n",
    "We will follow the Actor-Critic algorithm, check for instance: https://medium.com/intro-to-artificial-intelligence/the-actor-critic-reinforcement-learning-algorithm-c8095a655c14\n",
    "\n",
    "<hr>\n",
    "\n",
    "We have access to the game's history through three lists:\n",
    " * **Action probabilities**: Probability predicted by actor of the given action and for a certain state $\\pi_{\\theta}(a|s)$.\n",
    " * **Critic values**: Value predicted by critic $V_{\\phi}(s)$.\n",
    " * **Rewards** $r_t$: $+1$ for each step in CartPole.\n",
    "\n",
    "There is an extra hyperparameter that is  the **Discount Factor** $\\gamma$.\n",
    "\n",
    "<hr>\n",
    "\n",
    "First, let's define four functions to atomize the whole process:\n",
    "\n",
    "**1. Discounted Returns (gain)**.\n",
    "After the episode finishes, the total discounted return $G_t$ for each step $t$ is\n",
    "\\begin{equation}\n",
    "G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots = \\sum_{k=0}^{T-t-1} \\gamma^k r_{t+k}\n",
    "\\end{equation}\n",
    "It is a list of values, where $t=0,1,2,\\dots,T-1$ is the time step index.\n",
    "\n",
    "Write a function to compute the equation above. It should take a list of rewards as argument and return a same size list according to the equation.\n",
    "\n",
    "$\\gamma$ can be an extra argument. For games where more time means better result, it should be near to one. For games where as shorter as better, it should be near to 0. Let's set it to 0.99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain(rw_list, gamma=0.99):\n",
    "\n",
    "    gains = []\n",
    "    g = 0\n",
    "    \n",
    "    for r in rw_list[::-1]:\n",
    "        g = r + gamma*g\n",
    "        gains.append(g)\n",
    "\n",
    "    return gains[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Advantage**.\n",
    "The advantage $A_t$ indicates how much better an action is compared to the critic's expectation:\n",
    "\\begin{equation}\n",
    "A_t = G_t - V_{\\phi}(s_t)\n",
    "\\end{equation}\n",
    "Again, it is a list of $T$ itmes.\n",
    "\n",
    "Write a function to calculate the Advantage the Actor takes from the Critic. Take the rewards list and the critic's predictions list from the arguments, compute the discounted returns $G$ using the function you just create and compute the new list for the advantage token in every time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advantatge(rw_list, critics):\n",
    "    \n",
    "    return [g-v for g, v in zip(gain(rw_list), critics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Actor Loss**. \n",
    "The actor loss is defined to maximize the policy performance. We minimize the negative log-probability of selected actions weighted by the advantage:\n",
    "\\begin{equation}\n",
    "L_{\\text{actor}}(\\theta) = -\\frac{1}{T}\\sum_{t=0}^{T-1}\\log \\pi_{\\theta}(a_t|s_t) \\cdot A_t\n",
    "\\end{equation}\n",
    "\n",
    "Write a function to calculate the actor loss. Take as argument the rewards lists, the probabilities predictions list $[\\pi_\\theta]$ and the critic's predictions list. Compute the advantage using the function above. The actor loss has to be just an average number.\n",
    "\n",
    "To make the logarithm, use the `tensorflow.keras.ops.log()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_loss(rw_list, prob_list, critics):\n",
    "    \n",
    "    T = len(rw_list)\n",
    "\n",
    "    adv_list = advantatge(rw_list, critics)\n",
    "\n",
    "    out = 0\n",
    "    for prob, adv in zip(prob_list, adv_list):\n",
    "        out += keras.ops.log(prob)*adv\n",
    "\n",
    "    return -out/T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Critic Loss**.\n",
    "The critic is trained to minimize the Mean Squared Error (MSE) between the estimated state values $V_{\\phi}(s_t)$ and the computed returns $G_t$:\n",
    "\\begin{equation}\n",
    "L_{\\text{critic}}(\\phi) = \\frac{1}{T}\\sum_{t=0}^{T-1}\\left(G_t - V_{\\phi}(s_t)\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "Write a function to compute the averaged critic loss from the rewards list and the critic predictions list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_loss(rw_list, critics):\n",
    "    T = len(rw_list)\n",
    "    return sum([(g-c)**2 for g, c in zip(gain(rw_list), critics)])/T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's use all this small functions to compute the gradients and train the model.\n",
    "\n",
    "<hr>\n",
    "\n",
    "We will use the `Adam` optimizer from `keras.optimizers`.\n",
    "\n",
    "Since we will manually applay the gradients using [its own funciton](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam#apply), we do not need to compile the model with this optimizer, but we just need to define a variable with this optimizer with a certain learning rate, e.g. $\\eta=0.005$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model just by playing some (or many) complete games and learning from the rewards obtained.\n",
    "\n",
    "Do a *while* loop until the game is considered solved by the model. This is when it reaches 200 points for a single play.\n",
    "\n",
    "Then, in the while loop:\n",
    "\n",
    "1. Open a `tenosrflow.GradinetTape()` using a `with ... as ... :` block (https://www.tensorflow.org/guide/autodiff).\n",
    "   * Inside the `with_as` block:\n",
    "     1. Play a complete game using the function done in section 1.3 and get the lists of rewards, probability prediction, and critic predictions for that episode.\n",
    "     2. Get the actor and critic losses using the functions done before, and compute the total loss just by summing them.\n",
    "<br>\n",
    "<br>\n",
    "2. Compute the gradient using `tape.gradient(loss, variables)`, where loss is the total loss computed inside the `with_as` block, and the variables are the trainable variables of your model (`model.trainable_variables`). It returns the gradient.\n",
    "<br>\n",
    "<br>\n",
    "4. Apply the gradient to the model using the Adam optimizer defined above, using the `apply(grad, variables)` function of your optimizer, where `grad` is the gradient returned by the tape and `variables` are those trainable of your model.\n",
    "<br>\n",
    "<br>\n",
    "5. Check the total points obtained in this current episode (the sum of rewards_list), and print it. Use `print(points, end=', ')` to avoid many printed lines.\n",
    "\n",
    "Count the episodes required to train your model (the number of passes on this while loop), and print it when the loop finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.0, 24.0, 19.0, 26.0, 43.0, 22.0, 15.0, 98.0, 33.0, 29.0, 30.0, 34.0, 62.0, 62.0, 34.0, 24.0, 166.0, 44.0, 17.0, 50.0, 88.0, 56.0, 60.0, 139.0, 66.0, 46.0, 50.0, 86.0, 54.0, 85.0, 86.0, 59.0, 43.0, 142.0, 114.0, 82.0, 170.0, 72.0, 199.0, 71.0, 120.0, 104.0, 125.0, 297.0, Solved at episode 44 with 95.46499800682068 seconds.\n"
     ]
    }
   ],
   "source": [
    "alpha = 1\n",
    "\n",
    "episode_count = 0\n",
    "current_total_reward = 0\n",
    "t0 = time()\n",
    "\n",
    "while current_total_reward<200:  # Episodies loop: like epochs. Each play-loop is an episode\n",
    " \n",
    "    with tf.GradientTape() as tape:  # this will register all trainable parameters in a tape, to be able to find the gradient afterwards   \n",
    "\n",
    "        rewards_list, probs_predictions, critic_predictions = play()\n",
    "        \n",
    "        total_loss = (actor_loss(rewards_list, probs_predictions, critic_predictions) +\n",
    "                      alpha * critic_loss(rewards_list, critic_predictions))\n",
    "        \n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply(grads, model.trainable_variables)\n",
    "\n",
    "    current_total_reward = sum(rewards_list)\n",
    "\n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    print(current_total_reward, end=', ')\n",
    "\n",
    "print(f\"Solved at episode {episode_count} with {time()-t0} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.5 Checking the model\n",
    "\n",
    "Play again a complete game calling the function written in section 1.3 and check its behavior and the total reward obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(sum(play()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the model reaches the target rewards during the training just by luck. Check the obtained points' evolution. If it is consistent, probably you have just gotten a good behavior in this last play. However, if it suddenly reached the maximum, probably it was by luck.\n",
    "\n",
    "If the last, you can re-run the learning cell. It will continue with the training, not from the beginning, just continuing.\n",
    "\n",
    "Finally, let's close the environ, and the `pygame window`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "217.458px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
