{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><sub>This notebook is distributed under the <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\" target=\"_blank\">Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license</a>.</sub></div>\n",
    "<h1>Hands on Machine Learning  <span style=\"font-size:10px;\"><i>by <a href=\"https://webgrec.ub.edu/webpages/000004/ang/dmaluenda.ub.edu.html\" target=\"_blank\">David Maluenda</a></i></span></h1>\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://atenea.upc.edu/course/view.php?id=85709\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/upc_logo_49px.png\" width=\"130\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "  </td>\n",
    "  <td>   <!-- gColab -->\n",
    "    <a href=\"https://colab.research.google.com/github/dmaluenda/hands_on_machine_learning/blob/master/02_Training.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/colab_logo_32px.png\" />\n",
    "      Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- github -->\n",
    "    <a href=\"https://github.com/dmaluenda/hands_on_machine_learning/blob/master/02_Training.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/github_logo_32px.png\" />\n",
    "      View source on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- download -->\n",
    "    <a href=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/02_Training.ipynb\"  target=\"_blank\"\n",
    "          download=\"02_Training\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/download_logo_32px.png\" />\n",
    "      Download notebook\n",
    "      </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{II}$. Training Neural Networks with Pure Python\n",
    "\n",
    "Hands on \"Machine Learning on Classical and Quantum data\" course of\n",
    "[Master in Photonics - PHOTONICS BCN](https://photonics.masters.upc.edu/en/general-information)\n",
    "[[UPC](https://photonics.masters.upc.edu/en) +\n",
    "[UB](https://www.ub.edu/web/ub/en/estudis/oferta_formativa/master_universitari/fitxa/P/M0D0H/index.html?) +\n",
    "[UAB](https://www.uab.cat/web/estudiar/la-oferta-de-masteres-oficiales/informacion-general-1096480309770.html?param1=1096482863713) +\n",
    "[ICFO](https://www.icfo.eu/lang/studies/master-studies)].\n",
    "\n",
    "Tutorial 2\n",
    "\n",
    "This notebook shows how to:\n",
    "- understand the cost/loss function concept\n",
    "- implement a stochastic gradient descent to fit an arbitrary nonlinear function\n",
    "- implement backpropagation in pure python\n",
    "- train a deep fully connected net to reproduce an image\n",
    "- differentiate 'hyperparameters' from 'parameters'\n",
    "- choose a learning rate\n",
    "- choose a batch size\n",
    "- differentiate training, validation and test datasets\n",
    "- initialize a neural network\n",
    "- save and load networks\n",
    "- interpret the 'epochs' concept\n",
    "\n",
    "**References**:\n",
    "\n",
    "[1] [Machine Learning for Physicists](https://machine-learning-for-physicists.org/) by Florian Marquardt.\n",
    "<br>\n",
    "[2] [NumPy](https://numpy.org/doc/stable/user/whatisnumpy.html): the fundamental package for scientific computing in Python.\n",
    "<br>\n",
    "[3] [Matplotlib](https://matplotlib.org/stable/tutorials/introductory/usage.html): a comprehensive library for creating static, animated, and interactive visualizations in Python.<br>\n",
    "[4] \"Back-Propagation is very simple. Who made it Complicated?\", Prakash Jay at [medium](https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c).\n",
    "<br>\n",
    "[5] [\"A Step by Step Backpropagation Example\"](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/), Matt Mazur.\n",
    "<br>\n",
    "[6] [Backpropagation Step by Step](https://hmkcode.com/ai/backpropagation-step-by-step) from hmkcode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#0.-Imports:-only-numpy-and-matplotlib\" data-toc-modified-id=\"0.-Imports:-only-numpy-and-matplotlib-0\">0. Imports: only numpy and matplotlib</a></span></li><li><span><a href=\"#1.-Nonlinear-fitting-(base-for-training)\" data-toc-modified-id=\"1.-Nonlinear-fitting-(base-for-training)-1\">1. Nonlinear fitting (base for training)</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-Function-definition-and-dataset\" data-toc-modified-id=\"1.1-Function-definition-and-dataset-1.1\">1.1 Function definition and dataset</a></span></li><li><span><a href=\"#1.2-Cost-function-(also-known-as-loss-function)\" data-toc-modified-id=\"1.2-Cost-function-(also-known-as-loss-function)-1.2\">1.2 Cost function (also known as loss function)</a></span></li><li><span><a href=\"#1.3-Stochastic-gradient-descent\" data-toc-modified-id=\"1.3-Stochastic-gradient-descent-1.3\">1.3 Stochastic gradient descent</a></span></li><li><span><a href=\"#1.4-Non-constant-learning-rate-(Schedule-or-Adaptive)\" data-toc-modified-id=\"1.4-Non-constant-learning-rate-(Schedule-or-Adaptive)-1.4\">1.4 Non constant learning rate (Schedule or Adaptive)</a></span></li></ul></li><li><span><a href=\"#2.-Backpropagation\" data-toc-modified-id=\"2.-Backpropagation-2\">2. Backpropagation</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1-Implement-of-backpropagation-for-a-general-(fully-connected)-network\" data-toc-modified-id=\"2.1-Implement-of-backpropagation-for-a-general-(fully-connected)-network-2.1\">2.1 Implement of backpropagation for a general (fully connected) network</a></span></li><li><span><a href=\"#2.2-Train-a-net-on-batches-to-fit-a-2D-function-(an-arbitrary-image)\" data-toc-modified-id=\"2.2-Train-a-net-on-batches-to-fit-a-2D-function-(an-arbitrary-image)-2.2\">2.2 Train a net on batches to fit a 2D function (an arbitrary image)</a></span></li></ul></li><li><span><a href=\"#3.-Save-and-load-a-model\" data-toc-modified-id=\"3.-Save-and-load-a-model-3\">3. Save and load a model</a></span></li><li><span><a href=\"#4.-Inside-a-network\" data-toc-modified-id=\"4.-Inside-a-network-4\">4. Inside a network</a></span></li><li><span><a href=\"#5.-Play-with-https://playground.tensorflow.org\" data-toc-modified-id=\"5.-Play-with-https://playground.tensorflow.org-5\">5. Play with <a href=\"https://playground.tensorflow.org\" rel=\"nofollow\" target=\"_blank\">https://playground.tensorflow.org</a></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlT_ZrS-R1rG"
   },
   "source": [
    "## 0. Imports: only numpy and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T15:54:31.308199Z",
     "iopub.status.busy": "2021-04-19T15:54:31.307840Z",
     "iopub.status.idle": "2021-04-19T15:54:31.896606Z",
     "shell.execute_reply": "2021-04-19T15:54:31.895654Z",
     "shell.execute_reply.started": "2021-04-19T15:54:31.308135Z"
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1626874792493,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "zRmzXX4wR1rG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get the \"numpy\" library for linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# get \"matplotlib\" for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 300  # highres display\n",
    "from matplotlib.axes._axes import _log as mpl_ax_logger\n",
    "mpl_ax_logger.setLevel('ERROR')  # ignore warnings\n",
    "\n",
    "# for nice inset colorbars:\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "import requests  # to download files (for data/images)\n",
    "from io import BytesIO\n",
    "import imageio   # to deal with images\n",
    "\n",
    "# time control to count it and manage it\n",
    "from time import time, sleep\n",
    "\n",
    "# For simple animation\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Nonlinear fitting (base for training)\n",
    "\n",
    "Neural networks basically are nonlinear functions that connect $x$ inputs to $y$ outputs, through some $\\omega$ inner free parameters.\n",
    "\n",
    "The exactly dependence of $y$ on $x$ and $\\omega$ is completely defined by the neural network architecture, i.e. number of layers, number of neurons in each layer and which activation function is applied in each layer. At the end of the day, it can be written as $y=g(x;\\omega)$, where $g(\\cdot)$ is a quite complex function made of a concatenation of known and smaller functions (matrix multiplication and activation functions in each layer step).\n",
    "\n",
    "Notice that $x$ and $y$ might be vectors capable to hold several input and output values, whereas $\\omega$ here collects any free parameter on the neural network, that is all weights and biases in the network.\n",
    "\n",
    "The desired behavior of the network is to give an output $y$ for a given input $x$, this is called ***prediction*** or ***inference***. To do that, it has to adjust its $\\omega$ inner free parameters to accomplish this goal. This is done by providing examples of $(x,y)$ input-output pairs while the networks tries to minimize the error when predicting its own $y$ output. This minimizing process is call ***training***.\n",
    "\n",
    "Therefore, a network training is an optimization problem to fit a nonlinear function, where the whole collection of $\\omega$ inner parameters are the trainable values and the $x$ inputs and $y$ outputs are multidimensional (vectors). Therefore, deep learning training mathematically is a nonlinear and multidimensional fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Function definition and dataset\n",
    "\n",
    "Since neural networks usually have several input and output neurons, and typically many trainable parameters $\\omega$, it is not easy to visualize the training mechanism. Then, let's play with a toy model where we will have just one single input $x$, one single output $y$, and two trainable parameters $\\omega=(\\omega_0, \\omega_1)$.\n",
    "\n",
    "Let's define a non linear function to connect $y$ from $x$, through $\\omega$, for instance\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:func_def}\n",
    "g(x; \\omega) = \\frac{\\omega_0}{(x - \\omega_1)^2 + 1} = y\n",
    "\\end{equation}\n",
    "\n",
    "Notice that this function $g(\\cdot)$ do NOT represent any kind of neuron connexion within a neural network, neither any activation function. It is just an example of nonlinear function chosen only for illustrative purposes.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Then, define a function to return the $y$ predicted result for a given $x$ and $\\omega$ (input arguments). This would act as the network prediction (inference). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize our pseudo-network by setting arbitrary parameters for $\\omega$.\n",
    "\n",
    "Thus, declare a variable to store $\\omega$ and initialize it to $\\omega=(-2, 2)$, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset with one million of $(x, y)$ pairs and we want to train our pseudo-network (model) to reproduce this dataset.\n",
    "\n",
    "Typically, the dataset is called ***ground truth***, because it is the truth where we fix our target.\n",
    "\n",
    "Load the `curve_fitting_data.npz` numpy file to get the ground truth.\n",
    "\n",
    "> Check the [`numpy.load()`](https://numpy.org/doc/stable/reference/generated/numpy.load.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NpzFile 'object' with keys: x, y\n",
      "(1000000,) (1000000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = requests.get('https://github.com/dmaluenda/hands_on_machine_learning/raw/master/data/curve_fitting_data.npz', stream = True)\n",
    "data = np.load(BytesIO(r.raw.read()))\n",
    "print(data)\n",
    "\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "plt.figure(figsize=(3,1))\n",
    "plt.plot(x[:100], y[:100], '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is huge, create the a function to return two batches arrays (for $x$ and $y$) of a certain length given by the $M$ `batchsize` argument.\n",
    "\n",
    "So, randomly pick a $M$ number of elements on the whole $x$ input array and their corresponding $y$ output pairs.\n",
    "\n",
    "Check your function setting a small number to batchsize (e.g. $M=4$), and print the returned $x$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the randomly initialized model with the ground truth dataset, using just some randomly picked points.\n",
    "\n",
    "Thus, plot $M=40$ randomly picked samples from the dataset by using the function done above. Then, plot the network prediction for that same input in the same figure.\n",
    "\n",
    "> Check the `plt.scatter()` matplotlib's function.\n",
    "\n",
    "Add a legend to easily identify which data is from the dataset and which is the prediction.\n",
    "\n",
    "Set the $x$-axis limits to $[-4, 4]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cost function (also known as loss function)\n",
    "\n",
    "The fitting shown above is probably poor. However, we need to quantify the 'quality' of the current fitting by a numeric value. In other words, a cost function is a function to evaluate the quality of the fitting.  We will use the most simple cost function: the square of the discrepancy between the predicted values with the ground truth, averaged over all the evaluated samples. Notices that is a single value, known in this case as mean squared error (MSE).\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:loss_func}\n",
    "C(\\omega; y) = \\frac{1}{2} \\left\\langle \\left[ y\\,(x; \\omega) - y_{\\rm truth}(x)\\right]^2 \\right\\rangle_{\\!_M}\n",
    "\\end{equation}\n",
    "\n",
    "where $y(x; \\omega) = g(x; \\omega_0, \\omega_1)$ is the prediction made by the current state of the model and over a certain $x$ input set, and $y_{\\rm truth}(x)$ is the output pair of each $x$ input taken from the ground truth dataset. $\\left\\langle \\cdot \\right\\rangle_{\\!_M}$ denotes the average over all $M$ evaluated samples.\n",
    "\n",
    "> the $1/2$ factor is just to make things easier when deriving that cost function\n",
    "\n",
    "Notice that cost function $C$ indirectly depends on the dataset $(x, y)$. However, the main dependency is on the $\\omega$ free parameters. We will focus on this in a while.\n",
    "\n",
    "[Check this to see more cost/loss functions](https://www.theaidream.com/post/loss-functions-in-neural-networks).\n",
    "\n",
    "<hr>\n",
    "\n",
    "Create a function that returns the MSE-cost for a given set of pairs predicted-true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually the cost is calculated just for validation purposes. For fitting/training purposes, we only work with its derivative, as we will see in a while. \n",
    "\n",
    "However, since this example have only two trainable parameters, it is worth to check how the cost looks in what is called the *cost landscape*. This is an image where the axis represent the trainable parameters and the color is for the cost reached by this parameters combination.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Create a function, which returns a $40\\times40$ matrix containing the MSE-cost when $\\omega_0$ is in range $[-3, 6]$ and $\\omega_1$ in $[-2, 3]$. The MSE-cost have to be evaluated using a given $(x, y)$ values from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this function using a $M=20$ randomly picked $(x,y)$ samples, and plot with the `imshow` function the obtained landscape. \n",
    "\n",
    "Run several times this code cell. Is the landscape changing? Why?\n",
    "\n",
    "What happens when using more (or less) randomly picked samples, by increasing the $M$ (`batchsize`)?\n",
    "\n",
    "Should the cost landscape ideally depend on the specific $(x, y)$ values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Optional***: Replace the `imshow` with `contourf` and `contour` to easily visualize the changes between runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Stochastic gradient descent\n",
    "\n",
    "The gradient descent algorithm is based on walking to the minimum cost, by means of stepping down the gradient of that cost landscape.\n",
    "\n",
    "In math, the gradient is typically done over the space coordinates ($x$, $y$, $z$...). However, the cost function depends on the trainable parameters $\\omega$ or, better said, we want to modify $\\omega$ to get better results in prediction time (reducing the MSE). Thus, the gradient here is a vector whose components are the partial derivatives respect to each trainable parameter $\\omega_i$.\n",
    "\n",
    "\\begin{equation}  \\label{eq:gradient}\n",
    "\\vec{\\nabla_{\\omega}} C(\\omega; y) = \\left( \\frac{\\partial C(\\omega; y)}{\\partial \\, \\omega_0} \\; , \\;  \\dots \\; , \\; \\frac{\\partial C(\\omega; y)}{\\partial \\, \\omega_i} \\; , \\; \\dots \\; , \\; \\frac{\\partial C(\\omega; y)}{\\partial \\, \\omega_{N_{\\omega}}} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $C(\\omega; y)$ depends essentially on the different parameters $\\omega_i$ ; ($i=0\\dots N_{\\omega}$).\n",
    "Take into account that $x$ and $y$ are here just the environ where to compute the cost, but they do not play any other role in the training (*stochastic-ness* comes from here). All training focus are over the $\\omega$ trainable parameters, not over $(x,y)$.\n",
    "\n",
    "Remember the goal of the stochastic gradient descent is to update the trainable parameters by following the gradient. However, cost does not depend directly on $\\omega$, but it indirectly does through the inference function $y=g(x;\\omega)$. So, it is worth to apply the chain rule for the derivatives.\n",
    "\n",
    "$$\\omega_i^{\\text{new}} = \\omega_i - \\eta \\, \\delta(\\omega_i) = \\omega_i - \\eta \\, \\frac{\\partial C(\\omega;y)}{\\partial \\omega_i} \\quad ; \\quad (i=0,N_{\\omega})$$\n",
    "\\begin{equation}\\label{eq:grad_desc_func}\n",
    " = \\omega_i - \\eta \\, \\frac{\\partial C(\\omega;y)}{\\partial y(x;\\omega)} \\, \\frac{\\partial y(x;\\omega)}{\\partial \\omega_i}\\quad \\quad \\quad \\quad \\quad \\quad \\;\\;\\,\n",
    "\\end{equation}\n",
    "$$\\ = \\omega_i - \\eta \\,  \\left\\langle \\big[ y(x;\\omega_0, \\omega_1) - y_{\\rm truth}(x) \\big] \\, \\frac{\\partial y(x; \\omega)}{\\partial \\omega_i} \\right\\rangle_{\\!_M}$$\n",
    "\n",
    "\n",
    "where $\\eta$ is the learning rate (how fast we want to progress during the training) and $\\delta(\\omega_i)=\\frac{\\partial C(\\omega;y)}{\\partial \\omega_i}$ is the gradient step for each $\\omega_i$ trainable parameter.\n",
    "\n",
    "$\\frac{\\partial C(\\omega;y)}{\\partial y(x;\\omega)}$ is easily derived from Eq. (\\ref{eq:loss_func}) and its just a scalar factor (not a vector). It basically contains the inference of the current model over an input and its comparison with the ground truth.\n",
    "\n",
    "Finally, $\\frac{\\partial y(x;\\omega)}{\\partial \\omega_i}$ is the analytical gradient of $g(\\cdot)$ over the weights, Eq. (\\ref{eq:func_def}). It only depends on the model architecture (relation between inputs, weights and outputs) and it is the core of the backpropagation algorithm.\n",
    "\n",
    "In this particular case\n",
    "$$\\vec{\\nabla_{\\omega}} y(x;\\omega) = \\vec{\\nabla_{\\omega}} g(x;\\omega_0, \\omega_1) = \\left( \\dots \\; , \\; \\frac{\\partial g(x; \\omega)}{\\partial \\omega_i} \\; , \\; \\dots \\right) = \\left( \\frac{1}{(x - \\omega_1)^2 + 1} \\; , \\; \\frac{+2(x-\\omega_1)\\omega_0}{\\left[(x-\\omega_1)^2+1\\right]^2}  \\right)$$\n",
    "\n",
    "Notice that, since we deal with many input samples $x$, all this should be averaged over all $M$ samples, see the angle brackets $\\langle \\cdot \\rangle_{\\!_M}$ in Eq. (\\ref{eq:grad_desc_func}), coming from Eq. (\\ref{eq:loss_func})."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Create a function which calculates the gradient step $\\delta(\\omega) = \\frac{\\partial C(\\omega;y)}{\\partial \\omega_i}$ in this particular case, for a given $\\omega$ and $(x, y)$ ground truth data points and $y$ predicted outputs. \n",
    "\n",
    "How many components should this gradient step have?\n",
    "\n",
    "This function might return not only the $\\delta(\\omega)$, but also the MSE-cost in the current state and with the current batch of data. This last may be useful to monitor the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to perform a train step. This is `your_training_function` (needed in a while):\n",
    "\n",
    "0. The function should take as arguments the current $\\omega$, the learning rate $\\eta$ and the `batchsize`, in this specific order.\n",
    "1. Get a random sample of the ground truth data: a batch of $(x,y)$ input-output pairs from the dataset, this will be the ground truth in this training step. (Use the function done at the beginning of this tutorial)\n",
    "2. Update the $\\omega$ according to the *gradient_step*  function done before.\n",
    "3. Return the following variables: the $x$ input batch, the $y_{\\rm truth}$ target output, the $y$ predicted output, the $C$ MSE-cost obtained in this step (before updating the omegas), and the new $\\omega$ parameters (in this specific order).\n",
    "\n",
    "Follow the instructions 0. and 3. in detail to be able to monitor the training with the fancy function some cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a loop of to apply some training steps (using the function you have just made) and store in a list the MSE-cost returned in every iteration. Plot that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the prediction of the model over a random batch of inputs and compare it with the ground truth. This is exactly like at the end of section 1.1.\n",
    "\n",
    "Has the fitting improved? Is it good enough?\n",
    "\n",
    "If not, run again the training loop and plot again the prediction (against the ground truth).\n",
    "\n",
    "How many steps are needed to get an acceptable fitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Now, let's see in more detail what are happening during this training loop.\n",
    "\n",
    "To do that, we will see how the $\\omega$ descend the gradient on the cost landscape.\n",
    "\n",
    "First, let's define some functions to fancy plot the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(x, true_y, fit_y, legend=True, ax=None):\n",
    "    ax = plt.gca() if ax==None else ax\n",
    "\n",
    "    ax1 = ax.scatter(x, true_y, color=\"blue\", label='ground truth')\n",
    "    ax2 = ax.scatter(x, fit_y, color=\"orange\", label='fitting')\n",
    "    ax3 = ax.plot([x, x], [true_y, fit_y], 'y:', linewidth=1, \n",
    "                  label='deviation')\n",
    "    \n",
    "    if legend:\n",
    "        ax.legend(handles=[ax1, ax2, ax3[0]], loc='upper left', fontsize=6)\n",
    "    ax.set_xlim(-4,4)\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(f\"$y=f(x;{omegas})$\", fontsize=6)\n",
    "\n",
    "    \n",
    "def plot_landscape(true_y, true_x, ws, ax=None):\n",
    "    ax = plt.gca() if ax==None else ax\n",
    "    \n",
    "    Nw = 40\n",
    "    \n",
    "    # The landscape is on omegas, WHY?\n",
    "    w0s = np.linspace(-3, 6, Nw)  # exploring from -3 to 6 for th0 (40 samples)\n",
    "    w1s = np.linspace(-2, 3, Nw)  # exploring from -2 to 3 for th1 (40 samples)\n",
    "\n",
    "    landscape = np.zeros([Nw, Nw])  # init landscape\n",
    "    for j0, w0 in enumerate(w0s):\n",
    "        for j1, w1 in enumerate(w1s):\n",
    "            pred_y = w0 / ( (true_x-w1)**2 + 1.0 )\n",
    "            landscape[j1, j0] = 0.5 * np.mean((pred_y - true_y)**2)\n",
    "    \n",
    "    im = ax.contourf(landscape, extent=[-3, 6, -2, 3])\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    ax.contour(landscape, extent=[-3, 6, -2, 3], colors=\"white\")\n",
    "\n",
    "    ax.set_xlabel(r'$\\omega_0$')\n",
    "    ax.set_ylabel(r'$\\omega_1$')\n",
    "\n",
    "    \n",
    "def plot_info(x, true_y, pred_y, omegas, stepping, history, arrows=True):\n",
    "    \"\"\" This is a funtion to plot the progress on the training.\n",
    "    \"\"\"\n",
    "    max_MSE = history.max()*1.1 if history.size>0 else 1  # skipping empty hist.\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    fig = plt.figure(figsize=(5,3))\n",
    "    gs = fig.add_gridspec(2, 2, hspace=.35, wspace=.3)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax3 = fig.add_subplot(gs[1, :])\n",
    "    \n",
    "    plot_curves(x, true_y, pred_y, legend=True, ax=ax1)  # compare fiting with truth\n",
    "    plot_landscape(true_y, x, omegas, ax2)  # to see the progress\n",
    "\n",
    "    # plot where we are on the landscape\n",
    "    ax2.scatter([omegas[0]], [omegas[1]], color=\"orange\")  \n",
    "    if arrows:  # where we go\n",
    "        ax2.arrow(*(omegas),*(-stepping), color='yellow',\n",
    "                  width=0.05, length_includes_head=True)\n",
    "    \n",
    "    # let's plot the cost history\n",
    "    ax3.plot(range(history.size), history, marker='o')\n",
    "    ax3.set_xlim(0, history.size-1)\n",
    "    ax3.set_xticks(range(0, history.size, 3))\n",
    "    ax3.set_ylim(0.0, max_MSE)\n",
    "    ax3.set_xlabel(r\"$step$\")\n",
    "    ax3.set_ylabel(r\"$MSE$\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def animated_train(your_func, eta, batchsize, init_omegas=None, nsteps=20):\n",
    "    if init_omegas is None:\n",
    "        global omegas  # let's continue where we was\n",
    "    else:\n",
    "        omegas = init_omegas\n",
    "\n",
    "    cost_history = np.zeros(nsteps)\n",
    "    delta_w = 0\n",
    "    for n in range(nsteps):  # it can be replaced with a while with some criteria\n",
    "\n",
    "        old_omegas = omegas.copy()\n",
    "        x, y_true, y_pred, cost, omegas = your_func(omegas, 0.7, 100)\n",
    "        delta_w = old_omegas - omegas\n",
    "        \n",
    "        # let's plot where we are and where we go\n",
    "        plot_info(x, y_true, y_pred, old_omegas, delta_w, cost_history, True)\n",
    "        print('estimated omega:', omegas)\n",
    "        sleep(0.5)\n",
    "        \n",
    "        cost_history[n] = cost\n",
    "        \n",
    "        plot_info(x, y_true, y_pred, omegas, delta_w, cost_history, False)\n",
    "        print('estimated omega:', omegas)\n",
    "        sleep(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function below where the first argument is the training function you defined in a couple of cells above.\n",
    "\n",
    "Play with the parameters `eta` and `batchsize`.\n",
    "\n",
    "How does it influence learning progress?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# your_func: The trining funtion defined by your own. It has to follow the given instructions for the arguments and returning variables.\n",
    "# eta: \"learning rate\" (gradient descent step size)\n",
    "# batchsize: stochastic x samples used per step (just a few to be fast)\n",
    "# itit_omegas: the starting training process\n",
    "\n",
    "animated_train(your_func='your_training_function', eta=1.85, batchsize=300, init_omegas=[-2.3, 0.8])  # I modified this <===========  1.65 ; 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why we are not going directly to the minimum of the landscape?\n",
    "\n",
    "\n",
    "Why is this algorithm ***Stochastic*** *Gradient Descent*? Check the meaning and reason of a [stochastic](https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/) approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Non constant learning rate (Schedule or Adaptive)\n",
    "\n",
    "**Optional**: Modify the `train_step` in such a way to run with a non constant learning rate.\n",
    "\n",
    "[Check this to know more on learning rate](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backpropagation\n",
    "\n",
    "Like in the Stochastic Gradient Descent, we update trainable parameters of a neural network (weights and biases) in the following way\n",
    "\n",
    "\\begin{equation} \\label{eq:grad_desc1}\n",
    "\\omega_{ij}^l = \\omega_{ij}^l - \\eta \\, \\delta(\\omega_{ij}^l)\n",
    "\\end{equation}\n",
    "\\begin{equation} \\label{eq:grad_desc2}\n",
    "b_{i}^l = b_{i}^l - \\eta \\, \\delta (b_{i}^l)\n",
    "\\end{equation}\n",
    "\n",
    "where $l=1\\dots L$ label runs over all the net layers, $i=1\\dots N_l$ and $j=1\\dots N_{l-1}$ labels run over all the $N_l$ neurons of the $l$-th layer, and $\\delta (\\omega) = \\frac{\\partial C(\\omega;x)}{\\partial \\omega}$ is the gradient step to update the parameters, where $\\omega$ includes all weights and biases.\n",
    "\n",
    "In neural networks, the cost function $C(\\omega;y)$ can have a very deep dependency on weights and biases, especially those corresponding to the first layers. However, this deep dependency can be solved by means of the derivative chain rule. Moreover, the calculations done to estimate the gradient step for the last layers can be reused for deeper layers, as we recall below.\n",
    "\n",
    "* **Output layer** ($l=L$):\n",
    "\n",
    "The gradient step $\\delta$ to update weights $\\omega^{L}_{ij}$ and biases $b^{L}_{i}$ of the last layer ($l=L$), according to Eq. (\\ref{eq:grad_desc_func}), is\n",
    "\n",
    "\\begin{equation} \\label{eq:delta_a}\n",
    "\\delta(\\omega^L) = \\frac{\\partial C(\\omega;y) }{\\partial \\omega^L }\n",
    "                 = \\frac{\\partial C(\\omega;y) }{\\partial y^{L}_{i} }\\frac{\\partial y^{L}_{i} }{\\partial \\omega^{L} } \n",
    "\\end{equation}\n",
    "                        \n",
    "where $\\omega^L$ stands for any trainable parameter on the last layer; both, $\\omega^{L}_{ij}$ and $b^{L}_{i}$. Notice that the first factor depends only on the cost function, whereas the second one only does on the arquitecture of the network.\n",
    "\n",
    "Taking the simple MSE-cost function $C = \\frac{1}{2} \\left( y_i^L - \\hat{y}_i \\right)^2$, where $\\hat{y}_i=y^{\\rm truth}_i$ is the ground truth result for the $i$-th component (neuron) and $y_i^L=y_i^L(x;\\omega)$ is the actual value gotten with the current network state; we just simplify the notation.\n",
    "\n",
    "Notice that current output value can be expressed as a combination of the previous layer: \n",
    "\\begin{equation}\n",
    "y_i^L = f_L\\!\\!\\left[x_j^{L}\\right] =  f_L\\!\\!\\left[\\omega_{ij}^{L}y_j^{L-1} + b_i^L\\right]\n",
    "\\end{equation}\n",
    "where $f_l[\\cdot]$ is the activation function for the $l$-th layer, then\n",
    "\n",
    "\\begin{equation} \\label{eq:dC_dy_last}\n",
    "\\frac{\\partial C }{\\partial y^{L}_{i} } = \\left( y_i^L - \\hat{y}_i \\right) \\end{equation}\n",
    "\n",
    "this is just the deviation between the predicted result and the ground truth.\n",
    "\n",
    "\n",
    "On the other hand,\n",
    "\n",
    "\\begin{equation} \\label{eq:dy_dw_last}\n",
    "\\frac{\\partial y^{L}_{i} }{\\partial \\omega^{L}_{ij} } = f'_L\\!\\!\\left[ x_i^L\\right] \\, y_j^{L-1} \\end{equation}\n",
    "\n",
    "\\begin{equation} \\label{eq:dy_db_last}\n",
    "\\frac{\\partial y^{L}_{i} }{\\partial b^{L}_{i} } = f'_L\\!\\!\\left[ x_i^L\\right] \\end{equation}\n",
    "\n",
    "This two last expressions only contain the current value returned by certain neurons $y_j^{L-1}$ and the value when using the derivative of the activation function, instead.\n",
    "\n",
    "For convenience, let's define\n",
    "\\begin{equation} \\label{eq:delta_i}\n",
    "\\boxed{ \\Delta^{\\!L}_i = \\left( y_i^L - \\hat{y}_i \\right) \\, f'_L\\!\\!\\left[ x_i^L\\right] }\n",
    "\\end{equation}\n",
    "\n",
    "Finally,\n",
    "\n",
    "\\begin{equation} \\label{eq:omega_ij}\n",
    "\\boxed{ \\delta(\\omega_{ij}^L) = \\Delta_i^{\\!L} \\otimes y_j^{L-1} }\n",
    "\\end{equation}\n",
    "\\begin{equation} \\label{eq:b_i}\n",
    "\\boxed{ \\delta(b_{i}^L) = \\Delta_i^{\\!L} }\n",
    "\\end{equation}\n",
    "\n",
    "These boxed equations is all we need to be able to update the weight and biases of the last layer. Of course, all calculus made above are computed with matrices and vectors of indices $i$ and $j$. Notice that $\\otimes$ stands for the outer product, which generates a matrix from two vectors. Moreover, usually it is processed with batches of input-output pairs, where final values are averaged for those batches. An efficient way to implement this in batches is explained at the end of this cell.\n",
    "\n",
    "* **Last hidden layer**:\n",
    "\n",
    "The procedure is exactly the same than before. However, in order to update weights $\\omega^{L-1}_{jk}$ and biases $b^{L-1}_{j}$ of the last hidden layer ($l=L-1$), Eq. (\\ref{eq:delta_a}) becomes\n",
    "\n",
    "\\begin{equation} \\label{eq:delta_a2}\n",
    "\\delta(\\omega^{L-1}) = \\frac{\\partial {C} }{\\partial \\omega^{L-1} }\n",
    "            = \\frac{\\partial {C} }{\\partial y^{L}_{i} }\n",
    "              \\frac{\\partial y^{L}_{i} }{\\partial y^{L-1}_{j} }\n",
    "              \\frac{\\partial y^{L-1}_{j} }{\\partial \\omega^{L-1} } \n",
    "\\end{equation}\n",
    "Therefore,\n",
    "\\begin{equation} \\label{eq:omega_jk}\n",
    "{ \\delta(\\omega_{jk}^{L-1}) = \\Delta_i^{\\!L} \\; \\omega_{ij}^L \\, f'_{L-1}\\!\\!\\left[ x_j^{L-1}\\right] \\otimes y_k^{L-2} = \\Delta_j^{\\!L-1} \\otimes y_k^{L-2} }\n",
    "\\end{equation}\n",
    "\\begin{equation} \\label{eq:b_j}\n",
    "{ \\delta(b_{j}^{L-1}) = \\Delta_i^{\\!L} \\; \\omega_{ij}^L \\, f'_{L-1}\\!\\!\\left[ x_j^{L-1}\\right]  = \\Delta_j^{\\!L-1}}\n",
    "\\end{equation}\n",
    "\n",
    "* **General formula**:\n",
    "\n",
    "Notice that Eqs. (\\ref{eq:omega_jk}, \\ref{eq:b_j}) can be generalized for any layer $l=L-m$ as\n",
    "\\begin{equation} \\label{eq:omega_pq}\n",
    "\\boxed{ \\delta(\\omega_{pq}^{l}) = \\Delta_p^{\\!l} \\otimes y_q^{l-1} }\n",
    "\\end{equation}\n",
    "\\begin{equation} \\label{eq:b_p}\n",
    "\\boxed{ \\delta(b_{p}^{l}) = \\Delta_p^{\\!l} }\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation} \\label{eq:delta_p}\n",
    "\\boxed{ \\Delta_{p}^{l} = \\Delta_{r}^{\\!l+1}  \\cdot \\omega_{rp}^{l+1} \\, f'_{l}\\!\\!\\left[ x_p^{l}\\right] } \\quad ; \\quad (l=L-1\\dots 1)\n",
    "\\end{equation}\n",
    "\n",
    "Notice that Eq. (\\ref{eq:delta_p}) shows as the error $\\Delta$ done in the prediction is backpropagated according with weights and activations of every layer.\n",
    "\n",
    "* **General notes**:\n",
    "\n",
    "Notice that boxed equations contain all the backpropagation process. For Eq. (\\ref{eq:delta_i}), we need the prediction $y_i^L$ and the derivative of the last step $f'_L\\!\\!\\left[ x_i^L\\right]$. Moreover, we also need all the intermediate values $y_p^{l}$ and their derivatives $f'_l\\!\\!\\left[ x_p^l\\right]$ for Eq. (\\ref{eq:omega_pq}, \\ref{eq:delta_p}). Therefore, the forward prediction (inference) is needed in every training step, not only to get the cost value and its derivative (Eq. \\ref{eq:delta_i}), but also in such a way to store all intermediate neurons values $y_p^{l}$ and their derivative $f'_l\\!\\!\\left[ x_p^l\\right]$. Then, all the effort is to make forward inferences storing the intermediate states and its derivatives and, then, apply the backprogation depicted in Eq. (\\ref{eq:delta_p}).\n",
    "\n",
    "That's it!\n",
    "\n",
    "\n",
    "* **Implementation in batches**:\n",
    "\n",
    "As we said before, we must be able to accept input-output pairs in batches in order to increase the performances (in time and in precision, recall stochasticness/batchsize relation). Then, all variables regarding data should earn an extra dimension. These means that weights and the biases, and their gradient steps remain like in equations above. In practice, we need just to be carefully with the shape and order of the products. This is, since we want to average all values over the batches, then to compute Eqs. (\\ref{eq:omega_ij}, \\ref{eq:omega_pq}) we have to do a dot product (sum-product) with the batch index as inner index and divide by the batchsize. Similarly, for Eqs. (\\ref{eq:b_i}, \\ref{eq:b_p}) we have to sum over the $\\Delta$ and divide by the batchsize. In addition, the inner index of $\\Delta \\cdot \\omega$ (in Eq. \\ref{eq:delta_p}) is the repeated index $r$, getting a shape of `batchSize`$\\times$`layerSize`. Finally, this is multiplied element-wise for the activation contribution, like in Eq. (\\ref{eq:delta_i})."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implement of backpropagation for a general (fully connected) network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it step by step.\n",
    "\n",
    "Create a function doing a layer step, ready for training. This is, given an input vector $x$, a weights matrix $\\omega$, and a biases vector $b$, it computes the matrix operation (linear superposition) and separately apply the activation function and its derivative. Then, it returns both the result of apply the activation function and its derivative.\n",
    "\n",
    "Use the *Sigmoid* as activation function, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for testing, run this function you have just created for a layer-step dealing with 5 input and 7 output neurons, and a batchsize of 100.\n",
    "\n",
    "Create a valid $\\omega$ weights matrix, $b$ biases vector and $x$ input array. Use random numbers.\n",
    "\n",
    "Which size should the result be? Check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well.\n",
    "\n",
    "Create now a function to predict (inference) the result after apply the whole network for a given input vector $x$, a list of weights $[w]_l$ (one weights matrix for each layer) and a list of biases $[b]_l$ (idem), for $l=1\\dots L$ being $L$ the number of layers in the network. You may iteratively use the last function.\n",
    "\n",
    "Remember, you will need, not only the final result, but also all inner states of the neurons $y_q^{\\,l}$ and their derivative $y\\prime_q^{\\,l}=f'_{l}\\!\\!\\left[ x_p^{l}\\right]$. So, store and return a couple of list of neurons states $[y_q]_l$, one for the common state and the other for its derivative $[y_q^{\\,\\prime}]_l$, for $l=1\\dots L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, just for testing, run this function you have just created for a 3 layers network with 5, 7, 3 neurons each layer, and a batchsize of 100.\n",
    "\n",
    "Create a valid $[\\omega]_l$ list of weights matrices, $[b]_l$ list of biases vectors and $x$ input array. Use random numbers.\n",
    "\n",
    "Which size should the result be? Check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to back propagate the error prediction evaluated by the cost function. Thus, you should take the ground truth $y_i^{\\rm truth}$ and the current parameters, i.e. the weights list $[\\omega]_l$ and biases list $[b]_l$ in the arguments.\n",
    "\n",
    "This function have to return the gradient steps $\\delta(\\omega)$ and $\\delta(b)$.\n",
    "\n",
    "Note that $\\delta(\\omega)$ and $\\delta(b)$ have to be equal in size than $\\omega$ and $b$, then they are list of matrices ($\\omega$) and vectors ($b$). Thus, you can initialize them using the `np.zeros_like(a)` numpy's function.\n",
    "\n",
    "Then, you can fill their last item following Eqs. (\\ref{eq:omega_ij}, \\ref{eq:b_i}), using Eq. (\\ref{eq:delta_i}).\n",
    "\n",
    "Finally, you can fill them recursively backwards following Eqs. (\\ref{eq:omega_pq}, \\ref{eq:b_p}), using Eq. (\\ref{eq:delta_p}).\n",
    "\n",
    "Note that you will need all inner states of the neurons $[y_q]_l$ and their derivative $[y^{\\,\\prime}_q]_l$, so ask for them in the arguments.\n",
    "\n",
    "Take into account that all data variables has an extra dimension for batch processing and, then, you have to average over that dimension to get an stochastic result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it together.\n",
    "\n",
    "Create a function to perform a training step. This is to update the trainable parameters downing the gradient.\n",
    "\n",
    "To do that apply forward (inference) the whole network while storing the inner states (and their derivative) by means of the function defined before.\n",
    "\n",
    "Then, take that inner states and apply the backpropagation to get the gradient steps $\\delta(\\omega)$ and $\\delta(b)$.\n",
    "\n",
    "Now, you can update the weights and biases following the Eqs. (\\ref{eq:grad_desc1}, \\ref{eq:grad_desc2}).\n",
    "\n",
    "Finally, compute the MSE-cost of the network.\n",
    "\n",
    "We will use this function to completely train a network, then it is important to check that:\n",
    "\n",
    "- it has as argument: the input data $x$, the ground_truth $y^{\\rm truth}$, the current weights list $[\\omega]_l$, the current biases list $[b]_l$, and the learning rate $\\eta$.\n",
    "\n",
    "- it returns: the predicted result $y^L$, the cost $C$, the updated weights list $[\\omega^{\\rm new}]_l$, and the updated biases list $[b^{\\rm new}]_l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train a net on batches to fit a 2D function (an arbitrary image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a network having 3 layers with 2 input neurons, 100 hidden neurons, and 1 single output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an inference function to evaluate the network for a given input and weights/biases.  Use as activation function the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next nonlinear function as a ground truth, so as the target behavior of the network.\n",
    "Yes it is an arbitrary image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(url):\n",
    "    return requests.get(url).content\n",
    "\n",
    "# load the pixel image\n",
    "URL = \"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/data/Smiley.png\"\n",
    "face = imageio.imread(get_content(URL))\n",
    "\n",
    "pixel_image = face[:,:,0]\n",
    "pixel_image = pixel_image[:,::-1].astype('float64')  # and flip... to get the right view!\n",
    "\n",
    "pixel_image += np.random.uniform(low=0, high=0.2*pixel_image.max(), size=pixel_image.shape)\n",
    "\n",
    "# normalize between 0 and 1\n",
    "pixel_image -= pixel_image.min()\n",
    "pixel_image = (pixel_image.astype(dtype='float'))/pixel_image.max()*2 - 1  # [-1, 1]\n",
    "\n",
    "image_size = np.shape(pixel_image)[0] # assuming a square image!\n",
    "print(pixel_image.shape)\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(pixel_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to get a `batchsize` number of representations for the target function within a range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the batch of pixels in a image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to evaluate the training, this is by testing the whole function scope.\n",
    "\n",
    "Make a function to evaluate the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training step function (backpropagation) several times to train the network and recover the target image (smiling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save and load a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are the information about the model?\n",
    "\n",
    "Save that information using the `np.savez()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can recover a whole neural network model and state.\n",
    "\n",
    "Load that information in a new network and reproduce the image without making any new training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inside a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set to 0 all the weights on the last layer step except for one of the neurons and plot the result.\n",
    "\n",
    "Repeat this for all the rest last weights (100 in total) and check the produced images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Play with https://playground.tensorflow.org "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "223.852px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
