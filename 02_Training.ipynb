{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#0.-Imports:-only-numpy-and-matplotlib\" data-toc-modified-id=\"0.-Imports:-only-numpy-and-matplotlib-0\">0. Imports: only numpy and matplotlib</a></span></li><li><span><a href=\"#1-Stochastic-Gradient-Descent\" data-toc-modified-id=\"1-Stochastic-Gradient-Descent-1\">1 Stochastic Gradient Descent</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-Function-definition-and-dataset\" data-toc-modified-id=\"1.1-Function-definition-and-dataset-1.1\">1.1 Function definition and dataset</a></span></li><li><span><a href=\"#1.2-Cost-function-(also-known-as-loss-function)\" data-toc-modified-id=\"1.2-Cost-function-(also-known-as-loss-function)-1.2\">1.2 Cost function (also known as loss function)</a></span></li><li><span><a href=\"#1.3-Stochastic-gradient-descent\" data-toc-modified-id=\"1.3-Stochastic-gradient-descent-1.3\">1.3 Stochastic gradient descent</a></span></li><li><span><a href=\"#1.4-Non-constant-learning-rate-(Schedule-or-Adaptive)\" data-toc-modified-id=\"1.4-Non-constant-learning-rate-(Schedule-or-Adaptive)-1.4\">1.4 Non constant learning rate (Schedule or Adaptive)</a></span></li></ul></li><li><span><a href=\"#2.-Backpropagation\" data-toc-modified-id=\"2.-Backpropagation-2\">2. Backpropagation</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1-Implement-of-backpropagation-for-a-general-(fully-connected)-network\" data-toc-modified-id=\"2.1-Implement-of-backpropagation-for-a-general-(fully-connected)-network-2.1\">2.1 Implement of backpropagation for a general (fully connected) network</a></span></li><li><span><a href=\"#2.2-Train-a-net-to-fit-a-2D-function-(an-arbitrary-image)-in-batches\" data-toc-modified-id=\"2.2-Train-a-net-to-fit-a-2D-function-(an-arbitrary-image)-in-batches-2.2\">2.2 Train a net to fit a 2D function (an arbitrary image) in batches</a></span></li><li><span><a href=\"#2.3-Network-definition-and-evaluation\" data-toc-modified-id=\"2.3-Network-definition-and-evaluation-2.3\">2.3 Network definition and evaluation</a></span></li><li><span><a href=\"#2.4-Neural-network-training\" data-toc-modified-id=\"2.4-Neural-network-training-2.4\">2.4 Neural network training</a></span></li></ul></li><li><span><a href=\"#3.-Save-and-load-a-model\" data-toc-modified-id=\"3.-Save-and-load-a-model-3\">3. Save and load a model</a></span></li><li><span><a href=\"#4.-Inside-a-network\" data-toc-modified-id=\"4.-Inside-a-network-4\">4. Inside a network</a></span></li><li><span><a href=\"#5.-Play-with-https://playground.tensorflow.org\" data-toc-modified-id=\"5.-Play-with-https://playground.tensorflow.org-5\">5. Play with <a href=\"https://playground.tensorflow.org\" rel=\"nofollow\" target=\"_blank\">https://playground.tensorflow.org</a></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><sub>This notebook is distributed under the <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\" target=\"_blank\">Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license</a>.</sub></div>\n",
    "<h1>Hands on Machine Learning  <span style=\"font-size:10px;\"><i>by <a href=\"https://webgrec.ub.edu/webpages/000004/ang/dmaluenda.ub.edu.html\" target=\"_blank\">David Maluenda</a></i></span></h1>\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://atenea.upc.edu/course/view.php?id=95161\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/upc_logo_49px.png\" width=\"130\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "  </td>\n",
    "  <td>   <!-- gColab -->\n",
    "    <a href=\"https://colab.research.google.com/github/dmaluenda/hands_on_machine_learning/blob/master/02_Training.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/colab_logo_32px.png\" />\n",
    "      Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- github -->\n",
    "    <a href=\"https://github.com/dmaluenda/hands_on_machine_learning/blob/master/02_Training.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/github_logo_32px.png\" />\n",
    "      View source on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- download -->\n",
    "    <a href=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/02_Training.ipynb\"  target=\"_blank\"\n",
    "          download=\"02_Training\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/download_logo_32px.png\" />\n",
    "      Download notebook\n",
    "      </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{II}$. Training Neural Networks with Pure Python\n",
    "\n",
    "Hands on \"Machine Learning on Classical and Quantum data\" course of\n",
    "[Master in Photonics - PHOTONICS BCN](https://photonics.masters.upc.edu/en/general-information)\n",
    "[[UPC](https://photonics.masters.upc.edu/en) +\n",
    "[UB](https://www.ub.edu/web/ub/en/estudis/oferta_formativa/master_universitari/fitxa/P/M0D0H/index.html?) +\n",
    "[UAB](https://www.uab.cat/web/estudiar/la-oferta-de-masteres-oficiales/informacion-general-1096480309770.html?param1=1096482863713) +\n",
    "[ICFO](https://www.icfo.eu/lang/studies/master-studies)].\n",
    "\n",
    "**Tutorial 2**\n",
    "\n",
    "This notebook shows how to:\n",
    "\n",
    "- Understand supervised learning and ground truth datasets  \n",
    "- Define and interpret a cost (loss) function  \n",
    "- Implement stochastic gradient descent (SGD)  \n",
    "- Understand the meaning of learning rate and batch size  \n",
    "- Distinguish between parameters and hyperparameters  \n",
    "- Implement backpropagation in pure Python  \n",
    "- Train a deep fully connected network to reproduce an image  \n",
    "\n",
    "**References**:\n",
    "\n",
    "[1] [Machine Learning for Physicists](https://machine-learning-for-physicists.org/) by Florian Marquardt.\n",
    "<br>\n",
    "[2] [NumPy](https://numpy.org/doc/stable/user/whatisnumpy.html): the fundamental package for scientific computing in Python.\n",
    "<br>\n",
    "[3] [Matplotlib](https://matplotlib.org/stable/tutorials/introductory/usage.html): a comprehensive library for creating static, animated, and interactive visualizations in Python.<br>\n",
    "[4] \"Back-Propagation is very simple. Who made it Complicated?\", Prakash Jay at [medium](https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c).\n",
    "<br>\n",
    "[5] [\"A Step by Step Backpropagation Example\"](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/), Matt Mazur.\n",
    "<br>\n",
    "[6] [Backpropagation Step by Step](https://hmkcode.com/ai/backpropagation-step-by-step) from hmkcode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlT_ZrS-R1rG"
   },
   "source": [
    "## 0. Imports: only numpy and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1626874792493,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "zRmzXX4wR1rG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get the \"numpy\" library for linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# get \"matplotlib\" for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 300  # highres display\n",
    "from matplotlib.axes._axes import _log as mpl_ax_logger\n",
    "mpl_ax_logger.setLevel('ERROR')  # ignore warnings\n",
    "\n",
    "# for nice inset colorbars:\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "import requests  # to download files (for data/images)\n",
    "from io import BytesIO\n",
    "import imageio   # to deal with images\n",
    "\n",
    "# time control to count it and manage it\n",
    "from time import time, sleep\n",
    "\n",
    "# For simple animation\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def get_npz_remote(url):\n",
    "    \"\"\" This is a wrapper to get a npz from a url \"\"\"\n",
    "    r = requests.get(url, stream=True)\n",
    "    return BytesIO(r.raw.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Stochastic Gradient Descent\n",
    "### 1.1 Function definition and dataset\n",
    "\n",
    "Since neural networks usually have several input and output neurons, and typically many trainable parameters $\\omega$, it is not easy to visualize the training mechanism. Then, let's play with a toy model where we will have just one single input $x$, one single output $y$, and two trainable parameters $\\omega=(\\omega_0, \\omega_1)$.\n",
    "\n",
    "Let's define a nonlinear function to connect $y$ from $x$, through $\\omega$, for instance\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{2.1}\n",
    "g(x; \\omega) = \\frac{\\omega_0}{(x - \\omega_1)^2 + 1} = y\n",
    "\\end{equation}\n",
    "\n",
    "Notice that this function $g(\\cdot)$ do NOT represent any kind of neuron connection within a neural network, neither any activation function. It is just an example of nonlinear function chosen only for illustrative purposes. Although, a neural network can be understood as a nonlineal function that connects $\\vec{x}$ input to $\\vec{y}$ output neurons through some $w$ trainable parameters.\n",
    "\n",
    "This simplified setup allows us to visualize and fully understand the training mechanism before moving to high-dimensional neural networks.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Then, define a function to return the $y$ predicted result for a given $x$ and $\\omega$ (input arguments). This would act as the network prediction inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize this pseudo-network by setting arbitrary parameters for $\\omega$. For instance, declare a variable to store $\\omega$ and initialize it to $\\omega=(-2, 2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the model it's just \"random\" initialized, but we want to train it, i.e. To modify the $\\omega$ in such a way to reproduce some behavior.\n",
    "\n",
    "To train the model (or fit the function), we will follow the supervised learning strategy, in this case. In supervised learning, we use a dataset containing many input–output pairs $(x, y)$ representing the expected behavior of the system. This is what is called the ***ground truth***, because it is the truth where we fix our target.\n",
    "\n",
    "For this exercise, we have a dataset with one million of $(x, y)$ pairs, and we want to train this pseudo-network (model) to reproduce this dataset. To have many samples in the training dataset is great, because it provides very valuable information. However, evaluating the full dataset at every iteration would be computationally inefficient. This motivates the use of mini-batches.\n",
    "\n",
    "The next code cell downloads a NPY array with all this examples and it shows a reduced samples of them (the first one hundred)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NpzFile 'object' with keys: x, y\n",
      "(1000000,) (1000000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAFoCAYAAACfaI5hAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAuIwAALiMBeKU/dgAARfRJREFUeJzt3Ql4FGXW6PETEhKBgIACQfZN2WRkcwRUcBg3QBYFUXEct08lOtt3x2Wuzgi4zIxzZ1VxueMVFZcIIvKh4qgDCggaiCKbIGtYTESQJQESEvo+p5zG7s7W3fVWd1X1//c8+ebrpqvy2l2prlPvec9JCwQCAQEAAAAAH6mX7AEAAAAAgGkEOgAAAAB8h0AHAAAAgO8Q6AAAAADwHQIdAAAAAL5DoAMAAADAdwh0AAAAAPgOgQ4AAAAA3yHQAQAAAOA7BDoAAAAAfIdABwAAAIDvEOgAAAAA8B0CHQAAAAC+Q6ADAAAAwHcIdAAAAAD4DoEOAAAAAN8h0AEAAADgOwQ6AAAAAHyHQAcAAACA7xDoAAAAAPAdAh0AAAAAvkOgAwAAAMB3CHQAAAAA+A6BDgAAAADfIdABAAAA4DsEOgAAAAB8h0AHAAAAgO9kJHsAfrN//3754IMPTjxu166dZGVlJXVMAAAAgB1lZWWyY8eOE4+HDh0qTZs2FTcj0DFMg5yxY8ea3i0AAADgGnPnzpUxY8aIm5G6BgAAAMB3CHQAAAAA+A6pa4bpmpzIab2uXbua/jUAABf5sviQvL3mK9n8dYkcPXZcTqpfT7q0zJZLe7eWbq0aJ3t4AGDbpk2bwpZnRF7zuhGBjmGRhQc0yOnVq5fpXwMAcIHPd+6X++etlU8L94tIpog0F0kTkQqRTbtF3tm9Vxpl7pdTG2fJKY0ypedpTeSqge2ld5uTkz10ALDFC8W2CHQAAIjBml0H5JX8Qlm2ea9s2VMqgTpeX1peKaV7D8v2vYeloHC/zFxeKN1zsuWR8T+QPm3dXbEIALyMQAcAgJhnb+z5oqhERj+2VG4+t6PcN4pZfwBwAsUIAACow/vri2XCk8uMBDmh/rlkm5z7h39bQRQAwCwCHQAAaqFBSO6LBVJWcdyR92nn/iNyxRMfWcEUAMAcAh0AAGqh6WpOBTlBxyoDVjDFzA4AmEOgAwBADVbvPGA8Xa0mGkxNmbeWzwIADCHQAQCgBnkrChP63mhVtskvrrQquwEA7KHqGgAANVi3+2DC35u3VxdZP/3aN5Upo3tRghoA4sSMDgAAIXQ25b65q+Xy6UtlbRICndDZHa30RpECAIgPMzoAAPynutqdsz+XDUWHXPN+6LodLVIw67ZBzOwAQIyY0QEApLxnFm+RMY8vdVWQE0SRAgCID4EOAEBSPch54M31EgiIa2kaGwUKACA2BDoAgJROV3vwzfXiBXn5O5I9BADwFAIdAEDKunP2KnHxRE6YdV8lrzACAHgRgQ4AIGWbgW4oKhGv2FtSluwhAICnEOgAAFKSE81AtffNvDuGWD9tmzUwuu9tew9bJa813Q4AUDfKSwMAUpKJZqBZGfWkd5uTpWfrJjJxYDvr/w9acveP5PWCnfLQW+vlm5JyMdlbZ/qkfjK8Rysj+wQAvyLQAQCkpH2l9oMPDWxemzy4xn8f16+t9aMV055YtFneXP2V7d9Jbx0AiA6pawCAlKKpX+OmL7VSwezSmZxoA6LHJ/WTvu2bign01gGAuhHoAABSxvvri63Ur08Lzaxz0XS1WEwd3ctKdzOB3joAUDsCHQBAyszk5L5YYM2GmNA9p3HYmpxo9Gnb1FpfYyrYobcOANSMQAcAkBLun7fWWJCTlibyyPg+cW2rRQRm3TbIqtBmF711AKBmBDoAgJTomWMqXU3dN6KHNTsTL912Tu4Q6XhKQ1vjKC2rsLU9APgZgQ4AwPemL9pkZD9pIvLbkT3kpvM6G9lf80aZtrZvlEXxVACoCYEOAMD3BQgWrCmyvZ/uOdnyxh1DjAU5qudp0VVtq8mho8es0tUAgKoIdAAAvi9AELC5H00xW/DLobbS1apz1cD2trbfWFwiox5dIpdPX2r9twIAvkegAwDwLVMFCE7JzhInaNU2E711tNS0ls3W2SsAwHcIdAAAvmSyAEG0jUHjYaq3jgZ0OnvFzA4AfIdABwDgS3krCo3tK9bGoMnqraPBzpR5a42MCwC8jkAHAOBL63YfNLIf7XcTa2PQZPbW0TQ2ChQAAIEOAMCnDpdX2t6HzrJMGd1LEiHYW2f+z86V01tl29rXb+asNjYuAPAqZnQAAL7UMDPd1vZpaWKllJmutFYXnT3KttkfZ/WuA/Layh3GxgQAXkSgAwDwJbs9ai7tlWOllHl1Nup3b7BWB0BqI9ABAPiS3R41uRd0Fa/ORqnS8kqZ++kuI+MBAC8i0AEA+JKdHjWJKEDg5GxU0PRFm4zsBwC8iEAHAOBb8fSoSWQBAqdmo4J27T9iZD8A4EUEOgAA34q1R42+LhkFCEzORoWqqAwYGQ8AeBGBDgDA16LtUaP/rq9LVgGC6maj7MpITzMyFgDwInv1KwEA8IBgjxptpJmXv0PWfXVQSssqpFFWhvRs3UQmDmyX1DU5NY35tKYnye79R+PeR5umDYyOCQC8hEAHAJAyNJhxW0BTm7su7i6/zPss7u1zhyWvchwAJBupawAAuNTYvm3iLjXdKDPd2h4AUhWBDgAALvbAmPjW6kyLczsA8AtS1wAAcLEr+reT7XsPyz/+HX1PnJwmWfJ/F2+VFz8utHryaLlqL6XsAYAJBDoAALjcf190hnQ4paH87o21UlpeWefriw6WWT+qoHC/zFxeaFWV0/5AyS6dDQCJQuoaAAAemdlZO+0S+dvEs+T0VtnSKCvd6vtzUkY9iaaItAY8E55cJu+vL07AaAEg+ZjRAQB4mpaMfiW/UNbtPiiHyyutxft+TtfSAgPBIgOf7/wueIm2LWhZxXHJfbHA6hfEzA4AvyPQAQB4kl7k3z9vrXxauL/Kv6VKupb+92vwEgt9/ZR5a62+QgDgZ6SuAQA8R9OvdCajuiAnVdK1Vu88UOd/f23vi86EAYCfEegAADw3k6PpV9HOZATTtXQ7P8lbUWhv+/wdxsYCAG5EoAMASJl0LT/RNUm2tv/K3vYA4HYEOgAAzyBd63taeMGO0rIK258HALgZgQ4AwDNI1/qeVpezo1EW9YgA+BuBDgDAM0jX+p6W0LajZ2t72wOA2xHoAAA8Y29pua3t/ZSupX2C7Jg4sJ2xsQCAGxHoAAA8QUtEF+49bGsffkrX0maofdvH1x9I+wv5sZkqAIQi0AEAeKakdMDmfvyWrjV1dC/Jyojtq1xfr01UAcDvCHQAAL4sKZ0K6Vp92jaV6ZP6RR3s6Ov09bodAPgdgQ4AwLclpVMhXWt4j1Yy67ZB1n9fbfTf9XX6egBIBf5JVgYA+JLdktKpkK6lMzRzcofIml0HJC9/h9UMVAsv6JokTdfTmSw/BnkAUBsCHQCAr0tKp6VJyqRraTBDQAMA3yF1DQDgaofLK21t3755Q9K1ACAFEegAAFxdbW3HPnslpU/NzjI2HgCAd5C6BgBwbd8cLSltt9qa30pKAwA8HOhUVlbKpk2bZN26dbJ79245cOCAZGVlSbNmzaRLly4yYMAAadSoUbKHCQBwuG8OJaUBAJ4PdAoLC2XOnDny3nvvyeLFi+XgwZoXn6anp8uFF14od9xxh4wcOTKh4wQAeKdvjl9LStul1dleyS+0Cj3oGqiGmenS87QmctXA9rxfAHzDFYHONddcIy+//HJMMz4LFiywfkaNGiX//Oc/pVUr+gIAgB+Y6pvj95LS8c6UaRBZ3ftbULhfZi4vtIJDfd9SoUodAH9zRaCzcePGap9v06aNdOvWzQpiKioqZMuWLbJq1So5fvz7u3zz58+X888/Xz744APJyclJ4KgBAG7um5MqJaVNr3nSgGfCk8us94/mogC8zBWBTqi+ffvKjTfeKJdeeqm1HifSrl27ZNq0afL000+HBUoTJkyQDz/8UNK0YQIAIGX75jTKTJeXbzmHIMfGmid9nb5+1m2DeB8BeJYryktrcKJrbfLz86WgoMBae1NdkBOc5Xnqqafk8ccfD3t+yZIlkpeXl6ARAwDc2jenXfOGXJwbWPOkr58yb62tzwIAJNUDnVmzZlkpaFpNLVq5ublyxRVXhD33wgsvODA6AEAi6cJ4OxpluS5ZwbNrnjSNTQsXAIAXuSLQ6dixY1zb3X777WGPFy5caGhEAIBk0epftranb47RNU95+TtsbQ8AKR3o2FnPE+rIkSOyf7/9Sj0AgOQZ0KG5re0nDmxnbCx+YHfN07qv7G0PAMni6fn9jIyqwy8vL0/KWAAAzpU+jhZ9c8yveSotq7C1PQAki6cDnU2bNlUJfE499dSkjQcAEH/p49tmrpRjlYG430L65lSPNU8AUpWnU9dmz54d9liLGdSr5+n/JABIyZmcW1+wH+TQN6d6rHkCkKo8GxWUlJTIM888E/bcuHHjkjYeAEB87py1SiqOB2ylq2m/F5pbVu+qge1tHZqseQLgVZ5NXfvNb34jRUVFJx43bdpUbr755qSOCQAQe+njDcUlcb9tf5t4lozt24a3vRa925wsfds3jWvtE2ueAHiZJwOd119/XR577LGw5x566CFp3txepZ5IX3/9tezZs8fWuiEAQM2mL7J3zvzXuiICnShMHd1LJjy5LKamoWn/KWRw39zV1qyQBkwA4CWeC3RWrVol1113XdhzF110kUyePNn475o+fbpMnTrV+H4BAN/J37bP1luRv/Vb3soo9Gnb1FrDlPtiQdTBjiYTflF0yPqZubzQmt2ZMrqXtS8A8AJPrdEpLCyUkSNHWutzgjp06CAzZ86UtDS99wQA8BK7pYtLyo4ZG4vf6RomXcukAUs8Cgr3W7NCWiEPALzAM4GOppFdeOGFsmvXrhPP5eTkyLvvvistWrRI6tgAAPGJvwRBEDe5YqGzMXNyh8j8n50rPzmng3TPaRzTO6izQTorpJXyAMDtPJG6tm/fPvnxj38sGzduPPGc9st57733pFu3bo793tzcXJkwYULMa3TGjh3r2JgAwE+yszLk6LFyW9sjdrreRn/GTV8ac7Cpwc6UeWutgAkA3Mz13xAHDhyw1uCsXr36xHPNmjWzZnJ69erl6O9u2bKl9QMAcMYZrRrLNyV7497+7E5mi9CkWsW7eCqxBdPY1uw6QIECAK7m6tS1Q4cOySWXXCIrV6488VyTJk1kwYIFctZZZyV1bACA+Gnq08V/+1CWbo4/yFGTh3XhY4hT3opCW+9dXv4O3nsArubaGZ3S0lIZMWKELF++/MRz2dnZ8vbbb8vZZ5+d1LEBAOL3zOIt8uBb6yVgc4GOri+h5HH81u0+aOv9X/eVve0BICUDnSNHjsioUaNkyZIlJ55r2LChvPnmmzJ48OCkjg0AYC/IeeDN9bbfwvrpafLI+D58FDZoj5xkVswDgJRLXTt69KiMHj1aFi1adOK5k046SebNmyfnn39+UscGALCXrvaggSAnM72ePHltf/q52NQwM93W9o0oBAHA5VwV6JSXl8vll19uVVMLysrKkrlz58rw4cOTOjYAgD13zl5lu5y09oCZPXmQ1RMG9vQ8rYm97Vvb2x4AUibQqaiokCuvvNJagxNUv359mT17tlx88cVJHRsAwH6Frw1F3zd7jseIM3OsksbaCwb2XTWwva3tJw5sx8cAwNVcEehUVlbKpEmT5I033jjxXEZGhuTl5VlrdQAA3vbndzfY3kfxwTIjY8F3tJBD3/ZN455ZoxAEALdzRTGCG2+8UV599dWw5x5++GHp27evbNu2LaZ95eTkWGt6AADuWZvzwcY9tvfD4nfzpo7uJROeXGY1AY1WVkY9mTLa2T52AOCbQOf555+v8txdd91l/cRq4cKFMmzYMEMjAwDYdf+8tbZLSSsWv5unaYDTJ/WT3BcLogp2NMjR15M+CMALXJG6BgDw79qcTwv3G9kXi9+doYUdZt02yEpHq43+u76OQhAAvMIVMzoAAH/KW1FobF8sfneOztBooYc1uw5IXv4OqxmopgrqLJoGmPre65oc/ff75q62mo1qHx4tUa3V27SwAWt2ALiNKwKdgImcBgCA6+Rv3WdkP91zGnMhnQAarFQXsOg6q3HTl1Y7O1dQuF9mLi+0Znx07Q5pbQDcgtQ1AIAj3l9fLBuL7ZWUVmlpIo+M72NkTIjvc9SCBXWlIGrAo6/T1wOAGxDoAACM0xkAXeBuYr7+vhE9mCVI8ucYbVU2fZ2+XrcDgGQj0AEAOFJpLZaSxTXp36Gp3HReZyNjQmI+R339XbM/5+0GkHQEOgAA11Zamzq6t5H9ILGf4xdFh+T/LdnC2w4gqQh0AACurLSmi9up5OXdz/HBN9eTwgYgqQh0AABGrdj2re19aGNKreCF5NES0nYcD4hMmbfW2HgAIFYEOgAAY7Ti1oaiQ7b2kSYi0yf1owBBkmmfHLu0Epv23gGAZCDQAQC4qtLaGTmNZXiPVnwqSabNQE3QBqQAkAwEOgAAV1VaG9ixuZHxwJ6epzUx8hau+8peChwAxItABwDgqkprEwe2M7If2HPVwPZG3sLSsgo+CgBJQaADALCNSmv+oxXv+rZvans/jbIyjIwHAGJFoAMASHqFLkWlNfeZOrqXpKdpeYj49WxtJgUOAGJFoAMAsF2EwO46DCqtuVOftk3l3pE9bO2DVEQAyUKgAwCwVU56wpPL5Ogxe0UIqLTmXjee20nOaJUd17Y0fQWQTAQ6AABb5aSptOZ/f5rwAyu1MBakIgJINgIdAEBSy0kr0pvcn8KmTVyjDXb0dTR9BZBslEIBACS1nDTpTd6gTVxn3TZIpsxbKwW1fPb6eU4Z3UvqpaXJfXNXW4UqDpdXWg1ItTePlq3Wim4A4DQCHQBAzKYv+tLIu0Z6k/dmdubkDpE1uw5IXv4OqwiF9snREtJaXU1n5o4HAtZsX3WBsAZIM5cXngiGdH8A4BQCHQBAzAUIFqwttv2ukd7kXTojU92sjB4b0azb0oBHi1hoepvOFAGAE1ijAwCIqQDB5JkFEgjYe9Ma1K9npUFxkesfsRan0Nfp63U7AHACgQ4AIGo3zVgh5ZX2CxD0PO1k0pZ8Jp7iFPp6XfMDAE4g0AEAROWW5/NlT0mZkXdL13PAP+wUp9A0Nl3zAwCmEegAAOr02sod8q91Xxt7pygn7S95KwptbX//vDXGxgIAQQQ6AIA6/WaOuQtRykn7j5aQtmPl9v3yzOItxsYDAIpABwBQqwfnrzOyLkdRTtqftE+OXQ++uZ7CBACMItABANSasvbPJVuNvENp2n9nUj+KEPiQNgO1Swv53TV7lZHxAIAi0AEAVEt7ovx69ufG3p1Lz8yhnLRP9TzNTHGJL4pKKEwAwBgCHQCAY/1yQuUO68o77VNXDWxvbF95+TuM7QtAaiPQAQBU2xPF1Loc1SI7S3q3OZl32qf0s+3bvqmRfa37yl5hAwAIItABABjriVKT/z2iO++yz00d3ctah2VXaVmFgb0AAIEOAMBwT5RILbMzZVy/trzPPtenbVMZekYL2/tplJVhZDwAwIwOAMBoT5RI/7x+IO9wivj1RWfY3seho8coSADACAIdAECYfaXlxt6Rm4Z0pJx0iq3VOSOnsa19bCwukVGPLpEBD74rrxfsNDY2AKmHQAcAcKLS2rjpS2Xb3sNG3pGLeraU317Wi3c3xfxpfB9JM7BY55uScvnVq6tk+J8X0UgUQFwIdAAAVs+cCU8uM1aEoMMpDeXp60hZS9W1OveN6GFsf5v3lFoBuB6jABALAh0ASHE6k5P7YoGUVZgpJ10/PU0evbqvkX3Bm246r7P8dmQPI1XYlFY6v/WFlczsAIgJgQ4ApDjtmWMqyNEL2yev7c+6HFjBzht3DJHuOdlG3o2K4wG5c9Yq3lkAUSPQAYAUZrJnjq7L+D8T+sjwHq2M7A/+SGNb8MuhMv9n58rprewHPBuKS6jIBiBqBDoAkMKmf7DJyH4y0+vJP68bIFf0b2dkf/BfNbZsQ/1xpi8yc8wC8D8CHQBI4Qprb68usr2vfu2byuzJg5jJQa0Ol1caeYf+tbaYtToAokKgAwApxmSFtY6nNJQ5uUNYk4M6NcxMN7ZWR49fqrABqAuBDgCkENMV1k7JzjKyH/hfz9OaGNuXHr96HOvxDAA1IdABgBRissKa6tna3MUr/O2qge2N7k+P4ynz1hrdJwB/IdABgBRhssJa0MSBFB9A9AUJ+rZvavTtKijcTxU2ADUi0AGAFKApPjfM+MToPrUIgV68AtGaOrqX1VDWpOuf/YQUNgDVItABgBQpPvBNSbmxfWZl1JMpo3sZ2x9Sp6+ONpQ1GerocU1xAgDVIdABAB8zXXwgGORMn9SPSmuIizaU1cayJoMdihMAqA6BDgD4mOniA5quNus2eubAHm0s+8+fDjCaxkZxAgCRCHQAwKfmfrrLaPGBkWe2pmcOjM7svDZ5sJyanWlsnxQnABCKQAcAfJiudvHfPpBf5n1mdL+Th3Uxuj9A1+zMuOFso29EXv4O3lgAlozv/gcA4AfPLN4iD765XgKG90uFNThddtrU7OOcT3dKQAJW3x6qAgKpjRkdAPCJB/5nrTzgQJBDhTUkouy0HmcmlJZVyszlhTLq0SVy+fSllJ4GUhiBDgD4ZCbnmaXbjO+XCmtIVAqbVvIzFeyErtkZ/8Qyq8Q6gNRDoAMAPliTo+lqpukicSqsIZHFCfR4M1mcQJVXHpebn18hr61k7Q6Qagh0AMDj7py9yni6mtJF4nqnHfBycQIVCIj8etbnzOwAKYZABwA8bE7BTtlQVGJ8vxQfQLKLE5imNwNum7mSNTtACiHQAQCP0nUHv561yvh+KT4APxUnCHWsMiBT5q01vl8A7kSgAwAeXZeT+2KBHDecs0bxAfi5OIGiqSiQOgh0AMCD7p+3VsoqjhtPV6P4ANxWnECPS9NoKgqkBhqGAoCHrNl1QB5fuMlYc8Wgv008S8b2bWN0n4CJmZ05uUOs416DE20Gqn1y7Fr31UE+HCAFEOgAgEdS1XQWx3SAo7rnZBPkwPUFCvQnIAGrGahdpWUVRsYFwN1IXQMAl/t/S7bKuMc/ciTISRORR8b/wPh+ASdcNbC9kf00yuI+L5AK+EsHABfP4tw5a5VsKDZfPjrovpE96JUDz5Wethv092zdxNiYALgXgQ4AuLR0tFZVM11wINRNQzrKTed1dmz/gFOlp8c/sUzKK+P/25g4sJ217ueV/EJZt/ugHC6vlIaZ6dLztCbWrJEGVAC8j0AHAFxaOtrJIOe3I3sQ5MCzBQqeuLaf3Pz8CgnEUV79jFbZ8ts31lQ7K6Slp3UNkFZ6mzK6F7OdgMexRgcAUqB0dKi/XvkDghx4vvT0/xnfx1pjFouMemmy9ZvDdaa+acAz5vGlcv2zn1gzPwC8iUAHAFxk9c4DjhQdCNI71eP6tXVs/0CiXNG/nfzzpwOkfnp04Y6+Li1Nok5509miRRv2yKhHl8jl05daM60AvIVABwBcJG+F/dK5NdEu85qOA/hpZue1yYPrbCqq/9751Gw5VhlHrtt/ZniueOIja+0cAO9gjQ4AJFnoouj1DjUy1CBn+qR+rDmA75uKajNQ7ZOjJaS1upoWHtDZmcseW2Lr92iQdOsLK2VO7mD+jgCPINABgCR5vWCnPPTWevmmpNzR38PCaqRSU9Hq3Dd3tZHfUXE8ID97uUA+uPNHRvYHwFkEOgCQYJrr/8u8z2TLnlJHf8+IM3Mkd1hXSuUi5elsqSnb9x6RS/72oTwyvg8zO4DLsUYHABJIc/y1B4jTQY7O4kyf1J8gBxCx+uSY9EXRIZnw5DLW7AAux4wOACSArh94fOEmWbCmSOJbDh09ig4A4bQZqGlaAv7m51bIJb1z5PYLmDkF3IhABwAcTlPTvjhOlowORdEBoKqepzWxKqeZpjct3l5TZP2cmp0p947oQfl2wEVIXQMAB9PUNL0lUUFO95zGMuu2QVbJXQDfu2pge8ffDi0q8qtXV8nwPy+i5w7gEgQ6AODQTE7uiwVWeovT6qWJ/G5UD1nwy/NZHA1UQ6ux9a2j144pm/eU0nMHcAkCHQBwgKarJSLI0aIDc28fIjee29nx3wV42dTRvazUzkTQnjt6o0NveABIHtboAIBhq3cecCxdrUH9dGu9QbARYk19QwBUbSyqTXMTNdOqv+Pqp5fLuH5trNQ5/laBxCPQAQDD8lYUOvaeju/fVh4Y29ux/QN+puvXdB3blHlrHSlOEKm0vFJmLi+0fmjcCyQeqWsA4OLmhJF0FgeAvZmdOblD5K9X/sBa35YoGljRewdILAIdAHB5c8IgvSNM+gtgxrh+beX/XjdAMtMTdymk6Wys3QESh0AHADzQnJAmoIAzqWyzJw+ybiIkMtjR1DkAziPQAQDDtFiASfXT06xF1JpyA8D7qWyaxrZm14HE/DIghVGMAAAi6AXIK/mF1lobTUPTGRoNXqKtnKSv08XHJnRp0Uj+OvEsghwgAalsTRrUT1hVtrz8HaSiAg4j0AGA/9CeF9r/prrS0HoHNtrKScHmhHZKTLfIzpL/PaK7dfEFwH9V2dZ95VzREgDfIdABkNKCszfLNu+VLXtKJRBl5SRNJdOLotqaE+rrYrkznJYmcmmvHMm9oCt3eoEkp7LpuWH6wk3y9toiCdR1YohDaVmF+Z0CCEOgAyDl6AXM4ws3yYcb91h9LuKtnKR3fmua2Ym1OaEWG6greAKQODozO/3a/vL++mKZPLNAyivNprM1yuISDHAaxQgApExwM3nmSun1uwUy6tEl8vaaoriCnFgqJwXTYOqq6KT/rq8jyAHcW5lN18uZ1LO12aIlAKridgKAlF13Y6pyUm0FCkLTYHTxsebla8qK3s3VCx1tAEpvHMDd9O/4/f81TF4v2CkPvbVevikpT1rzX7vFUoBUQqADwLc05cTpCkrRVk7S13ARAnibFgfRn+CNizkFO+OaGY6n+W80xVIa1K8n7Zs3lIGdmhP4AAQ6APwi8i5nIBCQTV+XSqUTq4hDUDkJSD3BGxcTBrSNuehIPM1/o71pc+TYcdlQXGL9aOBzUv160vGURjKgYzMCH6QkZnQAeJrJVJJ4UDkJSF3xFh2JpfmvzuTEOzN99Nhx+aLokPUTbXl8wE8IdAB4q1raok2Sv3WfHDpakZCmfnWhchKQ2qLtvRNvkKHpaqbOddGWxwf8gkAHgOvpHc07Z62y0jHchspJAJwqOrJ65wHjhVQ0aLrl+ZXStnkDOXDkmByrOC710+tJzslZMqAja3vgLwQ6AFy91uZY5fGoGnkmS7yVkwD4j+miI3krCsUJunZx+97Doc/I/iPH5Iui79b2kOIGvyDQAeD7MtBOiadyEgBES2/4JIOmuI17/CPpeGpDa7aHEtbwKgIdAClRBtq0eConAUAsdFY7WXTWZ/Oe0iolrJntgZfUS/YAAKQ2OxWFkiWeykkAECudSXGbYEEDvUEFuB2BDoCkMllRKBH0bqZWWKJiEQCn9TytiSvfZD1n6w0qvVEFuBmpawCMlXzWKkNaNCA7K0PO7tRccod1rXUNixMVhUxLE5EuLRrJoC6nxl05CQDicdXA9la6mFuDHS2prdXmoi0uw1ofJBqBDoA66ZfV9EWb5JOt+6SkrMK6+Nf0rYrjASkpq5pDfvRYuby1usj66Z7TWB4Z36faNC+nKgqZQi46gGTSGyt92zd17Q0hTWPT74fIG0C1FZdhrQ8SidQ1ADXSL6uL//qhjHp0iRW0fFNSbnXaPnLsuOw/UlFtkBNJO3Jf8cRH1eZzJ6uiUG0aZaXLiN45Mv9n51p3KlmHAyCZpo7uZd1YcivtGxRKz/W6hqeu4Iy1PkgEZnSAFBRMJ1ix7VspOni02oZxxQePym0zV8qxSvsdbHQfk2cWyOzJg8ICh2RWFIoMboZ2ayG5F9SeagcAiabnTC1+4taiLdocNd7iMsG1PrrusaabSqS/wQ4CHSCV1tIs3CQfbtwjpdUGGOEN4zQ9zWSTzvLKqvncia4oVC9N5PRWja3/327HcgBIFC1+osGAnkN1JsRNdG2mneIyNa31If0NJhDoAD4XbyNOk0FOTfncWlEoUV/amen15Ilr+1EtDYAn6YyHBgN6DtV0sfxt+2Tb3lIrnTiZ9KaR3eIykd8N0fZWC6a/6YwXlTBRHQIdwMXsTtm7sRGnfkEHx56oikJaNe2vE89ivQ0Az9PzZ+j5PzTw2b631FpDmUg6M26iuEzwu8GJ9DekLgIdIEmBy77ScquCmfynHHPzRpkngpjjgYDtijVubcQZms/tdEWhU7Mz5d4RPWRcv7aO7B8A3Br46Lk2mFa2sfiQHHdiml7ESv81UVwm+N1gMv0NINABEhDU7C0tl28OldWwNkasambb9h4+EcSkpYkE6vhSqmvK3q2NOEPzuYMVhfS/w8RYtTJRp1MbycCOzVl7AyAlRQY+Ts7u6w234O+yW1xGvxtMpr8BikAHcMmamFB1BTl1Tdm7uRFnMJ873opCWlBAZ8COVepr0040J508rAtfcACQoGIGemNJswpMFZfR7wZT6W9AEIEOYFAy1sRUN2Xv5kacwXzueL6EaeAJAGaKGazYvk++OnDUummk7QWyM9Nl94GjUaW4aZCjN6hCb7DZLS6j3w1rdx8QU6nRfluPS5nt+BDowPcSdXJI5pqYyCl7NzbijMznrutLOJhfThloAHA2tS30eyzeG052i8vod8OvZ+0Tk6nRbhdNCe3uOY0lEAjIhuKSGl/DTcCaEejAfb1eFm2S/K37rBOW3lgKpiblDoutmWOia/Ane01M6JS9Wxpx1pbPHc+XMADAOXZuONkpLhP8bjCR/ua3DJAvig7VuS/KbNfMO0cEfD1TokHJnbNWVXvH4uixcnlrdZH1o3c2Hhnfp87AJNE1+N2wJiZ0yj7RjTij7WMTms8NAHCneG84xVNcJnStj4n0Ny9wIgOEMtseDnS2bt0qn332mezevVtKSkqkdevW0qFDBxk8eLDUr18/2cNzhUTmbpqeKdGg5NYXVkpFFInBemfjiic+kiev7V9jYJKMGvxuWBMTOmWfyEac0aifnmY166THAQD4V6zFZSLX+phIf/MCpzJAKLPtsUBn9uzZ8pe//EWWLVtW7b83b95cJk6cKNOmTZNTTz1VUlGi07NMz5To+G+bGV2QE3SsMiCTZxbI7MnVBybJqMHvhjUxoVP2iWrEGY1oZ+EAAN5np7iMifQ3t3M6A4Qy2+HqiQvprM3VV18tEyZMqDHIUfv27ZMnnnhCevfuLe+8846kGg06NJio6w8mGHTo6+2Id6ZEt6uJBiUauMSqvPK7wCSSiRr88XDDmpjQKfvgl4UJaVG8JjsrXZo2rC8N6teTBvXTpUV2low8s7XM/9m5suCX5xPkAEAKrvXR74CfnNNB+ndoZt300v/Vx/q8/nt1N8A0/U1neuyUunazRGSA6PoquHRGp7Ky0pqleeutt8Keb9GihfTt21dOPvlk2bx5s3z66adWFQpVXFwsY8aMkffee0/OPfdcSQXJSM8yPVNi965GdXctklWD3w1rYiKn7E004gymFbRqcpI8sWizfLJ1n5RYKXIByc6qLwM7NYu5SAQAIDXEs9bHbvqb2yUiA8SrZbZTItC55557woIcXYOj6Wu33HKLZGZmnnh+3bp1cvPNN5+Y8SkrK5OxY8fK6tWrrTU8fpfo9CwnuhWbuKsRGZjYPYHEe3JI9pqY6qbsY/2yqCut4PFJ/YyNFwCAmvi5t1oiMkC8VmY7ZQKdLVu2yN///vew52bNmmXN1kTq2bOnvP/++zJ8+PATwc7evXtl6tSp8uSTT4qfORF01MWJmRITdzUiAxO7J5B4Tw7JXBNT25R9tF8W9dK+m5Vq16yhDOjYvNYSogAAOM2vvdUSkQHipTLbTnPVO6FByrFjx048vv7666sNcoIaNGggM2bMkDPPPFPKy8ut55555hm56667pHPnzuJXyUjPcmKmxMRdjcjAJFk1+O0soLQjmil7v35ZAAD8z2+91RKRAeKVMtspFegcOXLEqrIW6u67765zu9NPP91KWXv11VetxxUVFfLSSy/JfffdJ36VjPQsJ2ZKTNzViAxMklmD38SamMgiALWVaYh1yt5vXxYAAHhNIjJAvFJmO6WqrmnVtMOHD594PGjQIOnevXtU295www1hj+fMmSN+loz0LCdmSjQosSsyMNETSLJODsE1MbFWi6kpiHnjjvgq1gAAAHcyWRXVy2W2U25GZ8GCBWGPhw0bFvW25513nmRkZFizOUorsmkltlat4u9072bJSM9yYqbExF2NyMAk2TX4o10T0ygrXZqclCEVld8/PiU7q9pUMk5YAAD4h+kMEC+W2U65QGfNmjVhj3VGJ1qNGjWy1ulogBO0du1a3wY6yUjPcqJbsd11LTUFJvGcQEyeHFgTAwAAartOsFMV1Q9ltlMudW39+vVhj7t27RrT9l26dAl7rOWn/SoZ6Vl2plprmynRoKR+ejQtKcNlptccmMSaQubUyUH/mx8Y21temzzYapqp/6uPmaEBACC1BTNA9BqpNpq2fkar7Fpfo/vQfek+4cIZnX379lk/odq3j+1iPvL1X375pfhVstKznJgp0eDiyWv7y60vrJSK47Utvf+eBkZPXFt7YOLnGvwAAMD7YskAoXKqhwOd/fvDL0QbNmxopaPFomXLlmGPDxw4YHtcX3/9tezZsyembTZt2iSJkIz0LKe6FWtQMid3sNw5a5VsKC6p887GI+P7RBWYkEIGAADcLpqqqFRO9XCgU1JSUqU/Tqwitzl06JDtcU2fPt3q7eNGTgUddXFqpkRf986vhlp3LKYv2iT5W7+VkjLtqZQm2VkZcnan5jJ5WJe4ZqM4OQAAAKQeVwY6J510ku1AJ3KffpSs9CwnZ0p0u+mT+hsZJwAAAFKXKwKdSGlpaQnZxg+SmZ7FTAkAAADcyhWBTnZ2eDWJI0eOxLyPyG0i9xmP3NxcmTBhQkzbaLW3K6+8MuFrdjTMu6pbmki3iKBm/05Zu39nQsYAAAAAf9oUcU1bVlYmbkegU0eBg8giB7EeBGPHjo3vkwEAAABcaseOHdKvXz9xM1f00Tn55PBZiMOHD0tpaWnMFdJCNW1KuWAAAAAgVbki0DnllFOkWbNmYc8VFhbGtI/t27eHPe7WrZuRsQEAAADwHlekrqkePXrIRx99FJYCps9Fa8uWLVX2lwxDhw6VuXPnnnjcrl07ycrKcvR36nsVmiKnv79r166O/k6A4xBuxPkQbsBxCD8eh2VlZVa6Wug1r9u5JtDp3bt3WKCzbNkyueyyy6LaVtPcPv/88yr7SwZNmRszZowkkx7EvXrF3xgU4DiEX3A+hBtwHMIvx2E/l6/JcWXqmrrkkkvCHi9atCjqbRcvXiwVFRUnHvft21datWpldHwAAAAAvMM1gc7FF18c1vRTZ3S++OKLqLadMWNG2ONx48YZHx8AAAAA73BNoNOwYUMZP3582HN//OMf69xu48aN8vrrr594nJGRIddcc40jYwQAAADgDa4JdNSUKVOkfv36YTM18+bNq/H1R48elRtuuEHKy8tPPHfTTTdJly5dHB8rAAAAAPdyVaDTuXNn+cUvfhH2nM7yPPbYY2HBjFq/fr0MHz48rICBlqm+//77EzZeAAAAAO7kmqprQX/4wx9k7dq18vbbb1uPjx07Jj/72c/kgQcesCo9NG7c2ColXVBQIIFA4MR2mZmZVgpb69atkzh6AAAAAG7gukAnPT1dXn31Vbn55pslLy/vxPNff/21LFiwoNptWrZsKc8995ycd955CRwpAAAAALdyVepaUHZ2trzyyisya9YsOeecc2p8XfPmzWXy5MmyZs2aKuWpAQAAAKQu183oRK7P0Z+tW7daqWq7d++2moPm5ORIhw4dZMiQIVbKGgAAAAB4JtAJ6tSpk/UDAAAAAJ5NXQMAAAAA38/ooHYtWrQIK6utj4FE4ziEG3Acwg04DuEGLbg+lLRAaI1mAAAAAPABUtcAAAAA+A6BDgAAAADfIdABAAAA4DsEOgAAAAB8h0AHAAAAgO8Q6AAAAADwHQIdAAAAAL5DoAMAAADAdwh0AAAAAPgOgQ4AAAAA3yHQAQAAAOA7BDoAAAAAfCcj2QOAfxw/flw+/vhj2bJli+zevVsyMzOlTZs20qtXL+nRo0eyhwebysvLZePGjbJ+/XopKiqSgwcPSsOGDaVZs2bSvXt36du3r2RlZfE+I2pbt26Vzz77zDpflJSUSOvWraVDhw4yePBgqV+/flLfyYKCAvnyyy9l165d1mM9l51++unWcQ7vq6yslE2bNsm6deus4+/AgQPW+UvPZ126dJEBAwZIo0aNkj1MwBUKvHw+DMCVfvrTnwb044n35/7770/YWA8dOhS4++67A6eddlqN4+nTp0/gqaeeChw/fjxh44J969evDzz88MOB4cOHBxo0aFDrMZeZmRmYMGFCYPHixQl76zt06GDr72ThwoUJGyu+N2vWrMCgQYNq/FyaN28emDx5cmDPnj0JfdvKy8sDv//97wNdunSpcWxdu3YN/OEPf7BeC2/Zvn174K9//Wtg5MiRgSZNmtR6bkhPTw9ccsklgfnz5ydsfEOHDrV1Pnv22WcTNlbET6/P7HzOen2YCOU+OR8S6LiUVwKdjz/+ONC5c+eox3XhhRcGiouLEzI22DN48OC4j78bbrghcPDgQcc/AgIdb9GbIldddVXUx1GrVq0CCxYsSMjYNm7cGOjXr1/UY+vfv3/gyy+/TMjYYN/VV18d9/ls1KhRgaKiIsc/BgKd1OCFQGejj86HpK4hbjrlf/HFF8v+/fvDnu/du7ecccYZcvjwYfn8889PTHWqd999V0aOHCkffPCBlfYE99qwYUO1z3fu3Fk6deokLVq0kKNHj1rpbHoshHr22Wet7d955x3Jzs5O0Ijh9lShiRMnyltvvRX2vB5Hmv5w8skny+bNm+XTTz/VG3DWvxUXF8uYMWPkvffek3PPPdexsWkq5oUXXijbt28Pe75r165W6q2OZ+3atdb4glauXCkXXXSRLF++XFq2bOnY2GCGnqeqoyk43bp1k1atWklFRYWVer1q1SorFTto/vz5cv7551vfWzk5OXwk8LUin50PCXQ8YvHixdK2bduoX9+0aVNHx1NaWmoFLKFBjq7TeO655+Tss88Ou7jJy8uT2267TQ4dOmQ9t2LFCrn55pvlpZdecnSMMOe8886TG264wTqR6YVBdRcR99xzj7z++usnnvvoo4+sz33mzJkJ+Sh0XEuWLIlpGy5aEkePj9AgR9fg/OUvf5FbbrnFWs8XpEGznh+WLVtmPS4rK5OxY8fK6tWrrTU8pukFre4/9Etdf8+MGTOs4z3UggULrL8DvRAIrjEaN26cddylpaUZHxucoYH1jTfeKJdeeqm1HieS3pybNm2aPP3002HnuAkTJsiHH36YsM9aj69YnHrqqY6NBc55+eWX5Zxzzon69U7ePDzux/NhsqeUEF3q2tatW131Vk2bNq1KnubevXtrfP0nn3wSqF+/ftg2H330UULHjNi0bNkyMGnSpMAXX3wR9TZ33XVXlSltJz/n0NQ1/f/hTps3b67y9z937twaX3/48OEqa3huvfVWR8b2/PPPV1kfVNv5dsuWLYFmzZqFbfPyyy87MjaYM2DAAGttTn5+ftTbPP7441XOZ05+1pGpa0iN1DU3rRV93ofnQ/6SXMrNgc63335bZSHnokWLYv7jvuCCCxIyXsQnnmNOi03oBUXo55ybm+vYR0Cg4w3XXXdd2DFx/fXX17nNhg0brAIXwW0yMjKsgMmkioqKQKdOncLGNmPGjDq300XfodvoYt3KykqjY4NZ8X6HXnHFFWGf9YgRIxz7aAh0UoNbA50Kn54P6aODmL3xxhtWaeEgnXIdOnRondv94he/kJNOOunE44ULF8qOHTv4BFyqY8eOMW+j09W5ublhz+nnjNR15MgRmT17dthzd999d53baelSTaEI0vUTptNdNcUiND1I0x+vvfbaOrf7yU9+EpbCqbnqmqoJf53P1O233x72mPMZ/GqJT8+HBDqIWeg6DKU5mtHQ/gS6sLi2fcH7Iuvqa48KpC4tSKGFSYIGDRpkreeLRuS5Zc6cOUbHFnn+ue666yQ9Pb3O7fQ1kRcApscGd57PNHCPLMAD+MHrPj0fEugg5oVqWjkt1LBhw6LePvK1b7/9Np+Az2RkZFRpNIrUpQtW4z1faBGM0ONJK7JpJTY3jI1zWWqezxTnNPjRAp+eDwl0EBMtvRl6d7Z58+ZWikm0tON5KC1RCH/RbuOhnKiUBe9Ys2ZN2GOd0YmWdqY/88wzHTlnaDW3yGM1lspHkecy7RrOBbD/RB4jGvhQ3Qx+U+bj8yGBjkc89NBDVh8JvWjMysqyykdrPxMt9/fAAw9YdzoTIbJfitZVj0VkKU9doxMsOw1/iFyPEVpu3Em6bkzLWZ911lnWhYiWLNb/1Z5OWhb2H//4R1hPJyTG+vXrjZ4zIs9B8dI+T1r+Pkh7PzRp0iTq7fW1oRe8uq+aerXAP+ezAQMGSL16ibl00nWtev7UY1PPZ3pjUXv+XHbZZfLII49wvPnEU089JT/+8Y+tdS66jrlx48bWmjJd+3zvvfda7UWctsHP58NkV0NAdFXXovn50Y9+FFixYoWjb+mf//znsN85fvz4mPeh5QpD91FQUODIWJF4hYWFgZNOOins883Ly0tI1bVofrSKl1b8Ki4udmxM+J6WnI/8DEpKSmJ6i/77v/87bPuf//znRt7i1157LWy/Wi0wVpGdw+fMmWNkbHCHQ4cOBXJycsI+4z/+8Y8Jq7pW10+9evUC48aNC2zatMmxMcH5qmvR/Oj56d1333Xs43jNx+dDZnR85N///rc1ffj444879jsiF2HG0wE3cpsDBw7YHhfcQSuuHT169MRjnXW8/PLLxS10Kl0bn+msjzb+g7MizxcNGza00tHccL7gXIa6/OY3vznRDFFpJoU2s3XTmlldQN6vXz957bXXkj0cOEgbrWsGj87waGsY0/b7+Nqu6io7uIqmeYwYMUL69+9vpeCcfPLJ1kG+Z88eyc/PtypbhJbx0wu5O+64w8ojvvXWW42Pp6SkJOxxgwYNYt5H5DakrvnD3/72N5k/f37Yc5ouVt1iXpM0jUTTSS6++GL5wQ9+IJ06dbKm0XUtmVZ807+PF154QbZt23Zim6+++sr6u9JgRy8S4Aw3ny/cPDYknwYQjz32WJUUck0fc5quS7v00kutGzJ6DaABlq6h+Prrr2XZsmWSl5cnq1evDkvbnThxosybN886r8EbNFVNPy9NT+zRo4d1bOn32d69e6WgoMD6PtWqlUF67ffwww9bAe7vf/97o2Mp8fP5MNlTSqjeSy+9FFi2bFlUb8+CBQsCrVq1CpsyTE9PD6xcudL42/tf//VfYb/n3nvvjXkfgwcPDtuH/rfC29555x2roWPo56rHitP+8pe/RNUIUBuh6Wvr168fNsY2bdoESktLHR9nqlq6dGmV9ztWTz/9dNg+LrroIiNje+ihh8L2O2nSpJj3cc0114Tt4+GHHzYyNiTXZ599FsjOzq5y3GlDZCc99dRTgTVr1kT12pkzZwYaN24cNkYd886dOx0dI+x78803re/MaI6n/Pz8QLdu3aqkss2dO9foR/GQj8+HpK5F0NkQbXro9M+UKVNqDUCvvvrqqCte6J1svcsTOm2oC8HuuececZr+tyRim1TjluMwGloIQxf7a0PHIJ2B1Nkcp/3qV7+KqhGg1vnX1+rC4tCFxFqcQGeikBhuPl+4eWxInMLCQhk5cmTYHe4OHTrIzJkzHf+8b7nlFunVq1dUr500aZK8//77VjpokI556tSpDo4QJugsjqahRXM8abbC8uXLq1S31eu70OIBpqX56HxIoOMTmq7z5JNPhj2n/W4iywXalZ2dXaV5Wqwit4ncJ7xDq6pccsklVupEkDaD1Br6Wj3GbUaPHm0FkaGeeOKJpI3H79x8vnDz2JAcmhp24YUXhlVnzMnJsb5LW7Ro4bqPZeDAgfLggw+GPffcc89JaWlp0sYE8zSl7eWXXw4LJL744gtZuHChsd+R7ePzIYGOj4wbN65Kx/HIBlB2+fmPAbHZunWrDB8+3Lo4CC0FrHcZ3XhREHonLPQLY+fOnVV6vcD/5ws3jw2Jt2/fPqvEb2hJXC2X+95771klnd1cACa0DLCu0zV5AQx30LWkOgvk1PVdto/PhxQjiDBmzBhp27at42+89sRxgqaxaaQf9PnnnxvdvxZDCKVFEWIVemGsdKElvHUcav+jH/3oR1aQEJreoZX/TjvtNHEz7UXVp08fWbVqVdjfSe/evZM6Lj+KPF9ogQi92xxL5TWnzhecyxBaHUovIkMX+Ddr1syayYk2lSxZtK/eBRdcIG+88UbY+WzUqFFJHRfM0+yJ0OIEJq/vTvbxtR2BTgSdttYfr4pcrxDPwVqbyDtb27dvj2l7vdDRiiJ2GgimAjcfh1qxTIOc0CpmWj1GZ3Lat28vXvk7CQ10TP+d4DunnHKKdcH47bffhq2B0ApD0Yo8x5i6u273XFbdNm6+84/qaWUovYBcuXLlied0hkTvlmvVMy9w+nsf/v+cu/n4fEjqms9ElveLZ/qxNpEXKJs3b45p+8jX66yFdgGGNxQXF1tBTujaL81h15mcyA72qfx3gprPGbGuG9yyZUvY41iCpNpouX4tUhF6NzKWcqi6Lu2bb7458Vj35ZYvdkRHZxd1Ybgu9g5Nt9E1hlry1ys4n6UGJz/nM3x8PiTQ8ZnQAy2YY2ySNoAMrfKiszOhOc11Wbp0adhj0oW8dWzpmpzQ1Ehdi6MzOZEVYVL97wQ1/41rhchYLkQj0zNMnTM05ScyOI9lbKH9y5R+qes+4Q16kajpXUuWLDnxnH63vfnmm1bjbS/hfJYanPycs3x8PiTQ8ZmPP/447LHp9RIapeuCzVCLFi2KevvI12pTNHhnoe7atWvD0pI0yOnZs6d4iZbkDE1TUW5fV+RlmhYU7/li8eLFYWXL+/btK61atXLF2DiXedfRo0etCoyhn6FWidSGm+eff754jdPf+0iNz/kSn54PCXR8RKca9cIz1LBhwxyp7hbq2WefjWo7zdPXL5JQY8eONTo2mLd//35rvVDompbgQl3t4O01mpYSumYkIyPDseIg+K5ASmjKhd4lDJ0VrM2MGTNqPffYFbm/F154IareFPoa7avi5NjgDK1Kdvnll1vV1IL0zvPcuXOtGWuv0QIKoUUUnPreR/KD8zlz5jj6OY/z6/kw2R1LYc71118f1pW2SZMmjnR9//bbb619h/6uRYsW1bndlClTwra54IILjI8NZh08eDDwwx/+sMpx9cknn3jyrS4pKQn06tUr7L/nxz/+cbKH5Xs/+clPwt5zPVfVZcOGDYHMzMwT22RkZAQ2bdpkdFwVFRWBTp06hY1txowZdW6nrwndpkuXLoHKykqjY4N5x44dC4wZMybss6tfv37gf/7nfzz5duvxO3z48LD/nq5duyZ7WHBA5PVTenp6YNu2bUZ/R4VPz4cEOi701FNPBfbv3x/1648fPx747W9/G3ag6c8DDzxQ57Zbt26tsp0+V5dp06ZVObnu27evxtfrhXHoRYv+fPTRR1H/NyLxNEg+77zzwj6z7OxsRz63WI/DPXv2BJ5//nnrxBxL0HbppZdW+T2LFy829F+BmmzevNm6oAx93994440aX3/kyJHA4MGDw15/66231vkGR362CxcurHMbPY5Ct2nevHmtx57+W7NmzcK2efnll/nwXU7PFVdeeWXY56bB85w5cxz5fbEei//4xz+s4z5aZWVlgRtuuKHK73nhhRcMjB5O0fNNUVFRTNs8/fTTgbS0tLDP+aabbqpzO86H3yHQcaEOHToEmjZtGpg8eXLg3//+d+Do0aM1Bjj678OGDatyQJ955plRzebEG+jonfGOHTuGbde9e/cqd/o1qn/ppZcCjRs3Dnvt1VdfHcM7gkTTL1Gd6Yi8g6QnaT0+Yv3RO6kmj8Pg6zt37hy4//77A6tWrarxDpJePDz33HPWayN/x4033mj7vUJ0fv3rX1e5k/7oo49ax1qodevWVQlyTjnllMDu3bsd+WLX4yZy1rJ169aBd955p8prFyxYEMjJyQl7rY5Vz8Vwt+uuu67K8fHII4/EdT6LJiCJ9VjU1+ixpX8ny5cvr/Gcqc/PnTs3cNZZZ1X5HXrO5lh0t6FDhwYaNGhgHY/z58+3rqVqkp+fHxg3blyVz7lNmzaBr776qs7fxfnwO2n6f5KdPoeqtdJD65HrGgItqaqlmLWpk35kWn2joKAgbK1BUKdOnaxKMtEsVNNeKPr6yI73kfXaq7Nu3TqrOo02Wwul6za0CpfmlOq6jtCmkmrAgAHywQcfhFVvg7tUd1zYUdcxFetxWN3rtQmlVuRq2bKl1QdDqyppzx/9OykrK6uyj5EjR1p5+fr3BedpHvdll11mrZEKpZ+Xdv3WMvNaSlo/r9CvpczMTGs9xXnnnVfn70hLSwt7rB3io8lj1+PknHPOsXr8RFYO0oaROh4txBFZGluPTy1NbLJAApwReWzYEc1xFeuxGPl6XTekx542ONbv/WPHjlnrcLWQSklJSZXt9XtVy/zTrsHd9BjQ65+gevXqWecZPZfo56wFn7SarV47aTuHSM2bN7e2j6b6JOfD//hPwAOXzehERuLR/kycOLHWFDJTMzpBeuepujvlNf3oHafi4uI43xkkSnXHhZ2fuo6peGd04vnRmYQHH3wwprQ3mHHo0CHrHBXtZ9WyZcvA22+/HfX+47mDGbomqG/fvlGPrV+/foEvv/wyzncCiWbyfBbNcRXPjE48P5rS9POf/zymtDckd0Yn3s9a12Pt2LEj6t/F+fA7VF1zod/97ndWVRi9kxMNvQtw3XXXySeffCKvvPKKVRErUX74wx9adx7uvvvuWserszxPPfWU/Otf/7Lu4AJ2aP+eadOmWVWSor2D2a5dO7nnnnusu/L33ntvWHM0JIY2Y9Rz1KxZs6wZlJroXcvJkyfLmjVrqpQ8dYrOQmv51t///vdWv7CaaK8JfY3O5HTt2jUhY4P//elPf7Kal2rZ/mjPgbfffruVWfH3v//dKo8N9/vFL34h11xzjXTo0CGq12umglYw01lt/dHMnkQ43UfnQ1LXXE6nqtevX2+lf+3Zs0cOHz5sTUc2bdrUuhjQAELT2kxOy8fr+PHj1sGu6Se7d++2Uk40fU6nWL3WawXeoTeuNM3tyy+/lF27dlnpnJq2psefBv0aWGtaR5s2bZI9VETQz01T1fR8oc1Bc3JyrAuAIUOGWJ9fMmmKkDZD1rEpPZfpl3///v2TOi74n37fb9iwwfpfTWPS85nemNHzmTaJPOuss6o0d4Q3WzdoSuyOHTusNDW9vtPrKL2+089ar+369OnjiptyKz18PiTQAQAAAOA7pK4BAAAA8B0CHQAAAAC+Q6ADAAAAwHcIdAAAAAD4DoEOAAAAAN8h0AEAAADgOwQ6AAAAAHyHQAcAAACA7xDoAAAAAPAdAh0AAAAAvkOgAwAAAMB3CHQAAAAA+A6BDgAAAADfIdABAAAA4DsEOgAAAAB8h0AHAAAAgO8Q6AAAAADwHQIdAAAAAL5DoAMAAADAdwh0AAAAAPgOgQ4AAAAA3yHQAQAAAOA7BDoAAAAAfIdABwAAAIDvEOgAAAAA8B0CHQAAAAC+Q6ADAAAAwHcIdAAAAAD4DoEOAAAAAN8h0AEAAADgOwQ6AAAAAHyHQAcAAACA7xDoAAAAAPAdAh0AAAAAvkOgAwAAAMB3CHQAAAAA+A6BDgAAAADxm/8Pyt2TK5f1mK8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 900x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "URL_function = 'https://github.com/dmaluenda/hands_on_machine_learning/raw/master/data/curve_fitting_data.npz'\n",
    "data = np.load(get_npz_remote(URL_function))\n",
    "print(data)\n",
    "\n",
    "x = data['x']  # <-- YOU CAN USE THIS x IN YOUR CODE\n",
    "y = data['y']  # <-- YOU CAN USE THIS y IN YOUR CODE\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "plt.figure(figsize=(3,1))\n",
    "plt.plot(x[:100], y[:100], '.')  # we plot just the first 100 items, to save time and better visualize\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this dataset is huge, write a function to return two batches arrays (for $x$ and $y$) of a certain length given by the $M$ `batchesize` argument.\n",
    "\n",
    "So, randomly pick a $M$ number of elements on the whole $x$ input array and their corresponding $y$ output pairs.\n",
    "\n",
    "> Check the `np.random.randint()` function to generate random indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your function by plotting a batch of $M=40$ data points and compare the shape with the one above. They have to be very similar, but with different sampling points.\n",
    "\n",
    "Compare the randomly initialized model with the ground truth dataset, using just $M=40$ randomly picked points. Use the same $x$ input samples to make the prediction of your model, and plot both, the ground truth and the prediction on the same figure.\n",
    "\n",
    "> Check the `plt.scatter()` matplotlib's function.\n",
    "\n",
    "Add a legend to easily identify which data is from the ground truth and which is the prediction.\n",
    "\n",
    "Set the $x$-axis limits to $[-4, 4]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cost function (also known as loss function)\n",
    "\n",
    "The fitting shown above is probably poor. However, we need to quantify the 'quality' of the current fitting by a numeric value. In other words, a cost function is a function to evaluate the quality of the fitting for that given trainable parameters.  We will use the most simple cost function: the square of the discrepancy between the predicted values with the ground truth, averaged over all the evaluated samples. Notice that this expression produces a **single scalar value** that quantifies the prediction quality, known in this case as mean squared error (MSE).\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{2.2}\n",
    "C(\\omega; y) = \\frac{1}{2} \\left\\langle \\left[ y\\,(x; \\omega) - y_{\\rm truth}(x)\\right]^2 \\right\\rangle_{\\!_M}\n",
    "\\end{equation}\n",
    "\n",
    "where $y(x; \\omega) = g(x; \\omega_0, \\omega_1)$ is the prediction inferred by the current state of the model (given for that specific $\\omega$ parameters), and over a certain $x$ input set. $y_{\\rm truth}(x)$ is the output pair of every $x$ input, taken from the ground truth dataset. $\\left\\langle \\cdot \\right\\rangle_{\\!_M}$ denotes the average over all $M$ evaluated samples.\n",
    "\n",
    "> the $1/2$ factor is just to make things easier when deriving that cost function, as we will see in a while.\n",
    "\n",
    "[Check this to see more cost/loss functions](https://www.geeksforgeeks.org/deep-learning/loss-functions-in-deep-learning/).\n",
    "\n",
    "<hr>\n",
    "\n",
    "Usually the cost is calculated just for validation purposes. For fitting/training purposes, its derivative makes the work. However, we want to visualize its evolution in this example.\n",
    "\n",
    "Thus, write a function that returns the MSE-cost for a given set of pairs of predicted and true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cost function several times with different random picked samples. Are the results always the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, the cost function should not depend on any specific $x$ input data, nor on $y$ output, since it evaluates the fitting in whole terms. However, the cost function $C$ indirectly depends on the dataset $(x, y)$, see Eq. (2.2). Even thought, the main dependency is on the $\\omega$ free parameters, and $x$ and $y$ is just the supporting context on that single computation. We will focus on this in a while.\n",
    "\n",
    "Since this mini-model have only two trainable parameters, it is worth to check how the cost looks in what is called the *cost landscape*. This is an image where their axes represent the trainable parameters and the color (i.e. The value) is for the cost reached by the parameters' combination on that $(\\omega_0, \\omega_1)$ point.\n",
    "\n",
    "Create a function taht returns a $40\\times40$ matrix containing the MSE-cost when $\\omega_0$ is in range $[-3, 6]$ and $\\omega_1$ in $[-2, 3]$. The MSE-cost have to be evaluated using a given $(x, y)$ argument, correspondeing to the values from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this function using $M=20$ randomly picked $(x,y)$ samples on the dataset, and depict the obtained matrix with the `plt.imshow()` function. This is the cost landscape. \n",
    "\n",
    "Run several times this code cell. Is the landscape changing? Slightly or greatly? Why?\n",
    "\n",
    "What happens when using more/less randomly picked samples? (by increasing/decreasing $M$ `batchesize`)\n",
    "\n",
    "Should the cost landscape ideally depend on the specific $(x, y)$ set of values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Stochastic gradient descent\n",
    "\n",
    "To improve the model, we need to understand how the cost changes when we slightly modify the parameters. This requires computing derivatives with respect to each parameter. Indeed, the gradient descent algorithm is based on stepping down on the cost landscape to reach the minimum cost, i.e. Following the gradient of the cost function (indeed, just the opposite, since gradient is for stepping up, but it is the same with a minus sign).\n",
    "\n",
    "In this example, we are dealing with just two trainable parameters, then the cost landscape have just two dimensions, and it is quite easy to explore the whole plane (just like before) and to find the minima. However, this is not possible when dealing with many trainable parameters (i.e. High dimensional space). Therefore, the gradient descent is a good strategy to find the minima even with a high dimensional space, where the whole landscape cannot be computered.\n",
    "\n",
    "In math, the gradient is typically done over the space coordinates ($x$, $y$, $z$...). However, the cost function depends on the trainable parameters $\\omega$. In other words, training is to modify $\\omega$ to get better results in prediction/inference time (by reducing the cost function). Thus, the gradient here is a vector, whose components are the partial derivatives respect to each trainable parameter $\\omega_i$.\n",
    "\n",
    "\\begin{equation}  \\tag{2.3}\n",
    "\\vec{\\nabla}_{\\!\\!\\omega} C(\\omega; y) = \\left(\\frac{\\partial C(\\omega; y)}{\\partial \\, \\omega_0} \\;, \\;  \\dots \\;, \\; \\frac{\\partial C(\\omega; y)}{\\partial \\, \\omega_i} \\;, \\; \\dots \\;, \\; \\frac{\\partial C(\\omega; y)}{\\partial \\, \\omega_{N_{\\omega}}} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $C(\\omega; y)$ is the cost function and depends essentially on the different parameters $\\omega_i$; ($i=0\\dots N_{\\omega}$).\n",
    "As said before, take into account that $x$ and $y$ are here just like the environment where to compute the cost, but they do not play any other role in the training. All training focus is over the $\\omega$ trainable parameters, not over $(x,y)$. However, we need some data to compute the cost. Since we are changing the set of data points in every iteration, the cost landscape is slightly changing on every iteration too. This is the *stochastic-ness* of the process.\n",
    "\n",
    "Remember, the goal of the training is to update the trainable parameters somehow. Then, the stochastic gradient descent consist on doing steps according to the partial derivative of the cost function, over the parameter to be updated. However, cost does not depend directly on $\\omega$, but it indirectly does through the inference function $y=g(x;\\omega)$. So, it is worth to apply the chain rule for the derivatives.\n",
    "\n",
    "$$\\omega_i^{\\text{new}} = \\omega_i - \\eta \\, \\delta(\\omega_i) = \\omega_i - \\eta \\, \\frac{\\partial C(\\omega;y)}{\\partial \\omega_i} \\quad; \\quad (i=0,N_{\\omega})$$\n",
    "\\begin{equation}\\tag{2.4}\n",
    " = \\omega_i - \\eta \\, \\frac{\\partial C(\\omega;y)}{\\partial y(x;\\omega)} \\, \\frac{\\partial y(x;\\omega)}{\\partial \\omega_i}\\quad \\quad \\quad \\quad \\quad \\quad \\;\\;\\,\n",
    "\\end{equation}\n",
    "$$\\ = \\omega_i - \\eta \\,  \\left\\langle \\big[ y(x;\\omega_0, \\omega_1) - y_{\\rm truth}(x) \\big] \\, \\frac{\\partial y(x; \\omega)}{\\partial \\omega_i} \\right\\rangle_{\\!\\!_M}$$\n",
    "\n",
    "\n",
    "where $\\eta$ is the learning rate (how fast we want to progress during the training) and $\\delta(\\omega_i)=\\frac{\\partial C(\\omega;y)}{\\partial \\omega_i}$ is the gradient step for each $\\omega_i$ trainable parameter.\n",
    "\n",
    "$\\frac{\\partial C(\\omega;y)}{\\partial y(x;\\omega)}$ is easily derived from Eq. (2.2) and it is just a scalar factor, a number (not a vector). It basically contains the inference of the current model over an input and its comparison with the ground truth. It is just the discrepancy between the prediction and the target in the MSE case.\n",
    "\n",
    "Finally, $\\frac{\\partial y(x;\\omega)}{\\partial \\omega_i}$ is the analytical gradient of $g(\\cdot)$ over the weights, Eq. (2.1). It only depends on the model architecture (relation between inputs, weights and outputs) and it is the core of the backpropagation algorithm. It evaluates how to propagate the discrepancy through the network.\n",
    "\n",
    "In this particular case\n",
    "\\begin{equation}\\tag{2.5}\n",
    "\\vec{\\nabla}_{\\!\\!\\omega} \\, y(x;\\omega) = \\vec{\\nabla}_{\\!\\!\\omega} \\, g(x;\\omega_0, \\omega_1) = \\left(\\dots \\;, \\; \\frac{\\partial g(x; \\omega)}{\\partial \\omega_i} \\;, \\; \\dots \\right) = \\left(\\frac{1}{(x - \\omega_1)^2 + 1} \\;, \\; \\frac{+2(x-\\omega_1)\\omega_0}{\\left[(x-\\omega_1)^2+1\\right]^2}  \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Notice that, since we deal with many input samples $x$, all this should be averaged over all $M$ samples, see the angle brackets $\\langle \\cdot \\rangle_{\\!_M}$ in Eq. (2.4), coming from Eq. (2.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Write a function which calculates the gradient step $\\delta(\\omega) = \\frac{\\partial C(\\omega;y)}{\\partial \\omega_i}$ in this particular case, for the given $\\omega$ trainable parameters, $(x, y_{\\rm truth})$ ground truth data points and $y$ predicted outputs. \n",
    "\n",
    "How many components should this gradient step have?\n",
    "\n",
    "This function might return not only the $\\delta(\\omega)$, but also the MSE-cost in the current state and with the current batch of data. This last is just useful to monitor the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to perform a train step. This is `training_function` (needed in a while):\n",
    "\n",
    "0. The function should take as arguments the current $\\omega$ (a list of the parameters), the learning rate $\\eta$, and the `batchesize`, **in this specific order**.\n",
    "1. Get a random batch of $(x,y_{\\rm truth})$ input-output pairs from the dataset, this will be the ground truth in this supervised training step. (Use the function done in Section 1.1)\n",
    "2. Update the $\\omega$ calling to the gradient step function done just above.\n",
    "3. Return the following variables: the $x$ input batch, the $y_{\\rm truth}$ target output, the $y$ predicted output, the $C$ MSE-cost obtained in this step (MSE-cost typically is computed before updating the omegas), and the new $\\omega$ parameters (**in this specific order**).\n",
    "\n",
    "Follow the instructions 0. And 3. In detail to be able to monitor the training with the fancy function some cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function(omegas, eta, batchsize):\n",
    "\n",
    "    # your code\n",
    "    \n",
    "    return x_subset, y_subset, y_pred, cost, np.array(new_omegas)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a loop to apply some training steps (using the function you have just made) and store in a list the MSE-cost returned in every iteration. Plot that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the prediction of the model over a random batch of inputs and compare it with the ground truth. This is exactly like at the end of Section 1.1.\n",
    "\n",
    "Has the fitting improved? Is it good enough?\n",
    "\n",
    "If not, run again the training loop and plot again the prediction, and compare again it with the ground truth.\n",
    "\n",
    "How many steps are needed to get an acceptable fitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Now, let's see in more detail what are happening during this training loop.\n",
    "\n",
    "To do that, we will see how the $\\omega$ descend the gradient on the cost landscape.\n",
    "\n",
    "First, let's define some functions to fancy plot the progress. (just run the next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(x, true_y, fit_y, legend=True, ax=None):\n",
    "    ax = plt.gca() if ax==None else ax\n",
    "\n",
    "    \n",
    "    ax1 = ax.scatter(x, true_y, color=\"blue\", label='ground truth')\n",
    "    ax2 = ax.scatter(x, fit_y, color=\"orange\", label='fitting')\n",
    "    ax3 = ax.plot([x, x], [true_y, fit_y], 'y:', linewidth=1, \n",
    "                  label='deviation')\n",
    "    \n",
    "    if legend:\n",
    "        ax.legend(handles=[ax1, ax2, ax3[0]], loc='upper left', fontsize=6)\n",
    "    ax.set_xlim(-4,4)\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    omegas_str = ','.join([str(int(o*100)/100) for o in omegas])\n",
    "    ax.set_ylabel(f\"$y=f(x;{omegas_str})$\", fontsize=6)\n",
    "\n",
    "    \n",
    "def plot_landscape(true_y, true_x, ws, ax=None):\n",
    "    ax = plt.gca() if ax==None else ax\n",
    "    \n",
    "    Nw = 40\n",
    "    \n",
    "    # The landscape is on omegas, WHY?\n",
    "    w0s = np.linspace(-3, 6, Nw)  # exploring from -3 to 6 for th0 (40 samples)\n",
    "    w1s = np.linspace(-2, 3, Nw)  # exploring from -2 to 3 for th1 (40 samples)\n",
    "\n",
    "    landscape = np.zeros([Nw, Nw])  # init landscape\n",
    "    for j0, w0 in enumerate(w0s):\n",
    "        for j1, w1 in enumerate(w1s):\n",
    "            pred_y = w0 / ( (true_x-w1)**2 + 1.0 )\n",
    "            landscape[j1, j0] = 0.5 * np.mean((pred_y - true_y)**2)\n",
    "    \n",
    "    im = ax.contourf(landscape, extent=[-3, 6, -2, 3])\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    ax.contour(landscape, extent=[-3, 6, -2, 3], colors=\"white\")\n",
    "\n",
    "    ax.set_xlabel(r'$\\omega_0$')\n",
    "    ax.set_ylabel(r'$\\omega_1$')\n",
    "\n",
    "    \n",
    "def plot_info(x, true_y, pred_y, omegas, stepping, history, arrows=True):\n",
    "    \"\"\" This is a funtion to plot the progress on the training.\n",
    "    \"\"\"\n",
    "    max_MSE = history.max()*1.1 if history.size>0 else 1  # skipping empty hist.\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    fig = plt.figure(figsize=(5,3))\n",
    "    gs = fig.add_gridspec(2, 2, hspace=.35, wspace=.3)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax3 = fig.add_subplot(gs[1, :])\n",
    "    \n",
    "    plot_curves(x, true_y, pred_y, legend=True, ax=ax1)  # compare fiting with truth\n",
    "    plot_landscape(true_y, x, omegas, ax2)  # to see the progress\n",
    "\n",
    "    # plot where we are on the landscape\n",
    "    ax2.scatter([omegas[0]], [omegas[1]], color=\"orange\")  \n",
    "    if arrows:  # where we go\n",
    "        ax2.arrow(*(omegas),*(-stepping), color='yellow',\n",
    "                  width=0.05, length_includes_head=True)\n",
    "    \n",
    "    # let's plot the cost history\n",
    "    ax3.plot(range(history.size), history, marker='o')\n",
    "    ax3.set_xlim(0, history.size-1)\n",
    "    ax3.set_xticks(range(0, history.size, 3))\n",
    "    ax3.set_ylim(0.0, max_MSE)\n",
    "    ax3.set_xlabel(r\"$step$\")\n",
    "    ax3.set_ylabel(r\"$MSE$\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def animated_train(your_func, eta, batchsize, init_omegas=None, nsteps=20):\n",
    "    if init_omegas is None:\n",
    "        global omegas  # let's continue where we was\n",
    "    else:\n",
    "        omegas = init_omegas\n",
    "\n",
    "    cost_history = np.zeros(nsteps)\n",
    "    delta_w = 0\n",
    "    for n in range(nsteps):  # it can be replaced with a while with some criteria\n",
    "\n",
    "        old_omegas = omegas.copy()\n",
    "        x, y_true, y_pred, cost, omegas = your_func(omegas, eta, batchsize)  # <-----------   CHECK THIS: Should NOT be numbers!!\n",
    "        delta_w = old_omegas - omegas\n",
    "        \n",
    "        # let's plot where we are and where we go\n",
    "        plot_info(x, y_true, y_pred, old_omegas, delta_w, cost_history, True)\n",
    "        print('estimated omega:', omegas)\n",
    "        sleep(0.5)\n",
    "        \n",
    "        cost_history[n] = cost\n",
    "        \n",
    "        plot_info(x, y_true, y_pred, omegas, delta_w, cost_history, False)\n",
    "        print('estimated omega:', omegas)\n",
    "        sleep(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function below where the first argument is the training function you defined in a couple of cells above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# your_func: The trining funtion defined by your own. It has to follow the given instructions for the arguments and returning variables.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# eta: \"learning rate\" (gradient descent step size)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# batchsize: stochastic x samples used per step (just a few to be fast)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# itit_omegas: the starting training process\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43manimated_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43myour_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_omegas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 83\u001b[0m, in \u001b[0;36manimated_train\u001b[1;34m(your_func, eta, batchsize, init_omegas, nsteps)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nsteps):  \u001b[38;5;66;03m# it can be replaced with a while with some criteria\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     old_omegas \u001b[38;5;241m=\u001b[39m omegas\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 83\u001b[0m     x, y_true, y_pred, cost, omegas \u001b[38;5;241m=\u001b[39m \u001b[43myour_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43momegas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# <-----------   CHECK THIS: Should NOT be numbers!!\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     delta_w \u001b[38;5;241m=\u001b[39m old_omegas \u001b[38;5;241m-\u001b[39m omegas\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# let's plot where we are and where we go\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m, in \u001b[0;36mtraining_function\u001b[1;34m(omegas, eta, batchsize)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_function\u001b[39m(omegas, eta, batchsize):\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# your code\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx_subset\u001b[49m, y_subset, y_pred, cost, np\u001b[38;5;241m.\u001b[39marray(new_omegas)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_subset' is not defined"
     ]
    }
   ],
   "source": [
    "# your_func: The trining funtion defined by your own. It has to follow the given instructions for the arguments and returning variables.\n",
    "# eta: \"learning rate\" (gradient descent step size)\n",
    "# batchsize: stochastic x samples used per step (just a few to be fast)\n",
    "# itit_omegas: the starting training process\n",
    "\n",
    "animated_train(your_func=training_function, eta=1, batchsize=300, init_omegas=[-2.3, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the parameters `eta` and `batchesize`.\n",
    "\n",
    "How do they influence learning progress?\n",
    "\n",
    "Why we are not going directly to the minimum of the landscape?\n",
    "\n",
    "Why is this algorithm called ***Stochastic*** *Gradient Descent*? Check the meaning and reason of a <u>[stochastic](https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/)</u> approach.\n",
    "\n",
    "Notice that, while the $\\omega$ parameters are learned from data, quantities such as the learning rate and batch size must be choosen manually. All non trainable parameters that defines the learning process or even the model architecture are called **hyperparameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Non constant learning rate (Schedule or Adaptive)\n",
    "\n",
    "**Optional**: Modify `your_training_function` in such a way to run with an [Adaptive Learning Rate Method](https://www.geeksforgeeks.org/deep-learning/adam-optimizer/): try only the Momentum approach and just read the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backpropagation\n",
    "\n",
    "Like in the Stochastic Gradient Descent, we update trainable parameters of a neural network (weights and biases) in the following way\n",
    "\n",
    "\\begin{equation} \\tag{2.6}\n",
    "\\omega_{ij}^l = \\omega_{ij}^l - \\eta \\, \\delta(\\omega_{ij}^l)\n",
    "\\end{equation}\n",
    "\\begin{equation} \\tag{2.7}\n",
    "b_{i}^l = b_{i}^l - \\eta \\, \\delta (b_{i}^l)\n",
    "\\end{equation}\n",
    "\n",
    "where $l=1\\dots L$ labels all the net layers, $i=1\\dots N_l$ and $j=1\\dots N_{l-1}$ label all the $N_l$ neurons of the $l$-th layer, and $\\delta (\\omega) = \\frac{\\partial C(\\omega;x)}{\\partial \\omega}$ is the gradient step to update the parameters ($\\omega$ with no indices includes, not only all weights, but also all biases, i.e. All trainable parameters).\n",
    "\n",
    "In neural networks, the cost function $C(\\omega;y)$ can have a very deep dependency on weights and biases, especially those corresponding to the first layers. However, this deep dependency can be solved through the derivative chain rule (like before). Moreover, the calculations done to estimate the gradient step for the last layers can be reused for deeper layers, as we recall below.\n",
    "\n",
    "* **Output layer** ($l=L$):\n",
    "\n",
    "The gradient step $\\delta$ to update weights $\\omega^{L}_{ij}$ and biases $b^{L}_{i}$ of the last layer ($l=L$), according to Eq. (2.4), is\n",
    "\n",
    "\\begin{equation} \\tag{2.8}\n",
    "\\delta(\\omega^L) = \\frac{\\partial C(\\omega;y) }{\\partial \\omega^L }\n",
    "                 = \\frac{\\partial C(\\omega;y) }{\\partial y^{L}_{i} }\\frac{\\partial y^{L}_{i} }{\\partial \\omega^{L} } \n",
    "\\end{equation}\n",
    "                        \n",
    "where $\\omega^L$ stands for any trainable parameter on the last layer; both, $\\omega^{L}_{ij}$ and $b^{L}_{i}$. Notice that the first factor depends only on the cost function, whereas the second only does on the architecture of the network.\n",
    "\n",
    "On the first factor: since we use the simple MSE-cost function $C = \\frac{1}{2} \\left(y_i^L - \\hat{y}_i \\right)^2$, where $\\hat{y}_i=y^{\\rm truth}_i$ is the ground truth result for the $i$-th neuron, and $y_i^L=y_i^L(x;\\omega)$ is the actual neuron value gotten with the current network state, the first factor becomes\n",
    "\n",
    "\\begin{equation} \\tag{2.10}\n",
    "\\frac{\\partial C }{\\partial y^{L}_{i} } = \\left(y_i^L - \\hat{y}_i \\right)\n",
    "\\end{equation}\n",
    "\n",
    "what is just the discrepancy between the predicted result and the ground truth.\n",
    "\n",
    "On the second factor: we must have on mind the network architecture. This is how layers are connected between them. For instance, the current output value can be expressed as a combination of the previous layer as\n",
    "\\begin{equation} \\tag{2.9}\n",
    "y_i^L = f_L\\!\\!\\left(x_j^{L}\\right) =  f_L\\!\\!\\left(\\omega_{ij}^{L}y_j^{L-1} + b_i^L\\right)\n",
    "\\end{equation}\n",
    "where $f_l[\\cdot]$ is the activation function for the $l$-th layer, then\n",
    "\n",
    "\\begin{equation} \\tag{2.11}\n",
    "\\frac{\\partial y^{L}_{i} }{\\partial \\omega^{L}_{ij} } = f'_L\\!\\!\\left(x_i^L\\right) \\, y_j^{L-1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation} \\tag{1.12}\n",
    "\\frac{\\partial y^{L}_{i} }{\\partial b^{L}_{i} } = f'_L\\!\\!\\left(x_i^L\\right)\n",
    "\\end{equation}\n",
    "\n",
    "These last two expressions only contain $y_j^{L-1}$, the current values of the neurons in the previous layer, and $f'_L\\!\\!\\left(x_i^L\\right)$, the value obtained which corresponds to the derivative of the activation function evaluated at that point.\n",
    "\n",
    "For convenience, let's define\n",
    "\\begin{equation} \\tag{2.13}\n",
    "\\boxed{ \\Delta^{\\!L}_i = \\left(y_i^L - \\hat{y}_i \\right) \\, f'_L\\!\\!\\left(x_i^L\\right) }\n",
    "\\end{equation}\n",
    "\n",
    "Finally,\n",
    "\n",
    "\\begin{equation} \\tag{2.14}\n",
    "\\boxed{ \\delta(\\omega_{ij}^L) = \\Delta_i^{\\!L} \\otimes y_j^{L-1} }\n",
    "\\end{equation}\n",
    "\\begin{equation} \\tag{2.15}\n",
    "\\boxed{ \\delta(b_{i}^L) = \\Delta_i^{\\!L} }\n",
    "\\end{equation}\n",
    "\n",
    "These boxed equations are all you need to be able to update the weight and biases of the last layer.\n",
    "\n",
    "Of course, all calculus made above are computed with matrices and vectors of indices $i$ and $j$. Notice that $\\otimes$ stands for the outer product, which generates a matrix from two vectors. Moreover, usually it is processed with batches of input-output pairs, where final values are averaged for those batches. An efficient way to implement this in batches is explained at the end of this cell.\n",
    "\n",
    "* **Last hidden layer**:\n",
    "\n",
    "The procedure is exactly the same as before. However, in order to update weights $\\omega^{L-1}_{jk}$ and biases $b^{L-1}_{j}$ of the last hidden layer ($l=L-1$), Eq. (2.8) becomes\n",
    "\n",
    "\\begin{equation} \\tag{2.16}\n",
    "\\delta(\\omega^{L-1}) = \\frac{\\partial {C} }{\\partial \\omega^{L-1} }\n",
    "            = \\frac{\\partial {C} }{\\partial y^{L}_{i} }\n",
    "              \\frac{\\partial y^{L}_{i} }{\\partial y^{L-1}_{j} }\n",
    "              \\frac{\\partial y^{L-1}_{j} }{\\partial \\omega^{L-1} } \n",
    "\\end{equation}\n",
    "Therefore,\n",
    "\\begin{equation} \\tag{2.17}\n",
    "{ \\delta(\\omega_{jk}^{L-1}) = \\Delta_i^{\\!L} \\; \\omega_{ij}^L \\, f'_{L-1}\\!\\!\\left(x_j^{L-1}\\right) \\otimes y_k^{L-2} = \\Delta_j^{\\!L-1} \\otimes y_k^{L-2} }\n",
    "\\end{equation}\n",
    "\\begin{equation} \\tag{2.18}\n",
    "{ \\delta(b_{j}^{L-1}) = \\Delta_i^{\\!L} \\; \\omega_{ij}^L \\, f'_{L-1}\\!\\!\\left(x_j^{L-1}\\right)  = \\Delta_j^{\\!L-1}}\n",
    "\\end{equation}\n",
    "\n",
    "* **General formula**:\n",
    "\n",
    "Notice that Eqs. (2.17, 2.18) can be generalized for any layer $l=L-m$ as\n",
    "\\begin{equation} \\tag{2.19}\n",
    "\\boxed{ \\delta(\\omega_{pq}^{l}) = \\Delta_p^{\\!l} \\otimes y_q^{l-1} }\n",
    "\\end{equation}\n",
    "\\begin{equation} \\tag{2.20}\n",
    "\\boxed{ \\delta(b_{p}^{l}) = \\Delta_p^{\\!l} }\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation} \\tag{2.21}\n",
    "\\boxed{ \\Delta_{p}^{l} = \\Delta_{r}^{\\!l+1}  \\cdot \\omega_{rp}^{l+1} \\, f'_{l}\\!\\!\\left(x_p^{l}\\right) } \\quad; \\quad (l=L-1\\dots 1)\n",
    "\\end{equation}\n",
    "\n",
    "Notice that Eq. (2.21) shows how the $\\Delta$ error made in the prediction is back-propagated according to weights and activations of every layer.\n",
    "\n",
    "Notice also that $y_q^{l-1}$ in Eq. (2.19) is the input data when $l=0$, the last step backing the propagation.\n",
    "\n",
    "* **General notes**:\n",
    "\n",
    "Notice that boxed equations contain all the backpropagation process. For Eq. (2.13), we need the prediction $y_i^L$ and the derivative of the last step $f'_L\\!\\!\\left(x_i^L\\right)$. Moreover, we also need all the intermediate values $y_p^{l}$ and their derivatives $f'_l\\!\\!\\left(x_p^l\\right)$ for Eqs. (2.19, 2.20). Therefore, the forward prediction (inference) is needed in every training step, not only to get the cost value and its derivative (Eq. 2.13), but also in such a way to store all intermediate neurons values $y_p^{l}$ and their derivative $f'_l\\!\\!\\left(x_p^l\\right)$. Then, all the effort is to make forward inferences storing the intermediate states and their derivatives and, then, apply the backprogation depicted in Eq. (2.21).\n",
    "\n",
    "That's it!\n",
    "\n",
    "\n",
    "* **Implementation in batches**:\n",
    "\n",
    "As we said before, we must be able to accept input-output pairs in batches in order to increase the performances (in time and in precision, recall stochasticness/batchesize relation). Then, all variables regarding data should earn an extra dimension (weights and the biases, and their gradient steps remain like in equations above, no extra dimension is needed for them). In practice, we need just to be carefully with the shape and order of the products. This is, since we want to average all values over the batches, then to compute Eqs. (2.14, 2.19) we have to do a dot product (sum-product) with the batch index as inner index and divide by the batchesize (in this way, we incorporate the average over the whole batch). Similarly, for Eqs. (2.15, 2.20) we have to sum over the $\\Delta$ and divide by the batchesize. In addition, the inner index of $\\Delta \\cdot \\omega$ (in Eq. 2.21) is the repeated index $r$, resulting to an array with a shape of `batchSize`$\\times$`layerSize` (or transposed). Finally, this is multiplied element-wise for the activation contribution and averaged over the batch, like in Eq. (2.13).\n",
    "\n",
    "> Recall the [`numpy.matrix.transpose()`](https://numpy.org/doc/stable/reference/generated/numpy.matrix.transpose.html) method (**`my_array.T`**) to swap the indices of a 2D-array in order to match the shapes on `np.dot()` multiplications. Avoid reshape here, it is not the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implement of backpropagation for a general (fully connected) network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it step by step.\n",
    "\n",
    "Firstly, write a function to apply the activation function. It has to be ready for training, it means that it has to return both, the result of applying the activation function, and its analytical derivative. Defining this function will let us to easily replace it with another activation function.\n",
    "\n",
    "Use the *Sigmoid* as activation function, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write a function to do a layer step, also ready for training. This is, given an input vector $x$, a weights matrix $\\omega$, and a biases vector $b$, compute the matrix operation and, then, apply the activation function and its derivative using the function made before. (This function will be also used in Section 2.3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write now a function to make a forward pass through the whole network for a given input vector $x$, a list of weights $[w]_l$ (one weights matrix for each layer) and a list of biases $[b]_l$ (idem), for $l=1\\dots L$, being $L$ the number of layers in the network. Thus, use iteratively the previous function, over a given list of weights and biases.\n",
    "\n",
    "This is very similar to what you did in Tutorial 1 (inference). However, keep in mind, we want it for training. Therefore, we need, not only the final result (like in prediction), but also all inner states of the neurons $y_q^{\\,l}$ and their derivative $y\\prime_q^{\\,l}=f'_{l}\\!\\!\\left(x_p^{l}\\right)$. So, store and return a couple of list of neuron states $[y_q]_l$, one for the common state and the other for its derivative $[y_q^{\\,\\prime}]_l$, for $l=1\\dots L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to back propagate the error prediction, evaluated by the cost function. Thus, you should take the ground truth $\\{x, y_i^{\\rm truth}\\}$ and the current parameters, i.e. The weights list $[\\omega]_l$ and biases list $[b]_l$ in the arguments.\n",
    "\n",
    "This function has to return the gradient steps $\\delta(\\omega)$ and $\\delta(b)$.\n",
    "\n",
    "Note that $\\delta(\\omega)$ and $\\delta(b)$ have to be of same shape than $\\omega$ and $b$, respectively. Then, they are list of matrices ($\\omega$) and list of vectors ($b$). Thus, you can initialize them using the `np.zeros_like()` numpy's function inside <u>[list comprehension](https://www.geeksforgeeks.org/python-list-comprehension/)</u>.\n",
    "\n",
    "Then, you can fill their last item following Eqs. (2.14, 2.15), using Eq. (2.13).\n",
    "\n",
    "Finally, you can fill them recursively backwards following Eqs. (2.19, 2.20), using Eq. (2.21).\n",
    "\n",
    "Note that you will also need all inner states of the neurons $[y_q]_l$ and their derivative $[y^{\\,\\prime}_q]_l$, so ask for them in the arguments.\n",
    "\n",
    "Take into account that all data variables has an extra dimension for batch processing and, then, you have to average over that dimension to get a stochastic result. This is explained in detail above (the \"Implementation in batches\" point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it together.\n",
    "\n",
    "Write a function to perform a training step. This is to update the trainable parameters stepping down the gradient.\n",
    "\n",
    "To do that, \n",
    "\n",
    "1. Apply forward pass (inference) through the whole network, while storing the inner states (and their derivative), just by calling the function defined before.\n",
    "\n",
    "1. Compute the MSE-cost of the network using the prediction and the ground truth (this is jut to monitor the trainning). \n",
    "\n",
    "1. Take the inner states and apply the backpropagation using the function made before, to get the gradient steps $\\delta(\\omega)$ and $\\delta(b)$.\n",
    "\n",
    "1. Update the weights and biases following the Eqs. (2.6, 2.7).\n",
    "\n",
    "We will use this function to completely train a network, then it is important to check that:\n",
    "\n",
    "- it has as argument: the input data $x$, the ground truth $y^{\\rm truth}$, the current weights list $[\\omega]_l$, the current biases list $[b]_l$, and the learning rate $\\eta$.\n",
    "\n",
    "- it returns: the predicted result $y^L$, the cost $C$, the updated weights list $[\\omega^{\\rm new}]_l$, and the updated biases list $[b^{\\rm new}]_l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train a net to fit a 2D function (an arbitrary image) in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next nonlinear function as a ground truth, so as the target behavior of the network.\n",
    "Yes, it is an arbitrary image. However, it can be seen as a nontrivial relation between a couple of inputs $(x_1,x_2)$ (corresponding to every position on the image), and the single output $y_{\\rm truth}$ (corresponding to the pixel value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_dataset=array([[0.9848748 ],\n",
      "       [0.96285182],\n",
      "       [0.98166966],\n",
      "       ...,\n",
      "       [1.        ],\n",
      "       [0.96977539],\n",
      "       [0.96660571]]): y_dataset.shape=(1024, 1)\n",
      "\n",
      "x_dataset=array([[-0.5    , -0.5    ],\n",
      "       [-0.5    , -0.46875],\n",
      "       [-0.5    , -0.4375 ],\n",
      "       ...,\n",
      "       [ 0.46875,  0.40625],\n",
      "       [ 0.46875,  0.4375 ],\n",
      "       [ 0.46875,  0.46875]]): x_dataset.shape=(1024, 2)\n"
     ]
    }
   ],
   "source": [
    "# load the pixel image\n",
    "URL = \"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/data/Smiley.png\"\n",
    "face = imageio.v2.imread(requests.get(URL).content)\n",
    "\n",
    "face = face[2::6,2::6,0]\n",
    "face = face[:,::-1].astype('float64')  # and flip... to get the right view!\n",
    "\n",
    "face += np.random.uniform(low=0, high=0.05*face.max(), size=face.shape)\n",
    "\n",
    "# normalize between 0 and 1\n",
    "face -= face.min()\n",
    "face_size = np.shape(face)[0] # assuming a square image!\n",
    "y_dataset = (face.astype(dtype='float'))/face.max()  # [0, 1]  <--- y_target is the goal of the training\n",
    "y_dataset = y_dataset.flatten()[:,None]\n",
    "print(f\"{y_dataset=}: {y_dataset.shape=}\")\n",
    "\n",
    "print(\"\")\n",
    "# single_range = np.arange(-face_size, face_size)/face_size\n",
    "x1_2D, x2_2D = np.mgrid[-face_size//2:face_size//2, -face_size//2:face_size//2] / face_size\n",
    "x_dataset = np.stack([x1_2D.flatten(), x2_2D.flatten()], axis=1)     #        <--- x is the input values\n",
    "print(f\"{x_dataset=}: {x_dataset.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the trainning dataset is: `x_dataset` and `y_dataset`, corresponding to all available inputs and their correspondig outputs. \n",
    "\n",
    "The next cell defines a function to get a reduced batch of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(all_x, all_y, batchsize=40):\n",
    "    all_samples = all_y.shape[0]\n",
    "    idxs = np.random.randint(0, all_samples-1, batchsize)\n",
    "\n",
    "    return all_x[idxs, :], all_y[idxs, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below shows a picture according to certain $\\vec{x}_M=(x_1, x_2)_M$ input batch and $y_M$ output batch. It is shown in the `axs` axis argument if it is passed, else it is shown in the active axis (current or new figure). Non set pixels are shown in white (out of the colormap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmaluenda\\AppData\\Local\\Temp\\ipykernel_26828\\2037755632.py:5: RuntimeWarning: divide by zero encountered in divide\n",
      "  image = 1/np.zeros_like(face)  # let's set unexplored points at NaN -> White in imshow (viridis)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow_inference(inputs, outputs, ax=None):\n",
    "\n",
    "    ax = ax or plt.gca()\n",
    "\n",
    "    image = 1/np.zeros_like(face)  # let's set unexplored points at NaN -> White in imshow (viridis)\n",
    "\n",
    "    idx = ((inputs+0.5)*face_size).astype(dtype='int')\n",
    "\n",
    "    image[idx[:,0], idx[:,1]] = outputs[:,0]\n",
    "   \n",
    "    im = ax.imshow(image, extent=(-0.5, 0.5, -0.5, 0.5))\n",
    "    ax.set_title(f\"vmin={outputs.min():.2f}\\nvmax={outputs.max():.2f}\", fontsize=6, y=1, pad=-14)\n",
    "    ax.set_xlabel(\"$x_1$\", fontsize=4)\n",
    "    ax.set_ylabel(\"$x_2$\", fontsize=4)\n",
    "    ax.tick_params(axis='both', labelsize=4)\n",
    "\n",
    "    \n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(2,2))\n",
    "imshow_inference(x_dataset, y_dataset, ax=axs[0])  # This shows the whole image in the first axis\n",
    "\n",
    "\n",
    "imshow_inference(*get_batch(x_dataset, y_dataset, 256), ax=axs[1])  # here a batch of 256 pixels is shown on the axs[1] (right side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As many samples to be computed at once, as better performance in the training (and less stochastic). However, it needs more computational resources (basically, RAM memory and CPU time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Network definition and evaluation\n",
    "\n",
    "Define a network having 4 layers with 2 input neurons, 50 neurons in the two hidden layers, and 1 single output neuron.\n",
    "\n",
    "Initialize the weights with random numbers within a Gaussian distribution and biases with zeros.\n",
    "\n",
    "> Check the `np.random.randn()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to make a simple prediction. This is just like in the previous tutorial or the one written above, but without any storing of the inner values. Just to return the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training step function (backpropagation) several times to train the network and recover the target image (smiling).\n",
    "\n",
    "Make a loop to apply many times (100k or, even, 1 million) the training step. _-you can stop the running by clicking the STOP button on the bar (ctrl+C in some environs)-_\n",
    "\n",
    "Before the loop:\n",
    "\n",
    " * Set a $\\eta$ learning rate (about 1).\n",
    " * Set a $M$ batch size (about 256).\n",
    " * Initialize an empty list to store the cost history.\n",
    "\n",
    "Inside the loop:\n",
    " * Create a batch of data using the `get_batch()` function written in Section 2.2.\n",
    " * Make a training step using this batch of data, using the last function written in Section 2.1.\n",
    " * Append the returned cost value to the cost list.\n",
    " * Only after some iterations (let's say 10000), do:\n",
    "   * Clear any previous figure by typing `clear_output(wait=True)`\n",
    "   * Create a $2\\times2$ subplot. You may set a `figsize=(3,3)` to contain the size of the figure.\n",
    "   * Call the `imshow_prediction` function to show the last gotten result (over the current batch) on the axis `[0,0]`.\n",
    "   * Apply the inference function just previously written to the whole `x_dataset` to validate the current state of the network, the result is the $y_{\\rm val}$ validation output.\n",
    "   * Call the `imshow_prediction` function using the whole `x_dataset` and $y_{\\rm val}$, on the axis `[0,1]`.\n",
    "   * Compute the cost $\\frac{1}{2}(y_{\\rm val}-y_{truth})^2$. Do not average this cost, i.e. It is a flattened image.\n",
    "   * Call the `imshow_prediction` function using the whole `x_dataset` and the cost just computered in the last point, on the axis `[1,0]`.\n",
    "   * Plot the cost history on the axis `[1,1]`.\n",
    "   * Add `plt.show()` after configuring the figure to show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with different configurations: learning rate, batch size, number of neurons, number of layers... Even, the activation function.\n",
    "\n",
    "To modify the activation function written in Section 2.1, just re-define it in the cell below, before reinitialize the training again.\n",
    "\n",
    "Check  https://medium.com/towards-data-science/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79\n",
    "\n",
    "Comment the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save and load a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are the information about the model?\n",
    "\n",
    "Save the information using the `np.savez()` function. Use `*weights_list` and `*biases_list`. See the asterisk in front of the list variables to be able to store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can recover a whole neural network model and state.\n",
    "\n",
    "Load that information again and reproduce the image without making any new training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inside a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set to 0 all the weights on all neurons of a certain hidden layer, except for one neuron and, then, let's plot the result to check the image produced by that neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, a model from internet is download to show this. You can load your own neural network model (weights and biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m URL_smile_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/dmaluenda/hands_on_machine_learning/raw/master/data/02_smile_model_2layers_5neurons_sigmoid.npz\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m stored_model \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mload(get_npz_remote(URL_smile_model))  \u001b[38;5;66;03m# You can replace 'get_npz_remote(...)' with your path set in last section.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "URL_smile_model = 'https://github.com/dmaluenda/hands_on_machine_learning/raw/master/data/02_smile_model_2layers_5neurons_sigmoid.npz'\n",
    "\n",
    "stored_model = np.load(get_npz_remote(URL_smile_model))  # You can replace 'get_npz_remote(...)' with your path set in last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_stored = [stored_model[\"arr_\"+str(i)] for i in range(len(stored_model)//2)]\n",
    "b_stored = [stored_model[\"arr_\"+str(len(stored_model)//2+i)] for i in range(len(stored_model)//2)]\n",
    "\n",
    "layers = len(w_stored)-1\n",
    "neurons = w_stored[1].shape[1]\n",
    "\n",
    "fig, axs = plt.subplots(neurons, layers, figsize=(layers, neurons))\n",
    "\n",
    "for l in range(layers):\n",
    "    for n in range(neurons):\n",
    "        w_ON = w_stored[l+1][n,:].copy()\n",
    "        \n",
    "        w_current = np.zeros_like(w_stored[l+1])\n",
    "        w_current[n,:] = w_ON\n",
    "\n",
    "        w_list = w_stored.copy()\n",
    "        w_list[l+1] = w_current\n",
    "        \n",
    "        y_predi_all = inference(x_dataset, w_list, b_stored)  # edit this according to your inference function done in Section 2.3\n",
    "        imshow_inference(x_dataset, y_predi_all, axs[n, l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Play with https://playground.tensorflow.org "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "223.852px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
