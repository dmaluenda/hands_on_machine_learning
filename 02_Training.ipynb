{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><sub>This notebook is distributed under the <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\" target=\"_blank\">Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license</a>.</sub></div>\n",
    "<h1>Hands on Machine Learning  <span style=\"font-size:10px;\"><i>by <a href=\"https://webgrec.ub.edu/webpages/000004/ang/dmaluenda.ub.edu.html\" target=\"_blank\">David Maluenda</a></i></span></h1>\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://atenea.upc.edu/course/view.php?id=95161\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/upc_logo_49px.png\" width=\"130\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "  </td>\n",
    "  <td>   <!-- gColab -->\n",
    "    <a href=\"https://colab.research.google.com/github/dmaluenda/hands_on_machine_learning/blob/master/02_Training.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/colab_logo_32px.png\" />\n",
    "      Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- github -->\n",
    "    <a href=\"https://github.com/dmaluenda/hands_on_machine_learning/blob/master/02_Training.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/github_logo_32px.png\" />\n",
    "      View source on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- download -->\n",
    "    <a href=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/02_Training.ipynb\"  target=\"_blank\"\n",
    "          download=\"02_Training\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/download_logo_32px.png\" />\n",
    "      Download notebook\n",
    "      </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{II}$. Training Neural Networks with Pure Python\n",
    "\n",
    "Hands on \"Machine Learning on Classical and Quantum data\" course of\n",
    "[Master in Photonics - PHOTONICS BCN](https://photonics.masters.upc.edu/en/general-information)\n",
    "[[UPC](https://photonics.masters.upc.edu/en) +\n",
    "[UB](https://www.ub.edu/web/ub/en/estudis/oferta_formativa/master_universitari/fitxa/P/M0D0H/index.html?) +\n",
    "[UAB](https://www.uab.cat/web/estudiar/la-oferta-de-masteres-oficiales/informacion-general-1096480309770.html?param1=1096482863713) +\n",
    "[ICFO](https://www.icfo.eu/lang/studies/master-studies)].\n",
    "\n",
    "**Tutorial 2**\n",
    "\n",
    "This notebook shows how to:\n",
    "- manage ground truth dataset for supervised learning\n",
    "- understand the cost/loss function concept\n",
    "- implement a stochastic gradient descent to fit arbitrary nonlinear functions\n",
    "- implement backpropagation in pure python\n",
    "- train a deep fully connected net to reproduce an image\n",
    "- differentiate 'hyperparameters' from 'parameters'\n",
    "- choose a learning rate (and its meaning)\n",
    "- choose a batch size (and its meaning)\n",
    "- differentiate training, validation and test datasets\n",
    "- initialize a neural network\n",
    "- get the 'epochs' concept\n",
    "- save and load networks\n",
    "- view inside a neural network\n",
    "\n",
    "**References**:\n",
    "\n",
    "[1] [Machine Learning for Physicists](https://machine-learning-for-physicists.org/) by Florian Marquardt.\n",
    "<br>\n",
    "[2] [NumPy](https://numpy.org/doc/stable/user/whatisnumpy.html): the fundamental package for scientific computing in Python.\n",
    "<br>\n",
    "[3] [Matplotlib](https://matplotlib.org/stable/tutorials/introductory/usage.html): a comprehensive library for creating static, animated, and interactive visualizations in Python.<br>\n",
    "[4] \"Back-Propagation is very simple. Who made it Complicated?\", Prakash Jay at [medium](https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c).\n",
    "<br>\n",
    "[5] [\"A Step by Step Backpropagation Example\"](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/), Matt Mazur.\n",
    "<br>\n",
    "[6] [Backpropagation Step by Step](https://hmkcode.com/ai/backpropagation-step-by-step) from hmkcode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlT_ZrS-R1rG"
   },
   "source": [
    "## 0. Imports: only numpy and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1626874792493,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "zRmzXX4wR1rG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get the \"numpy\" library for linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# get \"matplotlib\" for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 300  # highres display\n",
    "from matplotlib.axes._axes import _log as mpl_ax_logger\n",
    "mpl_ax_logger.setLevel('ERROR')  # ignore warnings\n",
    "\n",
    "# for nice inset colorbars:\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "import requests  # to download files (for data/images)\n",
    "from io import BytesIO\n",
    "import imageio   # to deal with images\n",
    "\n",
    "# time control to count it and manage it\n",
    "from time import time, sleep\n",
    "\n",
    "# For simple animation\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def get_npz_remote(url):\n",
    "    \"\"\" This is a wrapper to get a npz from a url \"\"\"\n",
    "    r = requests.get(url, stream=True)\n",
    "    return BytesIO(r.raw.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Stochastic Gradient Descent\n",
    "### 1.1 Function definition and dataset\n",
    "\n",
    "Since neural networks usually have several input and output neurons, and typically many trainable parameters $\\omega$, it is not easy to visualize the training mechanism. Then, let's play with a toy model where we will have just one single input $x$, one single output $y$, and two trainable parameters $\\omega=(\\omega_0, \\omega_1)$.\n",
    "\n",
    "Let's define a non-linear function to connect $y$ from $x$, through $\\omega$, for instance\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{2.1}\n",
    "g(x; \\omega) = \\frac{\\omega_0}{(x - \\omega_1)^2 + 1} = y\n",
    "\\end{equation}\n",
    "\n",
    "Notice that this function $g(\\cdot)$ do NOT represent any kind of neuron connection within a neural network, neither any activation function. It is just an example of nonlinear function chosen only for illustrative purposes. Although, a neural network can be understood as a non lineal function that connects $\\vec{x}$ input to $\\vec{y}$ output neurons through some $w$ trainable parameters.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Then, define a function to return the $y$ predicted result for a given $x$ and $\\omega$ (input arguments). This would act as the network prediction inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize our pseudo-network by setting arbitrary parameters for $\\omega$.\n",
    "\n",
    "Thus, declare a variable to store $\\omega$ and initialize it to $\\omega=(-2, 2)$, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset with one million of $(x, y)$ pairs, and we want to train our pseudo-network (model) to reproduce this dataset.\n",
    "\n",
    "To train the model (or fit the function), we will follow the supervised learning strategy. Typically, the dataset used in supervised learning is called ***ground truth***, because it is the truth where we fix our target.\n",
    "\n",
    "Load the `curve_fitting_data.npz` numpy file to get the ground truth.\n",
    "\n",
    "> Check the [`numpy.load()`](https://numpy.org/doc/stable/reference/generated/numpy.load.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_function = 'https://github.com/dmaluenda/hands_on_machine_learning/raw/master/data/curve_fitting_data.npz'\n",
    "data = np.load(get_npz_remote(URL_function))\n",
    "print(data)\n",
    "\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "plt.figure(figsize=(3,1))\n",
    "plt.plot(x[:100], y[:100], '.')  # we plot just the first 100 items, to save time and better visualize\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this dataset is huge, write a function to return two batches arrays (for $x$ and $y$) of a certain length given by the $M$ `batchsize` argument.\n",
    "\n",
    "So, randomly pick a $M$ number of elements on the whole $x$ input array and their corresponding $y$ output pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your function by plotting a new batch of $M=40$ data points and compare the shape with the one above. They have to be very similar.\n",
    "\n",
    "Let's compare the randomly initialized model with the ground truth dataset, using just some randomly picked points.\n",
    "\n",
    "Then, plot the network prediction for that same input, in the same figure.\n",
    "\n",
    "> Check the `plt.scatter()` matplotlib's function.\n",
    "\n",
    "Add a legend to easily identify which data is from the dataset and which is the prediction.\n",
    "\n",
    "Set the $x$-axis limits to $[-4, 4]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cost function (also known as loss function)\n",
    "\n",
    "The fitting shown above is probably poor. However, we need to quantify the 'quality' of the current fitting by a numeric value. In other words, a cost function is a function to evaluate the quality of the fitting for that given trainable parameters.  We will use the most simple cost function: the square of the discrepancy between the predicted values with the ground truth, averaged over all the evaluated samples. Notices that is a single value, known in this case as mean squared error (MSE).\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{2.2}\n",
    "C(\\omega; y) = \\frac{1}{2} \\left\\langle \\left[ y\\,(x; \\omega) - y_{\\rm truth}(x)\\right]^2 \\right\\rangle_{\\!_M}\n",
    "\\end{equation}\n",
    "\n",
    "where $y(x; \\omega) = g(x; \\omega_0, \\omega_1)$ is the prediction inferred by the current state of the model (given for that specific $\\omega$ parameters), and over a certain $x$ input set. $y_{\\rm truth}(x)$ is the output pair of every $x$ input, taken from the ground truth dataset. $\\left\\langle \\cdot \\right\\rangle_{\\!_M}$ denotes the average over all $M$ evaluated samples.\n",
    "\n",
    "> the $1/2$ factor is just to make things easier when deriving that cost function, as we will see in a while.\n",
    "\n",
    "Conceptually, the cost function should not depend on any specific $x$ input data, nor on $y$ output, since it evaluates the fitting in whole terms. However, the cost function $C$ indirectly depends on the dataset $(x, y)$, see Eq. (2.2). Even thought, the main dependency is on the $\\omega$ free parameters, and $x$ and $y$ is just the supporting context on that single computation. We will focus on this in a while.\n",
    "\n",
    "[Check this to see more cost/loss functions](https://www.theaidream.com/post/loss-functions-in-neural-networks).\n",
    "\n",
    "<hr>\n",
    "\n",
    "Usually the cost is calculated just for validation purposes. For fitting/training purposes, its derivative makes the work. However, we want to visualize its evolution in this example.\n",
    "\n",
    "Thus, write a function that returns the MSE-cost for a given set of pairs of predicted and true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this example have only two trainable parameters, it is worth to check how the cost looks in what is called the *cost landscape*. This is an image where their axes represent the trainable parameters and the color is for the cost reached by the parameters' combination on that $(\\omega_0, \\omega_1)$ point.\n",
    "\n",
    "Create a function which returns a $40\\times40$ matrix containing the MSE-cost when $\\omega_0$ is in range $[-3, 6]$ and $\\omega_1$ in $[-2, 3]$. The MSE-cost have to be evaluated using a given $(x, y)$ values from the dataset as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this function using $M=20$ randomly picked $(x,y)$ samples on the dataset, and depict the obtained matrix with the `imshow` function. This is the cost landscape. \n",
    "\n",
    "Run several times this code cell. Is the landscape changing? Slightly or greatly? Why?\n",
    "\n",
    "What happens when using more/less randomly picked samples, by increasing/decreasing $M$ (this is the `batchsize`)?\n",
    "\n",
    "Should the cost landscape ideally depend on the specific $(x, y)$ set of values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Stochastic gradient descent\n",
    "\n",
    "The gradient descent algorithm is based on stepping down on the cost landscape to reach the minimum cost, i.e. following the gradient of the cost function (indeed, just the opposite, since gradient is for stepping up, but it is the same with a minus sign).\n",
    "\n",
    "In this example, we are dealing with just two trainable parameters, then the cost landscape have just two dimensions, and it is quite easy to explore the whole plane (just like before) to find the minima. However, this is not possible when dealing with many trainable parameters (i.e. high dimensional space). Therefore, the gradient descent is a strategy to find the minima of a high dimensional function, where the whole landscape is unknown.\n",
    "\n",
    "In math, the gradient is typically done over the space coordinates ($x$, $y$, $z$...). However, the cost function depends on the trainable parameters $\\omega$. In other words, training is to modify $\\omega$ to get better results in prediction/inference time (by reducing the cost function). Thus, the gradient here is a vector, whose components are the partial derivatives respect to each trainable parameter $\\omega_i$.\n",
    "\n",
    "\\begin{equation}  \\tag{2.3}\n",
    "\\vec{\\nabla}_{\\!\\!\\omega} C(\\omega; y) = \\left( \\frac{\\partial C(\\omega; y)}{\\partial \\, \\omega_0} \\; , \\;  \\dots \\; , \\; \\frac{\\partial C(\\omega; y)}{\\partial \\, \\omega_i} \\; , \\; \\dots \\; , \\; \\frac{\\partial C(\\omega; y)}{\\partial \\, \\omega_{N_{\\omega}}} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $C(\\omega; y)$ is the cost function and depends essentially on the different parameters $\\omega_i$ ; ($i=0\\dots N_{\\omega}$).\n",
    "As said before, take into account that $x$ and $y$ are here just like the environment where to compute the cost, but they do not play any other role in the training. All training focus is over the $\\omega$ trainable parameters, not over $(x,y)$. However, we need some data to compute the cost, to make a fitting. Since we are changing the set of data points, the cost landscape is slightly changing on every iteration. This is the *stochastic-ness* of the process.\n",
    "\n",
    "Remember, the goal of the training is to update the trainable parameters somehow. Then, the stochastic gradient descent consist on doing steps according to the partial derivative of the cost function, over the parameter to be updated. However, cost does not depend directly on $\\omega$, but it indirectly does through the inference function $y=g(x;\\omega)$. So, it is worth to apply the chain rule for the derivatives.\n",
    "\n",
    "$$\\omega_i^{\\text{new}} = \\omega_i - \\eta \\, \\delta(\\omega_i) = \\omega_i - \\eta \\, \\frac{\\partial C(\\omega;y)}{\\partial \\omega_i} \\quad ; \\quad (i=0,N_{\\omega})$$\n",
    "\\begin{equation}\\tag{2.4}\n",
    " = \\omega_i - \\eta \\, \\frac{\\partial C(\\omega;y)}{\\partial y(x;\\omega)} \\, \\frac{\\partial y(x;\\omega)}{\\partial \\omega_i}\\quad \\quad \\quad \\quad \\quad \\quad \\;\\;\\,\n",
    "\\end{equation}\n",
    "$$\\ = \\omega_i - \\eta \\,  \\left\\langle \\big[ y(x;\\omega_0, \\omega_1) - y_{\\rm truth}(x) \\big] \\, \\frac{\\partial y(x; \\omega)}{\\partial \\omega_i} \\right\\rangle_{\\!\\!_M}$$\n",
    "\n",
    "\n",
    "where $\\eta$ is the learning rate (how fast we want to progress during the training) and $\\delta(\\omega_i)=\\frac{\\partial C(\\omega;y)}{\\partial \\omega_i}$ is the gradient step for each $\\omega_i$ trainable parameter.\n",
    "\n",
    "$\\frac{\\partial C(\\omega;y)}{\\partial y(x;\\omega)}$ is easily derived from Eq. (2.3) and it is just a scalar factor (not a vector). It basically contains the inference of the current model over an input and its comparison with the ground truth.\n",
    "\n",
    "Finally, $\\frac{\\partial y(x;\\omega)}{\\partial \\omega_i}$ is the analytical gradient of $g(\\cdot)$ over the weights, Eq. (2.1). It only depends on the model architecture (relation between inputs, weights and outputs) and it is the core of the backpropagation algorithm.\n",
    "\n",
    "In this particular case\n",
    "\\begin{equation}\\tag{2.5}\n",
    "\\vec{\\nabla}_{\\!\\!\\omega} \\, y(x;\\omega) = \\vec{\\nabla}_{\\!\\!\\omega} \\, g(x;\\omega_0, \\omega_1) = \\left( \\dots \\; , \\; \\frac{\\partial g(x; \\omega)}{\\partial \\omega_i} \\; , \\; \\dots \\right) = \\left( \\frac{1}{(x - \\omega_1)^2 + 1} \\; , \\; \\frac{+2(x-\\omega_1)\\omega_0}{\\left[(x-\\omega_1)^2+1\\right]^2}  \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Notice that, since we deal with many input samples $x$, all this should be averaged over all $M$ samples, see the angle brackets $\\langle \\cdot \\rangle_{\\!_M}$ in Eq. (2.4), coming from Eq. (2.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Write a function which calculates the gradient step $\\delta(\\omega) = \\frac{\\partial C(\\omega;y)}{\\partial \\omega_i}$ in this particular case, for the given $\\omega$, $(x, y_{\\rm truth})$ ground truth data points and $y$ predicted outputs. \n",
    "\n",
    "How many components should this gradient step have?\n",
    "\n",
    "This function might return not only the $\\delta(\\omega)$, but also the MSE-cost in the current state and with the current batch of data. This last is just useful to monitor the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to perform a train step. This is `your_training_function` (needed in a while):\n",
    "\n",
    "0. The function should take as arguments the current $\\omega$ (a list of the parameters), the learning rate $\\eta$, and the `batchsize`, in this specific order.\n",
    "1. Get a random batch of $(x,y_{\\rm truth})$ input-output pairs from the dataset, this will be the ground truth in this supervised training step. (Use the function done in Section 1.1)\n",
    "2. Update the $\\omega$ calling to the gradient step function done just before.\n",
    "3. Return the following variables: the $x$ input batch, the $y_{\\rm truth}$ target output, the $y$ predicted output, the $C$ MSE-cost obtained in this step (MSE-cost typically is computed before updating the omegas), and the new $\\omega$ parameters (in this specific order).\n",
    "\n",
    "Follow the instructions 0. and 3. in detail to be able to monitor the training with the fancy function some cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def your_training_function(omegas, eta, batchsize):\n",
    "\n",
    "    # your code\n",
    "\n",
    "\n",
    "    return x, y_truth, y_pred, cost, new_omegas    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a loop to apply some training steps (using the function you have just made) and store in a list the MSE-cost returned in every iteration. Plot that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the prediction of the model over a random batch of inputs and compare it with the ground truth. This is exactly like at the end of Section 1.1.\n",
    "\n",
    "Has the fitting improved? Is it good enough?\n",
    "\n",
    "If not, run again the training loop and plot again the prediction, and compare again it with the ground truth.\n",
    "\n",
    "How many steps are needed to get an acceptable fitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Now, let's see in more detail what are happening during this training loop.\n",
    "\n",
    "To do that, we will see how the $\\omega$ descend the gradient on the cost landscape.\n",
    "\n",
    "First, let's define some functions to fancy plot the progress. (just run the next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(x, true_y, fit_y, legend=True, ax=None):\n",
    "    ax = plt.gca() if ax==None else ax\n",
    "\n",
    "    ax1 = ax.scatter(x, true_y, color=\"blue\", label='ground truth')\n",
    "    ax2 = ax.scatter(x, fit_y, color=\"orange\", label='fitting')\n",
    "    ax3 = ax.plot([x, x], [true_y, fit_y], 'y:', linewidth=1, \n",
    "                  label='deviation')\n",
    "    \n",
    "    if legend:\n",
    "        ax.legend(handles=[ax1, ax2, ax3[0]], loc='upper left', fontsize=6)\n",
    "    ax.set_xlim(-4,4)\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(f\"$y=f(x;{omegas})$\", fontsize=6)\n",
    "\n",
    "    \n",
    "def plot_landscape(true_y, true_x, ws, ax=None):\n",
    "    ax = plt.gca() if ax==None else ax\n",
    "    \n",
    "    Nw = 40\n",
    "    \n",
    "    # The landscape is on omegas, WHY?\n",
    "    w0s = np.linspace(-3, 6, Nw)  # exploring from -3 to 6 for th0 (40 samples)\n",
    "    w1s = np.linspace(-2, 3, Nw)  # exploring from -2 to 3 for th1 (40 samples)\n",
    "\n",
    "    landscape = np.zeros([Nw, Nw])  # init landscape\n",
    "    for j0, w0 in enumerate(w0s):\n",
    "        for j1, w1 in enumerate(w1s):\n",
    "            pred_y = w0 / ( (true_x-w1)**2 + 1.0 )\n",
    "            landscape[j1, j0] = 0.5 * np.mean((pred_y - true_y)**2)\n",
    "    \n",
    "    im = ax.contourf(landscape, extent=[-3, 6, -2, 3])\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    ax.contour(landscape, extent=[-3, 6, -2, 3], colors=\"white\")\n",
    "\n",
    "    ax.set_xlabel(r'$\\omega_0$')\n",
    "    ax.set_ylabel(r'$\\omega_1$')\n",
    "\n",
    "    \n",
    "def plot_info(x, true_y, pred_y, omegas, stepping, history, arrows=True):\n",
    "    \"\"\" This is a funtion to plot the progress on the training.\n",
    "    \"\"\"\n",
    "    max_MSE = history.max()*1.1 if history.size>0 else 1  # skipping empty hist.\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    fig = plt.figure(figsize=(5,3))\n",
    "    gs = fig.add_gridspec(2, 2, hspace=.35, wspace=.3)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax3 = fig.add_subplot(gs[1, :])\n",
    "    \n",
    "    plot_curves(x, true_y, pred_y, legend=True, ax=ax1)  # compare fiting with truth\n",
    "    plot_landscape(true_y, x, omegas, ax2)  # to see the progress\n",
    "\n",
    "    # plot where we are on the landscape\n",
    "    ax2.scatter([omegas[0]], [omegas[1]], color=\"orange\")  \n",
    "    if arrows:  # where we go\n",
    "        ax2.arrow(*(omegas),*(-stepping), color='yellow',\n",
    "                  width=0.05, length_includes_head=True)\n",
    "    \n",
    "    # let's plot the cost history\n",
    "    ax3.plot(range(history.size), history, marker='o')\n",
    "    ax3.set_xlim(0, history.size-1)\n",
    "    ax3.set_xticks(range(0, history.size, 3))\n",
    "    ax3.set_ylim(0.0, max_MSE)\n",
    "    ax3.set_xlabel(r\"$step$\")\n",
    "    ax3.set_ylabel(r\"$MSE$\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def animated_train(your_func, eta, batchsize, init_omegas=None, nsteps=20):\n",
    "    if init_omegas is None:\n",
    "        global omegas  # let's continue where we was\n",
    "    else:\n",
    "        omegas = init_omegas\n",
    "\n",
    "    cost_history = np.zeros(nsteps)\n",
    "    delta_w = 0\n",
    "    for n in range(nsteps):  # it can be replaced with a while with some criteria\n",
    "\n",
    "        old_omegas = omegas.copy()\n",
    "        x, y_true, y_pred, cost, omegas = your_func(omegas, 0.7, 100)\n",
    "        delta_w = old_omegas - omegas\n",
    "        \n",
    "        # let's plot where we are and where we go\n",
    "        plot_info(x, y_true, y_pred, old_omegas, delta_w, cost_history, True)\n",
    "        print('estimated omega:', omegas)\n",
    "        sleep(0.5)\n",
    "        \n",
    "        cost_history[n] = cost\n",
    "        \n",
    "        plot_info(x, y_true, y_pred, omegas, delta_w, cost_history, False)\n",
    "        print('estimated omega:', omegas)\n",
    "        sleep(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function below where the first argument is the training function you defined in a couple of cells above.\n",
    "\n",
    "Play with the parameters `eta` and `batchsize`.\n",
    "\n",
    "How does it influence learning progress?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your_func: The trining funtion defined by your own. It has to follow the given instructions for the arguments and returning variables.\n",
    "# eta: \"learning rate\" (gradient descent step size)\n",
    "# batchsize: stochastic x samples used per step (just a few to be fast)\n",
    "# itit_omegas: the starting training process\n",
    "\n",
    "animated_train(your_func=your_training_function, eta=1.85, batchsize=300, init_omegas=[-2.3, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why we are not going directly to the minimum of the landscape?\n",
    "\n",
    "\n",
    "Why is this algorithm ***Stochastic*** *Gradient Descent*? Check the meaning and reason of a <u>[stochastic](https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/)</u> approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Non constant learning rate (Schedule or Adaptive)\n",
    "\n",
    "**Optional**: Modify `your_training_function` in such a way to run with a non-constant learning rate.\n",
    "\n",
    "[Check this to know more on learning rate](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backpropagation\n",
    "\n",
    "Like in the Stochastic Gradient Descent, we update trainable parameters of a neural network (weights and biases) in the following way\n",
    "\n",
    "\\begin{equation} \\tag{2.6}\n",
    "\\omega_{ij}^l = \\omega_{ij}^l - \\eta \\, \\delta(\\omega_{ij}^l)\n",
    "\\end{equation}\n",
    "\\begin{equation} \\tag{2.7}\n",
    "b_{i}^l = b_{i}^l - \\eta \\, \\delta (b_{i}^l)\n",
    "\\end{equation}\n",
    "\n",
    "where $l=1\\dots L$ labels all the net layers, $i=1\\dots N_l$ and $j=1\\dots N_{l-1}$ label all the $N_l$ neurons of the $l$-th layer, and $\\delta (\\omega) = \\frac{\\partial C(\\omega;x)}{\\partial \\omega}$ is the gradient step to update the parameters, where $\\omega$ includes, not only all weights, but also all biases.\n",
    "\n",
    "In neural networks, the cost function $C(\\omega;y)$ can have a very deep dependency on weights and biases, especially those corresponding to the first layers. However, this deep dependency can be solved through the derivative chain rule (like before). Moreover, the calculations done to estimate the gradient step for the last layers can be reused for deeper layers, as we recall below.\n",
    "\n",
    "* **Output layer** ($l=L$):\n",
    "\n",
    "The gradient step $\\delta$ to update weights $\\omega^{L}_{ij}$ and biases $b^{L}_{i}$ of the last layer ($l=L$), according to Eq. (2.4), is\n",
    "\n",
    "\\begin{equation} \\tag{2.8}\n",
    "\\delta(\\omega^L) = \\frac{\\partial C(\\omega;y) }{\\partial \\omega^L }\n",
    "                 = \\frac{\\partial C(\\omega;y) }{\\partial y^{L}_{i} }\\frac{\\partial y^{L}_{i} }{\\partial \\omega^{L} } \n",
    "\\end{equation}\n",
    "                        \n",
    "where $\\omega^L$ stands for any trainable parameter on the last layer; both, $\\omega^{L}_{ij}$ and $b^{L}_{i}$. Notice that the first factor depends only on the cost function, whereas the second only does on the architecture of the network.\n",
    "\n",
    "Taking the simple MSE-cost function $C = \\frac{1}{2} \\left( y_i^L - \\hat{y}_i \\right)^2$, where $\\hat{y}_i=y^{\\rm truth}_i$ is the ground truth result for the $i$-th component (neuron) and $y_i^L=y_i^L(x;\\omega)$ is the actual value gotten with the current network state; we just simplify the notation.\n",
    "\n",
    "Notice the current output value can be expressed as a combination of the previous layer: \n",
    "\\begin{equation} \\tag{2.9}\n",
    "y_i^L = f_L\\!\\!\\left[x_j^{L}\\right] =  f_L\\!\\!\\left[\\omega_{ij}^{L}y_j^{L-1} + b_i^L\\right]\n",
    "\\end{equation}\n",
    "where $f_l[\\cdot]$ is the activation function for the $l$-th layer, then\n",
    "\n",
    "\\begin{equation} \\tag{2.10}\n",
    "\\frac{\\partial C }{\\partial y^{L}_{i} } = \\left( y_i^L - \\hat{y}_i \\right)\n",
    "\\end{equation}\n",
    "\n",
    "this is just the deviation between the predicted result and the ground truth.\n",
    "\n",
    "On the other hand,\n",
    "\n",
    "\\begin{equation} \\tag{2.11}\n",
    "\\frac{\\partial y^{L}_{i} }{\\partial \\omega^{L}_{ij} } = f'_L\\!\\!\\left[ x_i^L\\right] \\, y_j^{L-1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation} \\tag{1.12}\n",
    "\\frac{\\partial y^{L}_{i} }{\\partial b^{L}_{i} } = f'_L\\!\\!\\left[ x_i^L\\right]\n",
    "\\end{equation}\n",
    "\n",
    "These two last expressions only contain the current value returned by certain neurons $y_j^{L-1}$ and the value when using the derivative of the activation function, instead.\n",
    "\n",
    "For convenience, let's define\n",
    "\\begin{equation} \\tag{2.13}\n",
    "\\boxed{ \\Delta^{\\!L}_i = \\left( y_i^L - \\hat{y}_i \\right) \\, f'_L\\!\\!\\left[ x_i^L\\right] }\n",
    "\\end{equation}\n",
    "\n",
    "Finally,\n",
    "\n",
    "\\begin{equation} \\tag{2.14}\n",
    "\\boxed{ \\delta(\\omega_{ij}^L) = \\Delta_i^{\\!L} \\otimes y_j^{L-1} }\n",
    "\\end{equation}\n",
    "\\begin{equation} \\tag{2.15}\n",
    "\\boxed{ \\delta(b_{i}^L) = \\Delta_i^{\\!L} }\n",
    "\\end{equation}\n",
    "\n",
    "These boxed equations are all we need to be able to update the weight and biases of the last layer. Of course, all calculus made above are computed with matrices and vectors of indices $i$ and $j$. Notice that $\\otimes$ stands for the outer product, which generates a matrix from two vectors. Moreover, usually it is processed with batches of input-output pairs, where final values are averaged for those batches. An efficient way to implement this in batches is explained at the end of this cell.\n",
    "\n",
    "* **Last hidden layer**:\n",
    "\n",
    "The procedure is exactly the same as before. However, in order to update weights $\\omega^{L-1}_{jk}$ and biases $b^{L-1}_{j}$ of the last hidden layer ($l=L-1$), Eq. (2.8) becomes\n",
    "\n",
    "\\begin{equation} \\tag{2.16}\n",
    "\\delta(\\omega^{L-1}) = \\frac{\\partial {C} }{\\partial \\omega^{L-1} }\n",
    "            = \\frac{\\partial {C} }{\\partial y^{L}_{i} }\n",
    "              \\frac{\\partial y^{L}_{i} }{\\partial y^{L-1}_{j} }\n",
    "              \\frac{\\partial y^{L-1}_{j} }{\\partial \\omega^{L-1} } \n",
    "\\end{equation}\n",
    "Therefore,\n",
    "\\begin{equation} \\tag{2.17}\n",
    "{ \\delta(\\omega_{jk}^{L-1}) = \\Delta_i^{\\!L} \\; \\omega_{ij}^L \\, f'_{L-1}\\!\\!\\left[ x_j^{L-1}\\right] \\otimes y_k^{L-2} = \\Delta_j^{\\!L-1} \\otimes y_k^{L-2} }\n",
    "\\end{equation}\n",
    "\\begin{equation} \\tag{2.18}\n",
    "{ \\delta(b_{j}^{L-1}) = \\Delta_i^{\\!L} \\; \\omega_{ij}^L \\, f'_{L-1}\\!\\!\\left[ x_j^{L-1}\\right]  = \\Delta_j^{\\!L-1}}\n",
    "\\end{equation}\n",
    "\n",
    "* **General formula**:\n",
    "\n",
    "Notice that Eqs. (2.17, 2.18) can be generalized for any layer $l=L-m$ as\n",
    "\\begin{equation} \\tag{2.19}\n",
    "\\boxed{ \\delta(\\omega_{pq}^{l}) = \\Delta_p^{\\!l} \\otimes y_q^{l-1} }\n",
    "\\end{equation}\n",
    "\\begin{equation} \\tag{2.20}\n",
    "\\boxed{ \\delta(b_{p}^{l}) = \\Delta_p^{\\!l} }\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation} \\tag{2.21}\n",
    "\\boxed{ \\Delta_{p}^{l} = \\Delta_{r}^{\\!l+1}  \\cdot \\omega_{rp}^{l+1} \\, f'_{l}\\!\\!\\left[ x_p^{l}\\right] } \\quad ; \\quad (l=L-1\\dots 1)\n",
    "\\end{equation}\n",
    "\n",
    "Notice that Eq. (2.21) shows how the $\\Delta$ error made in the prediction is back-propagated according to weights and activations of every layer.\n",
    "\n",
    "Notice also that $y_q^{l-1}$ in Eq. (2.19) is the input data when $l=0$, the last step backing the propagation.\n",
    "\n",
    "* **General notes**:\n",
    "\n",
    "Notice that boxed equations contain all the backpropagation process. For Eq. (2.13), we need the prediction $y_i^L$ and the derivative of the last step $f'_L\\!\\!\\left[ x_i^L\\right]$. Moreover, we also need all the intermediate values $y_p^{l}$ and their derivatives $f'_l\\!\\!\\left[ x_p^l\\right]$ for Eqs. (2.19, 2.20). Therefore, the forward prediction (inference) is needed in every training step, not only to get the cost value and its derivative (Eq. 2.13), but also in such a way to store all intermediate neurons values $y_p^{l}$ and their derivative $f'_l\\!\\!\\left[ x_p^l\\right]$. Then, all the effort is to make forward inferences storing the intermediate states and their derivatives and, then, apply the backprogation depicted in Eq. (2.21).\n",
    "\n",
    "That's it!\n",
    "\n",
    "\n",
    "* **Implementation in batches**:\n",
    "\n",
    "As we said before, we must be able to accept input-output pairs in batches in order to increase the performances (in time and in precision, recall stochasticness/batchsize relation). Then, all variables regarding data should earn an extra dimension (weights and the biases, and their gradient steps remain like in equations above, no extra dimension is needed for them). In practice, we need just to be carefully with the shape and order of the products. This is, since we want to average all values over the batches, then to compute Eqs. (2.14, 2.19) we have to do a dot product (sum-product) with the batch index as inner index and divide by the batchsize (in this way, we incorporate the average over the whole batch). Similarly, for Eqs. (2.15, 2.20) we have to sum over the $\\Delta$ and divide by the batchsize. In addition, the inner index of $\\Delta \\cdot \\omega$ (in Eq. 2.21) is the repeated index $r$, getting a shape of `batchSize`$\\times$`layerSize`. Finally, this is multiplied element-wise for the activation contribution, like in Eq. (2.13)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implement of backpropagation for a general (fully connected) network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it step by step.\n",
    "\n",
    "Firstly, write a function to apply the activation function. It has to be ready for training, it means that it has to return both, the result of applying the activation function, and its analytical derivative. Defining this function will let us to easily replaced with another activation function.\n",
    "\n",
    "Use the *Sigmoid* as activation function, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write a function to do a layer step, also ready for training. This is, given an input vector $x$, a weights matrix $\\omega$, and a biases vector $b$, compute the matrix operation and, then, apply the activation function and its derivative using the function made before. (This function will be also used in Section 2.3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write now a function to make a forward pass through the whole network for a given input vector $x$, a list of weights $[w]_l$ (one weights matrix for each layer) and a list of biases $[b]_l$ (idem), for $l=1\\dots L$, being $L$ the number of layers in the network. Thus, use iteratively the previous function, over a given list of weights and biases.\n",
    "\n",
    "This is very similar to what you did in Tutorial 1 (inference). However, keep in mind, we want it for training. Therefore, we need, not only the final result (like in prediction), but also all inner states of the neurons $y_q^{\\,l}$ and their derivative $y\\prime_q^{\\,l}=f'_{l}\\!\\!\\left[ x_p^{l}\\right]$. So, store and return a couple of list of neuron states $[y_q]_l$, one for the common state and the other for its derivative $[y_q^{\\,\\prime}]_l$, for $l=1\\dots L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to back propagate the error prediction, evaluated by the cost function. Thus, you should take the ground truth $\\{x, y_i^{\\rm truth}\\}$ and the current parameters, i.e. the weights list $[\\omega]_l$ and biases list $[b]_l$ in the arguments.\n",
    "\n",
    "This function has to return the gradient steps $\\delta(\\omega)$ and $\\delta(b)$.\n",
    "\n",
    "Note that $\\delta(\\omega)$ and $\\delta(b)$ have to be equal in size than $\\omega$ and $b$, then, they are list of matrices ($\\omega$) and list of vectors ($b$). Thus, you can initialize them using the `np.zeros_like(a)` numpy's function inside a <u>[list comprehension](https://www.geeksforgeeks.org/python-list-comprehension/)</u>.\n",
    "\n",
    "Then, you can fill their last item following Eqs. (2.14, 2.15), using Eq. (2.13).\n",
    "\n",
    "Finally, you can fill them recursively backwards following Eqs. (2.19, 2.20), using Eq. (2.21).\n",
    "\n",
    "Note that you will also need all inner states of the neurons $[y_q]_l$ and their derivative $[y^{\\,\\prime}_q]_l$, so ask for them in the arguments.\n",
    "\n",
    "Take into account that all data variables has an extra dimension for batch processing and, then, you have to average over that dimension to get a stochastic result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it together.\n",
    "\n",
    "Write a function to perform a training step. This is to update the trainable parameters stepping down the gradient.\n",
    "\n",
    "To do that, \n",
    "\n",
    "1. apply forward pass (inference) through the whole network, while storing the inner states (and their derivative), just by calling the function defined before.\n",
    "\n",
    "1. take that inner states and apply the backpropagation using the function made before, to get the gradient steps $\\delta(\\omega)$ and $\\delta(b)$.\n",
    "\n",
    "1. update the weights and biases following the Eqs. (2.6, 2.7).\n",
    "\n",
    "1. compute the MSE-cost of the network. (You can use the result provided by point 1.)\n",
    "\n",
    "We will use this function to completely train a network, then it is important to check that:\n",
    "\n",
    "- it has as argument: the input data $x$, the ground truth $y^{\\rm truth}$, the current weights list $[\\omega]_l$, the current biases list $[b]_l$, and the learning rate $\\eta$.\n",
    "\n",
    "- it returns: the predicted result $y^L$, the cost $C$, the updated weights list $[\\omega^{\\rm new}]_l$, and the updated biases list $[b^{\\rm new}]_l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train a net to fit a 2D function (an arbitrary image) in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next nonlinear function as a ground truth, so as the target behavior of the network.\n",
    "Yes, it is an arbitrary image. However, it can be seen as a non-trivial relation between a couple of inputs $(x_1,x_2)$ (corresponding to every position on the image), and the single output $y_{\\rm truth}$ (corresponding to the pixel value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pixel image\n",
    "URL = \"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/data/Smiley.png\"\n",
    "face = imageio.v2.imread(requests.get(URL).content)\n",
    "\n",
    "face = face[2::6,2::6,0]\n",
    "face = face[:,::-1].astype('float64')  # and flip... to get the right view!\n",
    "\n",
    "face += np.random.uniform(low=0, high=0.05*face.max(), size=face.shape)\n",
    "\n",
    "# normalize between 0 and 1\n",
    "face -= face.min()\n",
    "face_size = np.shape(face)[0] # assuming a square image!\n",
    "y_dataset = (face.astype(dtype='float'))/face.max()  # [0, 1]  <--- y_target is the goal of the training\n",
    "y_dataset = y_dataset.flatten()[:,None]\n",
    "print(f\"{y_dataset=}: {y_dataset.shape=}\")\n",
    "\n",
    "print(\"\")\n",
    "# single_range = np.arange(-face_size, face_size)/face_size\n",
    "x1_2D, x2_2D = np.mgrid[-face_size//2:face_size//2, -face_size//2:face_size//2] / face_size\n",
    "x_dataset = np.stack([x1_2D.flatten(), x2_2D.flatten()], axis=1)     #        <--- x is the input values\n",
    "print(f\"{x_dataset=}: {x_dataset.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the dataset is: `x_dataset` and `y_dataset` variables, corresponding to the available inputs and their outputs, respectively. \n",
    "\n",
    "Define a function to get a reduced batch of the whole dataset, containing a `batchsize` number of representations of the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below shows a picture according to certain $\\vec{x}_M=(x_1, x_2)_M$ input batch and $y_M$ output batch. It is shown in the `axs` axis argument if it is passed, else it is shown in the active axis (current or new figure). Non set pixels are shown in white (out of the colormap).\n",
    "\n",
    "Show a batch of `batchsize` $=M=256$ pixel in an image by finishing the next code cell (see comment at the end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_inference(inputs, outputs, ax=None):\n",
    "\n",
    "    ax = ax or plt.gca()\n",
    "\n",
    "    image = 1/np.zeros_like(face)  # let's set unexplored points at NaN -> White in imshow (viridis)\n",
    "\n",
    "    idx = ((inputs+0.5)*face_size).astype(dtype='int')\n",
    "\n",
    "    image[idx[:,0], idx[:,1]] = outputs[:,0]\n",
    "   \n",
    "    im = ax.imshow(image, extent=(-0.5, 0.5, -0.5, 0.5))\n",
    "    ax.set_title(f\"vmin={outputs.min():.2f}\\nvmax={outputs.max():.2f}\", fontsize=6, y=1, pad=-14)\n",
    "    ax.set_xlabel(\"$x_1$\", fontsize=4)\n",
    "    ax.set_ylabel(\"$x_2$\", fontsize=4)\n",
    "    ax.tick_params(axis='both', labelsize=4)\n",
    "\n",
    "    \n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(2,2))\n",
    "imshow_inference(x_dataset, y_dataset, ax=axs[0])  # This shows the whole image in the first axis\n",
    "\n",
    "# Add here you code to depict a batch of the dataset of 256 values on the second axis (axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As many samples to be computed at once, as better performance in the training (and less stochastic). However, it needs more computational resources (basically, RAM memory and CPU time). How many samples is a reasonable minimum for this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Network definition and evaluation\n",
    "\n",
    "Define a network having 4 layers with 2 input neurons, 50 neurons in the two hidden layers, and 1 single output neuron.\n",
    "\n",
    "Initialize the weights with random numbers within a Gaussian distribution and biases with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an inference function to evaluate the network for a given input and the current state of the model (weights and biases). To apply the activation function, use the same function done before in the Section 2.1. Now it is just to infer a result, so do not store the inner neuron states, neither their derivatives, and just return the final result. This is the prediction inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training step function (backpropagation) several times to train the network and recover the target image (smiling).\n",
    "\n",
    "Make a loop to apply many times (100k or, even, 1 million) the training step. _-you can stop the running by clicking the STOP button on the bar (ctrl+C in some environs)-_\n",
    "\n",
    "Before the loop:\n",
    "\n",
    " * Set a $\\eta$ learning rate (about 1)\n",
    " * Set a $M$ batch size, according to your checks before\n",
    " * Initialize an empty list to store the cost history.\n",
    "\n",
    "Inside the loop:\n",
    " * Create a batch of data using the function written in Section 2.2.\n",
    " * Make a training step using this batch of data, using the last function written in Section 2.1.\n",
    " * Append the returned cost value to the cost list.\n",
    " * Only after some iterations (let's say 10000), do:\n",
    "   * Clear any previous figure by typing `clear_output(wait=True)`\n",
    "   * Create a $2\\times2$ subplot. You may set a `figsize=(3,3)` to contain the size of the figure.\n",
    "   * Call the `imshow_prediction` function to show the last gotten result (over the current batch) on the axis `[0,0]`.\n",
    "   * Apply the inference function just previously written to the whole `x_dataset` to validate the current state of the network, the result is the $y_{\\rm val}$ validation output.\n",
    "   * Call the `imshow_prediction` function using the whole `x_dataset` and $y_{\\rm val}$, on the axis `[0,1]`.\n",
    "   * Compute the cost $\\frac{1}{2}(y_{\\rm val}-y_{truth})^2$. Do not average this cost, i.e. it is a flattened image.\n",
    "   * Call the `imshow_prediction` function using the whole `x_dataset` and the cost just computered in the last point, on the axis `[1,0]`.\n",
    "   * Plot the cost history on the axis `[1,1]`.\n",
    "   * Add `plt.show()` after configuring the figure to show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with different configurations: learning rate, batch size, number of neurons, number of layers... even, the activation function.\n",
    "\n",
    "To modify the activation function written in Section 2.1, just re-define it in the cell below, before reinitialize the training again.\n",
    "\n",
    "Check  https://medium.com/towards-data-science/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79\n",
    "\n",
    "Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save and load a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are the information about the model?\n",
    "\n",
    "Save the information using the `np.savez()` function. Use `*weights_list` and `*biases_list`. See the asterisk in front of the list variables to be able to store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can recover a whole neural network model and state.\n",
    "\n",
    "Load that information again and reproduce the image without making any new training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inside a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set to 0 all the weights on all neurons of a certain hidden layer, except for one neuron and, then, let's plot the result to check the image produced by that neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, a model from internet is download to show this. You can load your own neural network model (weights and biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_smile_model = 'https://github.com/dmaluenda/hands_on_machine_learning/raw/master/data/02_smile_model_2layers_5neurons_sigmoid.npz'\n",
    "\n",
    "stored_model = np.load(get_npz_remote(URL_smile_model))  # You can replace 'get_npz_remote(...)' with your path set in last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_stored = [stored_model[\"arr_\"+str(i)] for i in range(len(stored_model)//2)]\n",
    "b_stored = [stored_model[\"arr_\"+str(len(stored_model)//2+i)] for i in range(len(stored_model)//2)]\n",
    "\n",
    "layers = len(w_stored)-1\n",
    "neurons = w_stored[1].shape[1]\n",
    "\n",
    "fig, axs = plt.subplots(neurons, layers, figsize=(layers, neurons))\n",
    "\n",
    "for l in range(layers):\n",
    "    for n in range(neurons):\n",
    "        w_ON = w_stored[l+1][n,:].copy()\n",
    "        \n",
    "        w_current = np.zeros_like(w_stored[l+1])\n",
    "        w_current[n,:] = w_ON\n",
    "\n",
    "        w_list = w_stored.copy()\n",
    "        w_list[l+1] = w_current\n",
    "        \n",
    "        y_predi_all = inference(x_dataset, w_list, b_stored)  # edit this according to your inference function done in Section 2.3\n",
    "        imshow_inference(x_dataset, y_predi_all, axs[n, l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Play with https://playground.tensorflow.org "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "223.852px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
