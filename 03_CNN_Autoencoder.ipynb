{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><sub>This notebook is distributed under the <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\" target=\"_blank\">Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license</a>.</sub></div>\n",
    "<h1>Hands on Machine Learning  <span style=\"font-size:12px;\"><i>by <a href=\"https://webgrec.ub.edu/webpages/000004/cat/dmaluenda.ub.edu.html\" target=\"_blank\">David Maluenda</a></i></span></h1>\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://atenea.upc.edu/course/view.php?id=95161\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/upc_logo_49px.png\" width=\"130\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "  </td>\n",
    "  <td>   <!-- gColab -->\n",
    "    <a href=\"https://colab.research.google.com/github/dmaluenda/hands_on_machine_learning/blob/master/03_CNN_Autoencoder.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/colab_logo_32px.png\" />\n",
    "      Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- github -->\n",
    "    <a href=\"https://github.com/dmaluenda/hands_on_machine_learning/blob/master/03_CNN_Autoencoder.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/github_logo_32px.png\" />\n",
    "      View source on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- download -->\n",
    "    <a href=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/03_CNN_Autoencoder.ipynb\"  target=\"_blank\"\n",
    "          download=\"03_CNN_Autoencoder\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/download_logo_32px.png\" />\n",
    "      Download notebook\n",
    "      </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{III}$. Convolutional Neural Networks and Autoencoders (using Keras)\n",
    "\n",
    "Hands on \"Machine Learning on Classical and Quantum data\" course of\n",
    "[Master in Photonics - PHOTONICS BCN](https://photonics.masters.upc.edu/en/general-information)\n",
    "[[UPC](https://photonics.masters.upc.edu/en) +\n",
    "[UB](https://www.ub.edu/web/ub/en/estudis/oferta_formativa/master_universitari/fitxa/P/M0D0H/index.html?) +\n",
    "[UAB](https://www.uab.cat/en/uab-official-masters-degrees-study-guides/) +\n",
    "[ICFO](https://www.icfo.eu/lang/studies/master-studies)].\n",
    "\n",
    "**Tutorial 3**\n",
    "\n",
    "This notebook shows how to:\n",
    "- manage image dataset to be able to fit dense or convolutional models\n",
    "- create _one hot encoded_ labels\n",
    "- split datasets for training, validation and testing neural networks\n",
    "- implement neural networks using the Tensorflow/Keras module\n",
    "- recognize/classify images with dense nets (supervised learning)\n",
    "- recognize/classify images with convolutional nets (supervised learning)\n",
    "- implement image denoising using autoencoders (pseudo-unsupervised learning)\n",
    "- quick review of: VAE (unsupervised learning)\n",
    "- implement callbacks, like an automatic early stopper\n",
    "\n",
    "**References**:\n",
    "\n",
    "[1] [Machine Learning for Physicists](https://machine-learning-for-physicists.org/) by Florian Marquardt.<br>\n",
    "[2] [Keras](https://keras.io/getting_started/): a deep learning API written in Python.<br>\n",
    "[3] [Tensorflow](https://www.tensorflow.org/api_docs/python/tf): an open source machine learning platform.<br>\n",
    "[4] [Using neural nets to recognize handwritten digits](http://neuralnetworksanddeeplearning.com/chap1.html).<br>\n",
    "[5] [pix2pix](https://www.tensorflow.org/tutorials/generative/pix2pix): Image-to-image translation with a conditional GAN.<br>\n",
    "[6] Dimension reduction in [Towards data science](https://ekamperi.github.io/machine%20learning/2021/01/21/encoder-decoder-model.html) <br>\n",
    "[7] VAE example on [Towards data science](https://towardsdatascience.com/variational-autoencoders-as-generative-models-with-keras-e0c79415a7eb).<br>\n",
    "[8] https://github.com/kartikgill/Autoencoders.<br>\n",
    "[9] https://github.com/dhanushkamath/VariationalAutoencoder. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports: Basics and tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module tensorflow have to be installed (version tested 2.18).\n",
    "\n",
    "`!pip install tensorflow==2.18`\n",
    "\n",
    "Version of NumPy tested: 2.0\n",
    "\n",
    "`!pip install numpy==2.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist  # dataset of handwritten numbers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300  # highres display\n",
    "\n",
    "# for updating display \n",
    "# (very simple animation)\n",
    "from IPython.display import clear_output\n",
    "from time import time, sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import data\n",
    "\n",
    "In this tutorial, we will use the MNIST dataset. It is a dataset with many images depicting handwritten numbers. Additionally, it provides their corresponding numeric value (label).\n",
    "\n",
    "We will do three exercises with this dataset:\n",
    "\n",
    "1. detect the handwritten numbers (classification)\n",
    "1. denoise images (pseudo-autoencoder)\n",
    "1. dimensional reduction (autoencoder)\n",
    "\n",
    "One key point on using data for training neural networks is to have three independent subsets.\n",
    "\n",
    " - Training dataset: It is used for training. The biggest subset, typically around 70%.\n",
    " - Validation dataset: It is used to monitor the training, but not used for backpropagation. Typically around 15%.\n",
    " - Test dataset: It is used only after the training in order to check the performance of the neural network. Typically around 15%.\n",
    " \n",
    "The function below returns a whole dataset of 70k handwritten numbers and their corresponding number. It is split in this three independent sub-datasets with 50k for training, 10k for validation, and 10k for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mnist_loader\n",
    "~~~~~~~~~~~~\n",
    "\n",
    "taken from Nielsen's online book:\n",
    "http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "\n",
    "\n",
    "A library to load the MNIST image data.  For details of the data\n",
    "structures that are returned, see the doc strings for ``load_data``\n",
    "and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n",
    "function usually called by our neural network code.\n",
    "\"\"\"\n",
    "\n",
    "def load_data():\n",
    "    \"\"\" Return three datasets (train/val/test), each one being a tuple of\n",
    "         - numpy array of setSize x 28 x 28 x 1: Set of images\n",
    "         - numpy array of setSize x 1: Set of ground truth values\n",
    "        setSize is 50000 for the train set and 10000 for the val and test sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get raw data: a tuple of two entries (train/test) each one \n",
    "    #  being also a tuple of two entries, one for a set of 28x28 images\n",
    "    #  and the other being a set of their corresponding integers\n",
    "    (train_val_X, train_val_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "    # to convert values from 0 to 255 into range 0. to 1.\n",
    "    train_val_X = train_val_X.astype('float32') / 255.\n",
    "    test_X = test_X.astype('float32') / 255.\n",
    "    \n",
    "    # Conv2D need 3D images (row, columns, chanels), so adding the chanel dimension\n",
    "    train_val_X = np.reshape(train_val_X, (len(train_val_X), 28, 28, 1)) \n",
    "    test_X = np.reshape(test_X, (len(test_X), 28, 28, 1))\n",
    "    \n",
    "    train_X = train_val_X[:-10000]\n",
    "    val_X = train_val_X[-10000:]\n",
    "\n",
    "    train_y = train_val_y[:-10000]\n",
    "    val_y = train_val_y[-10000:]\n",
    "    \n",
    "    return (train_X, train_y), (val_X, val_y), (test_X, test_y)\n",
    "\n",
    "(train_x, train_y), (val_x, val_y), (test_x, test_y) = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to check a handwritten number. Run it several times to see check different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0,50000)\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(train_x[idx,:,:], cmap='gray')\n",
    "plt.title(f\"number: {train_y[idx]}\");\n",
    "plt.show()\n",
    "\n",
    "print(f\"{train_x.shape=}\")\n",
    "print(f\"{train_y.shape=}\")\n",
    "\n",
    "print(f\"{val_x.shape=}\")\n",
    "print(f\"{val_y.shape=}\")\n",
    "\n",
    "print(f\"{test_x.shape=}\")\n",
    "print(f\"{test_y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the datasets shape!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Labels preprocessing (One hot encoding for classification)\n",
    "\n",
    "The best strategy for categorical classification (classify input items into certain categories, where every item belong only in one of that categories) is to have one dedicated output neuron for each possible category. In this way, the assigned category for a given input will correspond to the most activated (hottest) output neuron. This is called one hot encoding.\n",
    "\n",
    "Check [<u>this</u>](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) for more information on one hot encoding.\n",
    "\n",
    "Therefore, the first step is to prepare the dataset in this way.\n",
    "\n",
    "The MNIST dataset contains pairs of $x$ images and $y$ labels. Labels are integers corresponding to the truly handwritten value. \n",
    "\n",
    "Write a function which take a set of integers values and returns a set of _one hot encoded_ labels. This is, they should be vectors of 10 components where all them are zero, except for the corresponding true handwritten number.\n",
    "\n",
    "For example, a $3$ should be `[0 0 0 1 0 0 0 0 0 0 0]` and a $7\\rightarrow$ `[0 0 0 0 0 0 0 1 0 0]` and so on.\n",
    "\n",
    "Then, take the three imported raw datasets and create their corresponding $y$ one hot encoded labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Tensorflow and Keras: Image recognition with a fully connected neural network\n",
    "\n",
    "Let's start with a fully connected network (also known as dense networks or perceptron).\n",
    "They are not convolutional networks, but let's start with the simplest case, just to introduce tensorflow and keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Network definition using tensorflow/keras\n",
    "\n",
    "Let's define a dense network with one hidden layer containing about 30 neurons.\n",
    "\n",
    "Check [`tensorflow.keras.Sequencial`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) class and its methods, e.g. [`add()`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#add), to create a neural network model.\n",
    "\n",
    "The first layer have to be a [`tensorflow.keras.layers.Input`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Input). How many neurons should it have?\n",
    "\n",
    "Check also [`tensorflow.keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) to add fully connected layers.\n",
    "This hidden layer may contain any common activation function (relu, sigmoid, etc) try different activation function for this hidden layer, and check the performance of the model.\n",
    "\n",
    "How many output neurons should the last layer have? \n",
    "For categorical classification, the output layer should deal with a [softmax activation function](https://gombru.github.io/2018/05/23/cross_entropy_loss/). This activation function perfectly matches with the hot encoding label you prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the method [`Sequential.compile()`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#compile) to configure the model for training, by assigning a loss function, and an optimizer (optionally, also some metrics).\n",
    "\n",
    " - The loss function is to compute the backpropagation. For the one hot encoding classification, the most sensitive loss function is the categorical cross entropy (check the link on softmax function above for more information).\n",
    " - The most used optimizer is the Adam, an adaptive learning rate based on the momentum statistics. Check [`tensorflow.keras.optimizers.adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) for more information.\n",
    " - The metrics are also evaluated over the training and the validation datasets, and they are stored in the history to show the progress of the training. This is just to monitor the training progress (not to train), then it is very optional argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `Sequential.summary()` method of your model to check the created model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the 'Output shape' column. Do these numbers make sense? What is for that `None` in the first dimension?\n",
    "\n",
    "Check also the number of trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Image preprocessing (flatten images for dense networks)\n",
    "\n",
    "The input of fully connected neural networks (dense networks) are several neurons, corresponding to a vector (1D). However, we have images as input data (2D). So, we have to flatten that images. So, take the images from the three raw datasets and flatten them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the method [`Squential.predict_on_batch()`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#predict_on_batch) over the flattened `test` images to get an initial prediction. Check the shape of the predicted result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below shows a handwritten image, where the title contains the predicted and the true values, based on their one hot encoding vector. It takes a **flattened image**, its ground truth one-hot label, and a predicted one-hot vector as arguments. Also, prints the predicted vector of probabilities, under the title.\n",
    "\n",
    "Call this function for a randomly picked input-ouput pair on the test dataset and its initial prediction made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction(flat_img, true_hot_lab, pred_hot_lab):\n",
    "\n",
    "    img = flat_img.reshape(28,28)\n",
    "    ture_lab = np.argmax(true_hot_lab)\n",
    "    pred_lab = np.argmax(pred_hot_lab)\n",
    "    \n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f\"True: {ture_lab} ; Pred: {pred_lab}\")\n",
    "    plt.axis('off')\n",
    "    red_vec = ', '.join([f\"{p:.2f}\" for p in pred_hot_lab])\n",
    "    plt.text(0.5,1.02,\"Prob. = \"+red_vec, fontsize=3, ha='center', transform=plt.gca().transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 One epoch training with single steps\n",
    "\n",
    "Train one epoch (the whole training dataset) on batches of 1000 items by using the [`train_on_batch()`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#train_on_batch) method of your model.\n",
    "\n",
    "Remember, one epoch is considered when the whole training dataset is used. Where, one step is one backpropagation made over one batch of data.\n",
    "Therefore, `steps * batchsize = training_dataset_size`.\n",
    "\n",
    "The `train_on_batch()` returns the cost before that certain training step. Store all that costs over all the iterations and plot them at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training, it's time to make a new prediction over the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the accuracy of the prediction, call the function below.\n",
    "\n",
    "This function:\n",
    " - compares the predicted handwritten values with the ground truth\n",
    " - computes the ratio of good predictions over the total.\n",
    " - shows five wrong predicted images using the function done in section 2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_accuracy(flat_imgs, true_hot, pred_hot):\n",
    "\n",
    "    pred_lab = [np.argmax(hot_lab) for hot_lab in pred_hot]\n",
    "    true_lab = [np.argmax(hot_lab) for hot_lab in true_hot]\n",
    "    \n",
    "    well = np.array([true==pred for (true, pred) in zip(true_lab, pred_lab)])\n",
    "    ratio = well.sum()/well.shape[0]\n",
    "    \n",
    "    print(f\" --> Total test Accuracy: {ratio*100:.2f}%\")\n",
    "\n",
    "    wrong_idx = np.where(well==False)[0]\n",
    "\n",
    "    for idx in np.random.choice(wrong_idx, 5):\n",
    "        show_prediction(flat_imgs[idx], true_hot[idx], pred_hot[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train for several epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To refine the training, you can re-train the model several epochs more, let's say 15 more. Check the [`Sequential.fit()`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit) method, where you can set the batchsize and the number of epochs to train as arguments. Also, check the `validation_data` argument to be able to provide the validation dataset. This method, returns a history. Store that history to check it after training.\n",
    "\n",
    "Also, measure the elapsed time in order to compare it with the convolutional network, in a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check every printed line. What are the _xxxx/xxxx_ numbers in front of the progress bar? See how `loss` and `val_loss` progress over the epochs.\n",
    "\n",
    "Which type of variable `history` is? Check also `history.history`.\n",
    "\n",
    "`history.history` contains the losses' evolution, for both the training and the validation datasets. Plot that curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happen if validation loss/accuracy is saturated while the training loss/accuracy is getting better? Is that situation good?\n",
    "\n",
    "Check again the accuracy of the model over the test dataset. Has it increase after training with more epochs? and training it with more and more epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image recognition with a CNN\n",
    "\n",
    "Let's do the same with a Convolutional Neural Network.\n",
    "\n",
    "![https://miro.medium.com/max/1400/1*vkQ0hXDaQv57sALXAJquxA.jpeg](https://miro.medium.com/max/1400/1*vkQ0hXDaQv57sALXAJquxA.jpeg)\n",
    "\n",
    "<hr>\n",
    "\n",
    "![https://docs.ecognition.com/Resources/Images/ECogUsr/UG_CNN_scheme.png](https://docs.ecognition.com/Resources/Images/ECogUsr/UG_CNN_scheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a convolutional neural network of three bottlenecks (`CNN2d+MaxPooling`). Use again a `Sequential()` model and use `tf.keras.layers.Conv2D` and `tf.keras.layers.MaxPool2D` layers.\n",
    "\n",
    "The idea is to reduce the 2D size at half on every convolutional step (i.e. `pool_size=2`), while increasing the features mapping. Then, set an increasing kernel size of Conv2D layers, e.g. $4\\rightarrow8\\rightarrow16$.\n",
    "\n",
    "Convolutional layers deals with 2D data. However, we want a 1D output of 10 neurons (one hot encoding vector). So use the `tf.keras.layers.Flatten` layer and add a final `Dense` layer (how many neurons should this last layer have?). Remember, this last layer should have the softmax activation function, while the convolutional could be (relu, sigmoid...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the summary of the model. Check how many *trainable parameters* it has, and compare it with the fully connected network done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `fit` method to train the model with 10 epochs. Check the elapsed time and compare it with the fully connected. Also store the history and plot the progress. Try with a different `batch_size` (100 or 5000), and check the elapsed time per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy of predictions over the test dataset and compare it with the dense model. Comment the performance in comparation of Dense model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Denoiser (pseudo-unsupervised learning)\n",
    "\n",
    "Let's try to remove noise from noisy images.\n",
    "\n",
    "An autoencoder is a model made of two parts: encoder + decoder, having a bottle neck in the middle (a flatten vector typically called code or embedding).\n",
    "\n",
    "Usually, autoencoders are fed with the same image $x$ in both, the input and the output. However, we can slightly modify this by setting an $x$ noisy image for the input and an $x'$ clean image for the output. Then, the model should learn to remove noise of noisy images.\n",
    "\n",
    "![https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Autoencoder_structure.png/350px-Autoencoder_structure.png](https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Autoencoder_structure.png/350px-Autoencoder_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to add some noise to images. Then we can use that as the noisy input, while the original will be the clean output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ae, vl_ae, te_ae = load_data()\n",
    "\n",
    "train_clean = tr_ae[0]\n",
    "val_clean = vl_ae[0]\n",
    "test_clean = te_ae[0]\n",
    "\n",
    "noise_factor = 0.5\n",
    "# random values as noise source\n",
    "train_noisy = train_clean + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=train_clean.shape)  \n",
    "val_noisy = val_clean + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=val_clean.shape) \n",
    "test_noisy = test_clean + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=test_clean.shape) \n",
    "\n",
    "# to make values in the range of 0 to 1: values<0 -> 0 while values>1 -> 1.\n",
    "train_noisy = np.clip(train_noisy, 0., 1.)   \n",
    "val_noisy = np.clip(val_noisy, 0., 1.)\n",
    "test_noisy = np.clip(test_noisy, 0., 1.)\n",
    "\n",
    "idxs = np.random.choice(test_noisy.shape[0], 5)\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i, idx in enumerate(idxs):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.axis(False)\n",
    "    plt.imshow(train_clean[idx,:,:], cmap='gray')\n",
    "    plt.subplot(2, 5, i+6)\n",
    "    plt.axis(False)\n",
    "    plt.imshow(train_noisy[idx,:,:], cmap='gray')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"{train_noisy.shape=}\")\n",
    "print(f\"{train_clean.shape=}\")\n",
    "print(f\"{val_noisy.shape=}\")\n",
    "print(f\"{val_clean.shape=}\")\n",
    "print(f\"{test_noisy.shape=}\")\n",
    "print(f\"{test_clean.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an autoencoder having three downsampling steps (encoder) and three upsamplig steps (decoder).\n",
    "\n",
    "In this case, try to do it in the _functional_ approach, instead of adding layers with the `Sequential.add()` method. It is an alternative approach to create a Sequential network, but sometimes it is more convenient, it is more flexible.\n",
    "\n",
    "Check https://www.tensorflow.org/guide/keras/functional_api#introduction\n",
    "\n",
    "Thus:\n",
    "1. create an input tensor using the `tf.keras.Input` class.\n",
    "1. apply this input tensor to a Conv2D layer. It will return an output tensor. Store it in a variable.\n",
    "1. apply this stored tensor to a MaxPooling2D layer and store the new returned tensor.\n",
    "1. repeat 2. and 3. two times more (set an appropriated kernel size like in section 3)\n",
    "2. apply a flatten layer (resulting to 144 linial neurons)\n",
    "3. reshape it back to (3,3,16)\n",
    "4. upsample the model using three steps (`Conv2DTranspose+UpSampling2D`), reducing the featuring maps just in an opposite way ($16\\rightarrow8\\rightarrow4$) while doubling the size of the images.\n",
    "\n",
    "Finally, create the model with `tf.keras.Model` class, using the input tensor and the last returned tensor as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model (in this case you can use the `MSE` as loss function, why?) and call the summary to check the model shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model for some epochs using the `fit()` method.\n",
    "\n",
    "In this case, let's learn some 'advanced' features provided by tensorflow (optional):\n",
    "\n",
    " - Check the `shuffle` input parameter of the `fit` function and activate it. What it does? Why can be useful?\n",
    "\n",
    " - It is possible to add callbacks on the `fit` method. Callbacks are functions (or classes) that are called after every training epoch (even every step). In this case, try to add an [`EarlyStopping()`](https://keras.io/api/callbacks/early_stopping/) callback to finish the training when the loss is saturated for **three epochs** on the **validation dataset** with a **minimum threshold of $0.05$**, set also the `verbose=1` to monitor it behavior. To be sure that the early stopper is triggered, set a huge number of epochs on the fit arguments.\n",
    "\n",
    "Plot also the progress via the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the noise on the test dataset using the `predict()` method.\n",
    "\n",
    "Make a subplot having three rows and 5 columns to show 5 different images. Show in the first row the clean image, the noisy in the second and, finally, the noise-removed image by the model in the third. $\\rightarrow$ Copy/paste the one in the beginning of this section and adapt the code to allow a new row for the denoised image by the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check better results on [the original page](https://keras.io/examples/vision/autoencoder/)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Autoencoder for dimension reduction (unsupervised training)\n",
    "\n",
    "Let's do a true autoencoder, where the input and the output are forced to be the same. It can seem stupid, but we will force passing all information of the image through a bottleneck, the embedding. Then, the model will learn to condensate all the information on an image to a few of numbers.\n",
    "\n",
    "Let's set an embedding of just two neurons.\n",
    "\n",
    "So, make an autoencoder, made of two independent models, the encoder and the decoder.\n",
    "\n",
    "Firstly, create an encoder model having 3 `Conv2D+MaxPooling2D` layers, and a final `Flattened+Dense` layer resulting with just 2 neurons. Use the functional approach like in section 4.\n",
    "\n",
    "Print the summary to check the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, create the decoder:\n",
    "1. begin with a Dense layer of 144 neurons, applied to last returned tensor from the encoder\n",
    "2. reshape it to (3, 3, 16)\n",
    "3. add 3 upsamplig steps like in section 4.\n",
    "4. create the model with `tf.keras.Model` using the last returned tensor of the encoder (used as first input in the decoder) and the last returned tensor of the decoder.\n",
    "5. print the summary of the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thirdly, let's build the autoencoder just by creating a model with the very first tensor for the encoder (Input) and the very last for the decoder, and print the summary.\n",
    "\n",
    "As the whole autoencoder is the only model to be trained, you should compile only the autoencoder. Set `MSE` as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit it with the images for 10 epochs and plot the history curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate fake digits by setting random numbers in the embedding vector: run (predict) the decoder model with random numbers and show the generated images. What you see? It is a number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a grid of $9\\times9$ subplots showing fake digits generated by increasing/decreasing the value of one or the other component of the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. [Just read] Autoencoders for random face generator\n",
    "\n",
    "Check the [`random_face_generator.ipynb` notebook](https://github.com/dhanushkamath/VariationalAutoencoder/blob/master/Variational_Autoencoder.ipynb), it is based on the same principle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "192.917px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
